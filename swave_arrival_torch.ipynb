{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os, random\n",
    "import h5py, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "### Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "### Torchvision\n",
    "from torchsummary import summary\n",
    "import torchvision.datasets as dest\n",
    "import torchvision.transforms as transformers\n",
    "\n",
    "### Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.1.min.js\": \"qkRvDQVAIfzsJo40iRBbxt6sttt0hv4lh74DG7OK4MCHv4C5oohXYoHUM5W11uqS\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.1.min.js\": \"Sb7Mr06a9TNlet/GEBeKaf5xH3eb6AlCzwjtU82wNPyDrnfoiVl26qnvlKjmcAd+\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.1.min.js\": \"HaJ15vgfmcfRtB4c4YBOI4f1MUujukqInOWVqZJZZGK7Q+ivud0OKGSTn/Vm2iso\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.1.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.1.min.js\": \"qkRvDQVAIfzsJo40iRBbxt6sttt0hv4lh74DG7OK4MCHv4C5oohXYoHUM5W11uqS\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.1.min.js\": \"Sb7Mr06a9TNlet/GEBeKaf5xH3eb6AlCzwjtU82wNPyDrnfoiVl26qnvlKjmcAd+\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.1.min.js\": \"HaJ15vgfmcfRtB4c4YBOI4f1MUujukqInOWVqZJZZGK7Q+ivud0OKGSTn/Vm2iso\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### seed_everythin\n",
    "seed = 1987\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "ngpu = 1\n",
    "batch_size = 512\n",
    "num_workers = 1\n",
    "channels = 3\n",
    "data_range = 10\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'selected_data.pkl'\n",
    "outfile = open(filename,'rb')\n",
    "data = pickle.load(outfile)\n",
    "xdata = np.array(data['data'], dtype = 'float32')\n",
    "ylablel = np.array(data['labels'], dtype = 'float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((145864, 50, 3), (36466, 50, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(xdata, ylablel, test_size=0.2)\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xscalers = {}\n",
    "for i in range(xdata.shape[2]):\n",
    "    xscalers[i] = StandardScaler()\n",
    "    X_train[:, :, i] = xscalers[i].fit_transform(X_train[:, :, i])\n",
    "\n",
    "yscaler = StandardScaler()\n",
    "y_train = yscaler.fit_transform(y_train.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "for i in range(xdata.shape[2]):\n",
    "    X_val[:, :, i] = xscalers[i].transform(X_val[:, :, i])\n",
    "\n",
    "y_val = yscaler.transform(y_val.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.706501, 7.02627)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.max(), y_train.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset\n",
    "class SeismicDataset(Dataset):\n",
    "    def __init__(self, method):\n",
    "        if method == 'train':\n",
    "            shp = X_train.shape\n",
    "            self.xdata = torch.Tensor(X_train).view(shp[0], shp[2], shp[1])[:,:, 0:data_range]\n",
    "            self.ylablel = torch.Tensor(y_train)\n",
    "        elif method == 'val':\n",
    "            shp = X_val.shape\n",
    "            self.xdata = torch.Tensor(X_val).view(shp[0], shp[2], shp[1])[:,:, 0:data_range]\n",
    "            self.ylablel = torch.Tensor(y_val)\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.xdata)\n",
    "        \n",
    "    def __getitem__(self, indx):\n",
    "        return self.xdata[indx], self.ylablel[indx]\n",
    "        \n",
    "### Dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    SeismicDataset('train'),\n",
    "    batch_size= batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = num_workers\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    SeismicDataset('val'),\n",
    "    batch_size= batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 3, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels,  bias = False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.batchnorm(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1_pool):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        self.branch1 = ConvBlock(in_channels, out_1x1, kernel_size = 1)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBlock(in_channels, red_3x3, kernel_size = 1),\n",
    "            ConvBlock(red_3x3, out_3x3, kernel_size = 3, padding = 1)\n",
    "        )\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            ConvBlock(in_channels, red_5x5, kernel_size = 1),\n",
    "            ConvBlock(red_5x5, out_5x5, kernel_size = 5, padding = 2)\n",
    "        )\n",
    "        \n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=3, stride = 1, padding = 1),\n",
    "            ConvBlock(in_channels, out_1x1_pool, kernel_size = 1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat(\n",
    "            [self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], 1\n",
    "        )\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeismicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SeismicNet, self).__init__()\n",
    "        self.conv1 = ConvBlock(3, 192, kernel_size = 2, stride = 2)\n",
    "        self.inception_1a = InceptionBlock(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception_1b = InceptionBlock(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=3, stride=2, padding = 1)\n",
    "        \n",
    "        self.averagepool1 = nn.AvgPool1d(kernel_size= 7, stride= 1)\n",
    "        self.fc1 = nn.Linear(3360, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "        self.dropout1 = nn.Dropout2d(p = 0.25)\n",
    "        self.dropout2 = nn.Dropout2d(p = 0.20)\n",
    "        self.dropout3 = nn.Dropout2d(p = 0.15)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.inception_1a(x)\n",
    "        x = self.inception_1b(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.averagepool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.input_dim = channels*data_range\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.35),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.35),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = Variable(torch.flatten(x, start_dim = 1))\n",
    "        x = self.main(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.input_dim = channels*data_range\n",
    "        self.fc1 = nn.Linear(self.input_dim, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dp1 = nn.Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dp2 = nn.Dropout(0.25)\n",
    "        self.fc3 = nn.Linear(64, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = Variable(torch.flatten(x, start_dim = 1))\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dp1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel().to(device) #SeismicNet().to(device)\n",
    "if (device.type == 'cuda' and (ngpu> 1)):\n",
    "    model = nn.DataParallel(model, list(range(ngpu)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = SummaryWriter()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0003)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=10, factor = 0.15, verbose=True\n",
    ")\n",
    "\n",
    "# scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0003, max_lr=0.1, cycle_momentum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for j, (data, label) in enumerate(train_dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        data, y = data.to(device), label.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "               \n",
    "        tb.add_histogram('gradients/fc1', model.fc1.weight.grad, epoch)\n",
    "        tb.add_histogram('gradients/fc2', model.fc2.weight.grad, epoch)\n",
    "        tb.add_histogram('gradients/fc3', model.fc3.weight.grad, epoch)\n",
    "        tb.add_histogram('gradients/fc4', model.fc4.weight.grad, epoch)\n",
    "        \n",
    "        tb.add_scalars('fc_grad/mean',{\n",
    "            'fc1': model.fc1.weight.grad.abs().mean(),\n",
    "            'fc2': model.fc2.weight.grad.abs().mean(),\n",
    "            'fc3': model.fc3.weight.grad.abs().mean(),\n",
    "            'fc4': model.fc4.weight.grad.abs().mean()\n",
    "            \n",
    "        }, epoch)\n",
    "        \n",
    "        tb.add_scalars('fc_grad/variance',{\n",
    "            'fc1': model.fc1.weight.grad.abs().var(),\n",
    "            'fc2': model.fc2.weight.grad.abs().var(),\n",
    "            'fc3': model.fc3.weight.grad.abs().var(),\n",
    "            'fc4': model.fc4.weight.grad.abs().var()\n",
    "            \n",
    "        }, epoch)\n",
    "        \n",
    "        tb.add_scalars('fc_grad/max',{\n",
    "            'fc1': model.fc1.weight.grad.abs().max(),\n",
    "            'fc2': model.fc2.weight.grad.abs().max(),\n",
    "            'fc3': model.fc3.weight.grad.abs().max(),\n",
    "            'fc4': model.fc4.weight.grad.abs().max()\n",
    "            \n",
    "        }, epoch)\n",
    "        \n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    mean_loss = sum(losses)/len(losses)\n",
    "    return mean_loss\n",
    "        \n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        for j, (data, label) in enumerate(val_dataloader, 0):\n",
    "            data = data.to(device).view(data.shape[0], data.shape[2], data.shape[1])\n",
    "            y = label.to(device)\n",
    "            output = model(data)\n",
    "            losses.append(criterion(output, y).item())\n",
    "    return sum(losses)/len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 0/300 | Train loss: 1.0029791399052268 | Val loss: 0.9932259801361296\n",
      "| Epoch: 1/300 | Train loss: 0.9994859699617352 | Val loss: 0.9951163712475035\n",
      "| Epoch: 2/300 | Train loss: 1.0048325480076306 | Val loss: 0.9920800377925237\n",
      "| Epoch: 3/300 | Train loss: 0.9996001885648359 | Val loss: 0.9892509596215354\n",
      "| Epoch: 4/300 | Train loss: 0.9946350043280083 | Val loss: 0.9910081012381448\n",
      "| Epoch: 5/300 | Train loss: 0.9949361491621587 | Val loss: 0.9870519472493066\n",
      "| Epoch: 6/300 | Train loss: 0.9871781255069532 | Val loss: 0.9800974047846265\n",
      "| Epoch: 7/300 | Train loss: 0.9831449949950503 | Val loss: 0.9760463221205605\n",
      "| Epoch: 8/300 | Train loss: 0.9767379390565972 | Val loss: 0.9696811272038354\n",
      "| Epoch: 9/300 | Train loss: 0.9714652433730009 | Val loss: 0.96396844006247\n",
      "| Epoch: 10/300 | Train loss: 0.9668804317189936 | Val loss: 0.9650855039556822\n",
      "| Epoch: 11/300 | Train loss: 0.9646151944210655 | Val loss: 0.9646155453390546\n",
      "| Epoch: 12/300 | Train loss: 0.9809925562457035 | Val loss: 0.9584505317939652\n",
      "| Epoch: 13/300 | Train loss: 0.9565242959741961 | Val loss: 0.9516270400749313\n",
      "| Epoch: 14/300 | Train loss: 0.9553068407794886 | Val loss: 0.9516283381316397\n",
      "| Epoch: 15/300 | Train loss: 0.9521652872102302 | Val loss: 0.9507714038093885\n",
      "| Epoch: 16/300 | Train loss: 0.9514852615824917 | Val loss: 0.9504286986258295\n",
      "| Epoch: 17/300 | Train loss: 0.9678028955794218 | Val loss: 0.9441015563077397\n",
      "| Epoch: 18/300 | Train loss: 0.9534315243101956 | Val loss: 0.9438780181937747\n",
      "| Epoch: 19/300 | Train loss: 0.9478583421623498 | Val loss: 0.9415189963248041\n",
      "| Epoch: 20/300 | Train loss: 0.9451815552878798 | Val loss: 0.9411150283283658\n",
      "| Epoch: 21/300 | Train loss: 0.9418065307433146 | Val loss: 0.9396025050017569\n",
      "| Epoch: 22/300 | Train loss: 0.9406475652728165 | Val loss: 0.9376479147209061\n",
      "| Epoch: 23/300 | Train loss: 0.9390338688565973 | Val loss: 0.9338832712835736\n",
      "| Epoch: 24/300 | Train loss: 0.9370637469124375 | Val loss: 0.9320807771550285\n",
      "| Epoch: 25/300 | Train loss: 0.9337412639668113 | Val loss: 0.9293013686935107\n",
      "| Epoch: 26/300 | Train loss: 0.931400642060397 | Val loss: 0.9249143128593763\n",
      "| Epoch: 27/300 | Train loss: 0.928398673367082 | Val loss: 0.922825437453058\n",
      "| Epoch: 28/300 | Train loss: 0.9273791110306455 | Val loss: 0.9198817354109552\n",
      "| Epoch: 29/300 | Train loss: 0.9249679023759407 | Val loss: 0.9171541449096468\n",
      "| Epoch: 30/300 | Train loss: 0.9255916817146436 | Val loss: 0.9129415195849206\n",
      "| Epoch: 31/300 | Train loss: 0.9193411318879379 | Val loss: 0.9099753929509057\n",
      "| Epoch: 32/300 | Train loss: 0.9176492057348552 | Val loss: 0.9099421915080812\n",
      "| Epoch: 33/300 | Train loss: 0.915338755490487 | Val loss: 0.9080415094892184\n",
      "| Epoch: 34/300 | Train loss: 0.9149697937463459 | Val loss: 0.9081210841735204\n",
      "| Epoch: 35/300 | Train loss: 0.9146198860385961 | Val loss: 0.9081900417804718\n",
      "| Epoch: 36/300 | Train loss: 0.9130652331469352 | Val loss: 0.9020345658063889\n",
      "| Epoch: 37/300 | Train loss: 0.9091447043837162 | Val loss: 0.8997559712992774\n",
      "| Epoch: 38/300 | Train loss: 0.9083111763000489 | Val loss: 0.8978239488270547\n",
      "| Epoch: 39/300 | Train loss: 0.9062366795121578 | Val loss: 0.896836499373118\n",
      "| Epoch: 40/300 | Train loss: 0.9073857104569151 | Val loss: 0.8965349015262392\n",
      "| Epoch: 41/300 | Train loss: 0.9051012658236319 | Val loss: 0.8975083472000228\n",
      "| Epoch: 42/300 | Train loss: 0.9044659210924517 | Val loss: 0.8954807139105267\n",
      "| Epoch: 43/300 | Train loss: 0.9023317780411034 | Val loss: 0.8917651126782099\n",
      "| Epoch: 44/300 | Train loss: 0.9027350620219582 | Val loss: 0.8892805476983389\n",
      "| Epoch: 45/300 | Train loss: 0.9003032475187067 | Val loss: 0.8909661314553685\n",
      "| Epoch: 46/300 | Train loss: 0.902359167943921 | Val loss: 0.8896413735217519\n",
      "| Epoch: 47/300 | Train loss: 0.8985053060347574 | Val loss: 0.8931339879830679\n",
      "| Epoch: 48/300 | Train loss: 0.8979972985752842 | Val loss: 0.8857512863145934\n",
      "| Epoch: 49/300 | Train loss: 0.8970857043015329 | Val loss: 0.8930166951484151\n",
      "| Epoch: 50/300 | Train loss: 0.8955341583804081 | Val loss: 0.8857594960265689\n",
      "| Epoch: 51/300 | Train loss: 0.8950460607545417 | Val loss: 0.887342614432176\n",
      "| Epoch: 52/300 | Train loss: 0.8938011706921092 | Val loss: 0.8851157195038266\n",
      "| Epoch: 53/300 | Train loss: 0.8947234367069445 | Val loss: 0.8820358349217309\n",
      "| Epoch: 54/300 | Train loss: 0.893747221586997 | Val loss: 0.8856379331813918\n",
      "| Epoch: 55/300 | Train loss: 0.8935050924619039 | Val loss: 0.8888652755154504\n",
      "| Epoch: 56/300 | Train loss: 0.892695008662709 | Val loss: 0.8834594579206573\n",
      "| Epoch: 57/300 | Train loss: 0.8936780795716402 | Val loss: 0.8797633010480139\n",
      "| Epoch: 58/300 | Train loss: 0.8920610572162427 | Val loss: 0.8777097662289938\n",
      "| Epoch: 59/300 | Train loss: 0.8914257779456022 | Val loss: 0.8793603190117412\n",
      "| Epoch: 60/300 | Train loss: 0.8900391047460992 | Val loss: 0.8784531495637364\n",
      "| Epoch: 61/300 | Train loss: 0.8891523955161111 | Val loss: 0.8840642919143041\n",
      "| Epoch: 62/300 | Train loss: 0.8895647042676023 | Val loss: 0.8765345306860076\n",
      "| Epoch: 63/300 | Train loss: 0.8887841333422745 | Val loss: 0.8780614311496416\n",
      "| Epoch: 64/300 | Train loss: 0.8887602852101911 | Val loss: 0.877571725183063\n",
      "| Epoch: 65/300 | Train loss: 0.8872441693356162 | Val loss: 0.8748596327172385\n",
      "| Epoch: 66/300 | Train loss: 0.8952596543128031 | Val loss: 0.8791907206177711\n",
      "| Epoch: 67/300 | Train loss: 0.8857084763677496 | Val loss: 0.8754057726926274\n",
      "| Epoch: 68/300 | Train loss: 0.88525284131368 | Val loss: 0.8761567581031058\n",
      "| Epoch: 69/300 | Train loss: 0.8848696652211641 | Val loss: 0.8763559775220023\n",
      "| Epoch: 70/300 | Train loss: 0.8841545770042821 | Val loss: 0.8745221810208427\n",
      "| Epoch: 71/300 | Train loss: 0.8837102333704631 | Val loss: 0.872695396343867\n",
      "| Epoch: 72/300 | Train loss: 0.882946647259227 | Val loss: 0.8760276801056333\n",
      "| Epoch: 73/300 | Train loss: 0.8837228302370038 | Val loss: 0.8744793211420377\n",
      "| Epoch: 74/300 | Train loss: 0.8830191925952309 | Val loss: 0.8725117047627767\n",
      "| Epoch: 75/300 | Train loss: 0.883315144296278 | Val loss: 0.8729842520422406\n",
      "| Epoch: 76/300 | Train loss: 0.8814858716830872 | Val loss: 0.8722386186321577\n",
      "| Epoch: 77/300 | Train loss: 0.8820414066314697 | Val loss: 0.8725568080941836\n",
      "| Epoch: 78/300 | Train loss: 0.8807075182596843 | Val loss: 0.8731645560926862\n",
      "| Epoch: 79/300 | Train loss: 0.8807220063711467 | Val loss: 0.8707978642649121\n",
      "| Epoch: 80/300 | Train loss: 0.8793176402125442 | Val loss: 0.8693436624275314\n",
      "| Epoch: 81/300 | Train loss: 0.8794994170205634 | Val loss: 0.8721688431170251\n",
      "| Epoch: 82/300 | Train loss: 0.8791104860473098 | Val loss: 0.8717704580889808\n",
      "| Epoch: 83/300 | Train loss: 0.8796383019079241 | Val loss: 0.8718385663297441\n",
      "| Epoch: 84/300 | Train loss: 0.8788377379116259 | Val loss: 0.8729541260335181\n",
      "| Epoch: 85/300 | Train loss: 0.8783692023210358 | Val loss: 0.8751605815357633\n",
      "| Epoch: 86/300 | Train loss: 0.8778821334504244 | Val loss: 0.874533529082934\n",
      "| Epoch: 87/300 | Train loss: 0.8763506218006737 | Val loss: 0.8688787892460823\n",
      "| Epoch: 88/300 | Train loss: 0.8769245986352887 | Val loss: 0.8642609814802805\n",
      "| Epoch: 89/300 | Train loss: 0.8767946312302037 | Val loss: 0.8783516859014829\n",
      "| Epoch: 90/300 | Train loss: 0.8767772005315413 | Val loss: 0.8672239027089543\n",
      "| Epoch: 91/300 | Train loss: 0.8759674724779631 | Val loss: 0.8717767281664742\n",
      "| Epoch: 92/300 | Train loss: 0.8758171616939077 | Val loss: 0.8658920807970895\n",
      "| Epoch: 93/300 | Train loss: 0.8758078909756845 | Val loss: 0.8646154486470752\n",
      "| Epoch: 94/300 | Train loss: 0.8748714405193664 | Val loss: 0.8689428584443198\n",
      "| Epoch: 95/300 | Train loss: 0.8747790027083012 | Val loss: 0.864980029563109\n",
      "| Epoch: 96/300 | Train loss: 0.8741349295565957 | Val loss: 0.8625604079829322\n",
      "| Epoch: 97/300 | Train loss: 0.8739046333128946 | Val loss: 0.8689058605167601\n",
      "| Epoch: 98/300 | Train loss: 0.8755553544613353 | Val loss: 0.8615632587009006\n",
      "| Epoch: 99/300 | Train loss: 0.8742026952275058 | Val loss: 0.8636101128326522\n",
      "| Epoch: 100/300 | Train loss: 0.8727550374834161 | Val loss: 0.8641252525978618\n",
      "| Epoch: 101/300 | Train loss: 0.8720432043075561 | Val loss: 0.8648238819506433\n",
      "| Epoch: 102/300 | Train loss: 0.8735216234859667 | Val loss: 0.8632546390096346\n",
      "| Epoch: 103/300 | Train loss: 0.8711631674515573 | Val loss: 0.8614765844411321\n",
      "| Epoch: 104/300 | Train loss: 0.8708145881953993 | Val loss: 0.8636588777105013\n",
      "| Epoch: 105/300 | Train loss: 0.872700485848544 | Val loss: 0.8642012476921082\n",
      "| Epoch: 106/300 | Train loss: 0.8718119539712604 | Val loss: 0.8667714455061488\n",
      "| Epoch: 107/300 | Train loss: 0.8745716310383981 | Val loss: 0.8603685291277038\n",
      "| Epoch: 108/300 | Train loss: 0.8715831595554686 | Val loss: 0.8620746061205864\n",
      "| Epoch: 109/300 | Train loss: 0.8731573263804118 | Val loss: 0.8627008439766036\n",
      "| Epoch: 110/300 | Train loss: 0.8689792308891029 | Val loss: 0.8626320130295224\n",
      "| Epoch: 111/300 | Train loss: 0.8701627656033165 | Val loss: 0.8607241652078099\n",
      "| Epoch: 112/300 | Train loss: 0.8716806302990829 | Val loss: 0.8640820715162489\n",
      "| Epoch: 113/300 | Train loss: 0.8712226566515471 | Val loss: 0.8681898696555032\n",
      "| Epoch: 114/300 | Train loss: 0.8708034494466949 | Val loss: 0.8642148085766368\n",
      "| Epoch: 115/300 | Train loss: 0.8697485254521955 | Val loss: 0.8624175132976638\n",
      "| Epoch: 116/300 | Train loss: 0.8695418587902136 | Val loss: 0.8612275264329381\n",
      "| Epoch: 117/300 | Train loss: 0.8672852869619403 | Val loss: 0.8622057280606694\n",
      "| Epoch: 118/300 | Train loss: 0.8691229688493829 | Val loss: 0.8666664163271586\n",
      "Epoch   119: reducing learning rate of group 0 to 4.5000e-05.\n",
      "| Epoch: 119/300 | Train loss: 0.8665893412472909 | Val loss: 0.8545201636022992\n",
      "| Epoch: 120/300 | Train loss: 0.8600851328749406 | Val loss: 0.8535970225930214\n",
      "| Epoch: 121/300 | Train loss: 0.8584405725462395 | Val loss: 0.851156579123603\n",
      "| Epoch: 122/300 | Train loss: 0.8586077939000046 | Val loss: 0.8517772489123874\n",
      "| Epoch: 123/300 | Train loss: 0.8595289636076542 | Val loss: 0.853693781627549\n",
      "| Epoch: 124/300 | Train loss: 0.8585927722746866 | Val loss: 0.8502450394961569\n",
      "| Epoch: 125/300 | Train loss: 0.8577920328106796 | Val loss: 0.8510375453366174\n",
      "| Epoch: 126/300 | Train loss: 0.8583768066607024 | Val loss: 0.8521143454644415\n",
      "| Epoch: 127/300 | Train loss: 0.8575428747294241 | Val loss: 0.8509988064567248\n",
      "| Epoch: 128/300 | Train loss: 0.8566640425146672 | Val loss: 0.8493018862273958\n",
      "| Epoch: 129/300 | Train loss: 0.8575253927916812 | Val loss: 0.8513496195276579\n",
      "| Epoch: 130/300 | Train loss: 0.8576258567341587 | Val loss: 0.8476940616965294\n",
      "| Epoch: 131/300 | Train loss: 0.857671944927751 | Val loss: 0.8477939408686426\n",
      "| Epoch: 132/300 | Train loss: 0.8567984890519527 | Val loss: 0.8521911087963316\n",
      "| Epoch: 133/300 | Train loss: 0.8569065987018116 | Val loss: 0.8496487620804045\n",
      "| Epoch: 134/300 | Train loss: 0.8559806940848367 | Val loss: 0.8493285328149796\n",
      "| Epoch: 135/300 | Train loss: 0.8575149322810925 | Val loss: 0.8508492261171341\n",
      "| Epoch: 136/300 | Train loss: 0.8571634330247578 | Val loss: 0.849496324857076\n",
      "| Epoch: 137/300 | Train loss: 0.8562904504307529 | Val loss: 0.8492202353146341\n",
      "| Epoch: 138/300 | Train loss: 0.8562490684944287 | Val loss: 0.8503643779291047\n",
      "| Epoch: 139/300 | Train loss: 0.8562841338023804 | Val loss: 0.8511463047729598\n",
      "| Epoch: 140/300 | Train loss: 0.8568188154906557 | Val loss: 0.8489835932850838\n",
      "| Epoch: 141/300 | Train loss: 0.8565189100148385 | Val loss: 0.8507407928506533\n",
      "Epoch   142: reducing learning rate of group 0 to 6.7500e-06.\n",
      "| Epoch: 142/300 | Train loss: 0.8553281160823086 | Val loss: 0.8471902476416694\n",
      "| Epoch: 143/300 | Train loss: 0.8541742726376182 | Val loss: 0.8472330760624673\n",
      "| Epoch: 144/300 | Train loss: 0.8547759528745684 | Val loss: 0.8461301773786545\n",
      "| Epoch: 145/300 | Train loss: 0.8547171389847471 | Val loss: 0.8481517177489069\n",
      "| Epoch: 146/300 | Train loss: 0.8544089043349551 | Val loss: 0.8484095964166853\n",
      "| Epoch: 147/300 | Train loss: 0.8533410906791687 | Val loss: 0.8464861404564645\n",
      "| Epoch: 148/300 | Train loss: 0.854009457220111 | Val loss: 0.8471004996034834\n",
      "| Epoch: 149/300 | Train loss: 0.8547577878885102 | Val loss: 0.8472664223776923\n",
      "| Epoch: 150/300 | Train loss: 0.8552509901816385 | Val loss: 0.8464678583873643\n",
      "| Epoch: 151/300 | Train loss: 0.8551100530122456 | Val loss: 0.8469278969698482\n",
      "| Epoch: 152/300 | Train loss: 0.8552991716485274 | Val loss: 0.8477353933784697\n",
      "| Epoch: 153/300 | Train loss: 0.8545419640708388 | Val loss: 0.8467124890949991\n",
      "| Epoch: 154/300 | Train loss: 0.853597254711285 | Val loss: 0.8492666565709643\n",
      "| Epoch: 155/300 | Train loss: 0.8540499984172353 | Val loss: 0.8470641374588013\n",
      "Epoch   156: reducing learning rate of group 0 to 1.0125e-06.\n",
      "| Epoch: 156/300 | Train loss: 0.8537150956036752 | Val loss: 0.8464208071430525\n",
      "| Epoch: 157/300 | Train loss: 0.8528134442212288 | Val loss: 0.8461688988738589\n",
      "| Epoch: 158/300 | Train loss: 0.8543912149312204 | Val loss: 0.8490226624740494\n",
      "| Epoch: 159/300 | Train loss: 0.8538639842418202 | Val loss: 0.8479504585266113\n",
      "| Epoch: 160/300 | Train loss: 0.8538110825053432 | Val loss: 0.8477222936020957\n",
      "| Epoch: 161/300 | Train loss: 0.8538166667285718 | Val loss: 0.846538417869144\n",
      "| Epoch: 162/300 | Train loss: 0.8524520424374363 | Val loss: 0.8459198152025541\n",
      "| Epoch: 163/300 | Train loss: 0.8540293530413979 | Val loss: 0.843956834740109\n",
      "| Epoch: 164/300 | Train loss: 0.8533421756928428 | Val loss: 0.8462983220815659\n",
      "| Epoch: 165/300 | Train loss: 0.8535173338756227 | Val loss: 0.8475641475783454\n",
      "| Epoch: 166/300 | Train loss: 0.8543812620012383 | Val loss: 0.8462239330013593\n",
      "| Epoch: 167/300 | Train loss: 0.853970229834841 | Val loss: 0.8462933144635625\n",
      "| Epoch: 168/300 | Train loss: 0.8534893324500636 | Val loss: 0.8456386683715714\n",
      "| Epoch: 169/300 | Train loss: 0.8535829106966655 | Val loss: 0.8468552695380317\n",
      "| Epoch: 170/300 | Train loss: 0.8538731727683754 | Val loss: 0.8467480838298798\n",
      "| Epoch: 171/300 | Train loss: 0.8527687018377739 | Val loss: 0.8462162936727206\n",
      "| Epoch: 172/300 | Train loss: 0.8570115695919907 | Val loss: 0.846104128493203\n",
      "| Epoch: 173/300 | Train loss: 0.8535178573508012 | Val loss: 0.8458490230970912\n",
      "| Epoch: 174/300 | Train loss: 0.853036031597539 | Val loss: 0.84371653613117\n",
      "| Epoch: 175/300 | Train loss: 0.8536872702732421 | Val loss: 0.8478861807121171\n",
      "| Epoch: 176/300 | Train loss: 0.8534551273312485 | Val loss: 0.8465276443296008\n",
      "| Epoch: 177/300 | Train loss: 0.8542974200165062 | Val loss: 0.8457823867599169\n",
      "| Epoch: 178/300 | Train loss: 0.8544097099387855 | Val loss: 0.8449895224637456\n",
      "| Epoch: 179/300 | Train loss: 0.8544052546484429 | Val loss: 0.845185281501876\n",
      "| Epoch: 180/300 | Train loss: 0.8537533203760783 | Val loss: 0.84735490133365\n",
      "| Epoch: 181/300 | Train loss: 0.8543144282541777 | Val loss: 0.8485263801283307\n",
      "| Epoch: 182/300 | Train loss: 0.852220961085537 | Val loss: 0.8460451372795634\n",
      "| Epoch: 183/300 | Train loss: 0.8542660769663359 | Val loss: 0.8479991414480739\n",
      "| Epoch: 184/300 | Train loss: 0.8535838852848923 | Val loss: 0.8489548878537284\n",
      "| Epoch: 185/300 | Train loss: 0.8541271747204295 | Val loss: 0.8455851938989427\n",
      "Epoch   186: reducing learning rate of group 0 to 1.5187e-07.\n",
      "| Epoch: 186/300 | Train loss: 0.8541914283183583 | Val loss: 0.846261005434725\n",
      "| Epoch: 187/300 | Train loss: 0.8540694048530177 | Val loss: 0.8447662202848328\n",
      "| Epoch: 188/300 | Train loss: 0.8537048659826579 | Val loss: 0.8474285329381624\n",
      "| Epoch: 189/300 | Train loss: 0.852650865546444 | Val loss: 0.8458828147914674\n",
      "| Epoch: 190/300 | Train loss: 0.8546912329238757 | Val loss: 0.8461317039198346\n",
      "| Epoch: 191/300 | Train loss: 0.8532247014213027 | Val loss: 0.8472814510265986\n",
      "| Epoch: 192/300 | Train loss: 0.8544987500759593 | Val loss: 0.8478262548645338\n",
      "| Epoch: 193/300 | Train loss: 0.8543564020541676 | Val loss: 0.8456414623392953\n",
      "| Epoch: 194/300 | Train loss: 0.8543322364489238 | Val loss: 0.845914788544178\n",
      "| Epoch: 195/300 | Train loss: 0.8544348327737106 | Val loss: 0.8458000504308276\n",
      "| Epoch: 196/300 | Train loss: 0.8541264732678732 | Val loss: 0.8460542874203788\n",
      "Epoch   197: reducing learning rate of group 0 to 2.2781e-08.\n",
      "| Epoch: 197/300 | Train loss: 0.8540168793577897 | Val loss: 0.8463854988416036\n",
      "| Epoch: 198/300 | Train loss: 0.8537394705571626 | Val loss: 0.843695853319433\n",
      "| Epoch: 199/300 | Train loss: 0.85479621427101 | Val loss: 0.8468555129236646\n",
      "| Epoch: 200/300 | Train loss: 0.8536004603954783 | Val loss: 0.8443396290143331\n",
      "| Epoch: 201/300 | Train loss: 0.8539315800917776 | Val loss: 0.8456117626693513\n",
      "| Epoch: 202/300 | Train loss: 0.8536551134628162 | Val loss: 0.8487417499224345\n",
      "| Epoch: 203/300 | Train loss: 0.854244337583843 | Val loss: 0.8474693670868874\n",
      "| Epoch: 204/300 | Train loss: 0.8552915631679067 | Val loss: 0.8476486669646369\n",
      "| Epoch: 205/300 | Train loss: 0.8539851360153734 | Val loss: 0.8446576338675287\n",
      "| Epoch: 206/300 | Train loss: 0.853326213778111 | Val loss: 0.8456074835525619\n",
      "| Epoch: 207/300 | Train loss: 0.8528878406474465 | Val loss: 0.8443676912122302\n",
      "Epoch   208: reducing learning rate of group 0 to 3.4172e-09.\n",
      "| Epoch: 208/300 | Train loss: 0.853492960595248 | Val loss: 0.8466357679830657\n",
      "| Epoch: 209/300 | Train loss: 0.8533476909001668 | Val loss: 0.8462009752790133\n",
      "| Epoch: 210/300 | Train loss: 0.8535471813720569 | Val loss: 0.8479237680633863\n",
      "| Epoch: 211/300 | Train loss: 0.8541766501309579 | Val loss: 0.846378448108832\n",
      "| Epoch: 212/300 | Train loss: 0.8530030790128206 | Val loss: 0.8467156382070647\n",
      "| Epoch: 213/300 | Train loss: 0.8537101929647881 | Val loss: 0.8447767330540551\n",
      "| Epoch: 214/300 | Train loss: 0.8541194961782087 | Val loss: 0.8453386409415139\n",
      "| Epoch: 215/300 | Train loss: 0.8529859279331408 | Val loss: 0.8467717501852248\n",
      "| Epoch: 216/300 | Train loss: 0.8537047752162866 | Val loss: 0.8453616698582967\n",
      "| Epoch: 217/300 | Train loss: 0.8535728339563337 | Val loss: 0.8465013404687246\n",
      "| Epoch: 218/300 | Train loss: 0.8534981997389542 | Val loss: 0.8468734588887956\n",
      "| Epoch: 219/300 | Train loss: 0.8528808277949952 | Val loss: 0.8471357449889183\n",
      "| Epoch: 220/300 | Train loss: 0.8529557255276462 | Val loss: 0.8458735893170038\n",
      "| Epoch: 221/300 | Train loss: 0.8540005556324072 | Val loss: 0.8481341575582823\n",
      "| Epoch: 222/300 | Train loss: 0.8528906997881438 | Val loss: 0.848011217183537\n",
      "| Epoch: 223/300 | Train loss: 0.8524899909370824 | Val loss: 0.8482818065418137\n",
      "| Epoch: 224/300 | Train loss: 0.8550067261645669 | Val loss: 0.845653249985642\n",
      "| Epoch: 225/300 | Train loss: 0.8539441968265332 | Val loss: 0.8463054713275697\n",
      "| Epoch: 226/300 | Train loss: 0.8539618354094656 | Val loss: 0.8475598005784882\n",
      "| Epoch: 227/300 | Train loss: 0.8534611743793153 | Val loss: 0.8474385663866997\n",
      "| Epoch: 228/300 | Train loss: 0.8529921638338189 | Val loss: 0.8484289778603448\n",
      "| Epoch: 229/300 | Train loss: 0.853314719492929 | Val loss: 0.8452932346198294\n",
      "| Epoch: 230/300 | Train loss: 0.8542895041014019 | Val loss: 0.8459081078569094\n",
      "| Epoch: 231/300 | Train loss: 0.8537542755143684 | Val loss: 0.8476276828183068\n",
      "| Epoch: 232/300 | Train loss: 0.8537867878612719 | Val loss: 0.8485133705867661\n",
      "| Epoch: 233/300 | Train loss: 0.8543328471351088 | Val loss: 0.8490634568863444\n",
      "| Epoch: 234/300 | Train loss: 0.8539383775309513 | Val loss: 0.8455798336201243\n",
      "| Epoch: 235/300 | Train loss: 0.8532983221505818 | Val loss: 0.8452538682354821\n",
      "| Epoch: 236/300 | Train loss: 0.8533187776281123 | Val loss: 0.844865600268046\n",
      "| Epoch: 237/300 | Train loss: 0.8545502210918225 | Val loss: 0.8465189023150338\n",
      "| Epoch: 238/300 | Train loss: 0.8550405406115348 | Val loss: 0.8447121381759644\n",
      "| Epoch: 239/300 | Train loss: 0.8540089824743439 | Val loss: 0.8477865416142676\n",
      "| Epoch: 240/300 | Train loss: 0.8529655883186742 | Val loss: 0.8463331659634908\n",
      "| Epoch: 241/300 | Train loss: 0.8538188961514256 | Val loss: 0.8452958539128304\n",
      "| Epoch: 242/300 | Train loss: 0.8536768474076923 | Val loss: 0.848026804625988\n",
      "| Epoch: 243/300 | Train loss: 0.85435901712953 | Val loss: 0.8455504104495049\n",
      "| Epoch: 244/300 | Train loss: 0.8538581590903432 | Val loss: 0.8478874183363385\n",
      "| Epoch: 245/300 | Train loss: 0.8543196345630445 | Val loss: 0.8462319076061249\n",
      "| Epoch: 246/300 | Train loss: 0.8543243372649477 | Val loss: 0.8462994429800246\n",
      "| Epoch: 247/300 | Train loss: 0.8542055291041993 | Val loss: 0.8459718086653285\n",
      "| Epoch: 248/300 | Train loss: 0.8540660699208578 | Val loss: 0.8473551952176623\n",
      "| Epoch: 249/300 | Train loss: 0.8540775957860445 | Val loss: 0.8459975976083014\n",
      "| Epoch: 250/300 | Train loss: 0.8531594979135614 | Val loss: 0.8472213985191451\n",
      "| Epoch: 251/300 | Train loss: 0.852579072692938 | Val loss: 0.8465881504946284\n",
      "| Epoch: 252/300 | Train loss: 0.8558552934412371 | Val loss: 0.8480893439716763\n",
      "| Epoch: 253/300 | Train loss: 0.8563215377037985 | Val loss: 0.8442713850074344\n",
      "| Epoch: 254/300 | Train loss: 0.8537275937565586 | Val loss: 0.8471551612019539\n",
      "| Epoch: 255/300 | Train loss: 0.8533402198239377 | Val loss: 0.847143094572756\n",
      "| Epoch: 256/300 | Train loss: 0.8535688335435432 | Val loss: 0.8469241700238652\n",
      "| Epoch: 257/300 | Train loss: 0.8548354673803898 | Val loss: 0.8477422893047333\n",
      "| Epoch: 258/300 | Train loss: 0.8528416729809946 | Val loss: 0.8463215836220317\n",
      "| Epoch: 259/300 | Train loss: 0.8530384348149885 | Val loss: 0.845608164038923\n",
      "| Epoch: 260/300 | Train loss: 0.8525779803593954 | Val loss: 0.845660462975502\n",
      "| Epoch: 261/300 | Train loss: 0.853866800299862 | Val loss: 0.8468333739373419\n",
      "| Epoch: 262/300 | Train loss: 0.8548656957191333 | Val loss: 0.8478834860854678\n",
      "| Epoch: 263/300 | Train loss: 0.8540082385665492 | Val loss: 0.8501029031144248\n",
      "| Epoch: 264/300 | Train loss: 0.8542642396793031 | Val loss: 0.8455686709947057\n",
      "| Epoch: 265/300 | Train loss: 0.854135320897688 | Val loss: 0.8480512367354499\n",
      "| Epoch: 266/300 | Train loss: 0.8541314432495519 | Val loss: 0.8465717650122113\n",
      "| Epoch: 267/300 | Train loss: 0.8533448409615901 | Val loss: 0.8470442319909731\n",
      "| Epoch: 268/300 | Train loss: 0.8536446429135507 | Val loss: 0.847450364794996\n",
      "| Epoch: 269/300 | Train loss: 0.8534512982033846 | Val loss: 0.8482824365297953\n",
      "| Epoch: 270/300 | Train loss: 0.854175679516374 | Val loss: 0.8472791819108857\n",
      "| Epoch: 271/300 | Train loss: 0.8548811644838568 | Val loss: 0.8443980597787433\n",
      "| Epoch: 272/300 | Train loss: 0.8545721154463919 | Val loss: 0.8477495726611879\n",
      "| Epoch: 273/300 | Train loss: 0.853699144982455 | Val loss: 0.8471571513348155\n",
      "| Epoch: 274/300 | Train loss: 0.8532034976440563 | Val loss: 0.846724802421199\n",
      "| Epoch: 275/300 | Train loss: 0.8526232591846533 | Val loss: 0.846354202263885\n",
      "| Epoch: 276/300 | Train loss: 0.8542959796754938 | Val loss: 0.8480681661102507\n",
      "| Epoch: 277/300 | Train loss: 0.8541194712906554 | Val loss: 0.8455347385671403\n",
      "| Epoch: 278/300 | Train loss: 0.8543163730387102 | Val loss: 0.8462361594041189\n",
      "| Epoch: 279/300 | Train loss: 0.854438597068452 | Val loss: 0.844758201804426\n",
      "| Epoch: 280/300 | Train loss: 0.8544881676372729 | Val loss: 0.8457736066646047\n",
      "| Epoch: 281/300 | Train loss: 0.8544412399593152 | Val loss: 0.8476475088132752\n",
      "| Epoch: 282/300 | Train loss: 0.8537177606632835 | Val loss: 0.8495496544573042\n",
      "| Epoch: 283/300 | Train loss: 0.8535830711063586 | Val loss: 0.8470979556441307\n",
      "| Epoch: 284/300 | Train loss: 0.8533989178506951 | Val loss: 0.8468715341554748\n",
      "| Epoch: 285/300 | Train loss: 0.8538974908360264 | Val loss: 0.8475332028335996\n",
      "| Epoch: 286/300 | Train loss: 0.8534508165560271 | Val loss: 0.8468283340334892\n",
      "| Epoch: 287/300 | Train loss: 0.8538088869630245 | Val loss: 0.845480728480551\n",
      "| Epoch: 288/300 | Train loss: 0.854213053301761 | Val loss: 0.8453835513856676\n",
      "| Epoch: 289/300 | Train loss: 0.8535521360865811 | Val loss: 0.8480245603455437\n",
      "| Epoch: 290/300 | Train loss: 0.8541261422006707 | Val loss: 0.8473062904344665\n",
      "| Epoch: 291/300 | Train loss: 0.8535459106428581 | Val loss: 0.8479940303497844\n",
      "| Epoch: 292/300 | Train loss: 0.853578633801979 | Val loss: 0.8478012192580435\n",
      "| Epoch: 293/300 | Train loss: 0.8538576243216531 | Val loss: 0.8482765315307511\n",
      "| Epoch: 294/300 | Train loss: 0.8526648860228689 | Val loss: 0.8473463190926446\n",
      "| Epoch: 295/300 | Train loss: 0.8550042968047292 | Val loss: 0.8460239221652349\n",
      "| Epoch: 296/300 | Train loss: 0.8537424574818527 | Val loss: 0.8446162698997391\n",
      "| Epoch: 297/300 | Train loss: 0.8534076569373148 | Val loss: 0.846491234170066\n",
      "| Epoch: 298/300 | Train loss: 0.8538461967518455 | Val loss: 0.8458268923891915\n",
      "| Epoch: 299/300 | Train loss: 0.8529256203718353 | Val loss: 0.8488330120841662\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 300\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "best_val_loss = np.inf\n",
    "\n",
    "for i, epoch in enumerate(range(num_epoch)):\n",
    "    train_loss.append(train(epoch))\n",
    "    val_loss.append(evaluate())\n",
    "    \n",
    "    tb.add_scalars('Loss', {'Train loss': train_loss[i], 'Val loss': val_loss[i]}, epoch)\n",
    "    tb.add_scalars('fc_weight/abs_mean',{\n",
    "            'fc1': model.fc1.weight.abs().mean(),\n",
    "            'fc2': model.fc2.weight.abs().mean(),\n",
    "            'fc3': model.fc3.weight.abs().mean(),\n",
    "            'fc4': model.fc4.weight.abs().mean()\n",
    "        }, epoch)\n",
    "    \n",
    "    tb.add_scalars('fc_weight/abd_var',{\n",
    "            'fc1': model.fc1.weight.abs().var(),\n",
    "            'fc2': model.fc2.weight.abs().var(),\n",
    "            'fc3': model.fc3.weight.abs().var(),\n",
    "            'fc4': model.fc4.weight.abs().var()\n",
    "        }, epoch)\n",
    "        \n",
    "    tb.add_histogram('weight/fc1', model.fc1.weight, epoch)\n",
    "    tb.add_histogram('weight/fc2', model.fc2.weight, epoch)\n",
    "    tb.add_histogram('weight/fc3', model.fc3.weight, epoch)\n",
    "    tb.add_histogram('weight/fc4', model.fc4.weight, epoch)\n",
    "    print(f'| Epoch: {epoch}/{num_epoch} | Train loss: {train_loss[i]} | Val loss: {val_loss[i]}')\n",
    "        \n",
    "    if val_loss[i] < best_val_loss:\n",
    "        best_val_loss = val_loss[i]\n",
    "        best_model = model\n",
    "        \n",
    "        best_model_weights = copy.deepcopy(model.state_dict())\n",
    "        torch.save(best_model_weights, f'./best_models/seismic_net_epoch_{epoch}_val_loss_{val_loss[i]}.pt')\n",
    "\n",
    "    scheduler.step(val_loss[i])\n",
    "\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"d0ce98c8-d7bd-4d4c-983f-c3e747e2bc2e\" data-root-id=\"1002\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"68f31def-683f-4e2a-b007-5e0bf5e1d103\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1013\"}],\"center\":[{\"id\":\"1016\"},{\"id\":\"1020\"},{\"id\":\"1048\"}],\"left\":[{\"id\":\"1017\"}],\"plot_height\":250,\"renderers\":[{\"id\":\"1038\"},{\"id\":\"1053\"}],\"title\":{\"id\":\"1003\"},\"toolbar\":{\"id\":\"1028\"},\"x_range\":{\"id\":\"1005\"},\"x_scale\":{\"id\":\"1009\"},\"y_range\":{\"id\":\"1007\"},\"y_scale\":{\"id\":\"1011\"}},\"id\":\"1002\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1043\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"axis\":{\"id\":\"1017\"},\"dimension\":1,\"ticker\":null},\"id\":\"1020\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1041\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1063\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1064\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"line_color\":\"blue\",\"line_width\":2,\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1036\",\"type\":\"Line\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1021\"},{\"id\":\"1022\"},{\"id\":\"1023\"},{\"id\":\"1024\"},{\"id\":\"1025\"},{\"id\":\"1026\"}]},\"id\":\"1028\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"LinearScale\"},{\"attributes\":{\"data\":{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299],\"y\":[1.0092054500914456,1.0017368467230545,0.9931883531704283,0.9748422246230276,0.9862630708175794,0.9619420413385358,0.9545001237015975,0.9498358264304044,0.9445272004395201,0.9499975804697003,0.9392756863644248,0.9342913238625777,0.9451413938873693,0.9287768861703706,0.9231628424242924,0.9196935643229568,0.918248312724264,0.9179083673577559,0.917367945010202,0.9124631697671455,0.9110459149929515,0.9069863043333355,0.9097084718838072,0.9067695950206958,0.9057553498368515,0.9040471976263481,0.9024431006950244,0.9032394164486935,0.9099018523567601,0.8998733549787287,0.8992898085661102,0.9016416024743465,0.8998073787019963,0.8992954789546498,0.8961732266242044,0.8961443311289737,0.896285465307403,0.8937346533725136,0.8942826216680961,0.8909663683489749,0.8922805733848036,0.888295674951453,0.8942305675724096,0.8920673315985161,0.8898250015158402,0.889671818624463,0.8879985361768489,0.8894479634469016,0.8882686175798115,0.8880280484232986,0.8856495491245336,0.8857632074439735,0.8857090889361867,0.8850421422406247,0.8857884869240877,0.9033980373750653,0.882883255941826,0.8971954759798552,0.8819287302201254,0.8840648680402522,0.8812216056020636,0.881589623083148,0.8804890344017431,0.8810835154433,0.9386091757238957,0.8810409798956754,0.8800465123695239,0.8795040264464261,0.877278821301042,0.8791684221803097,0.8766794098050971,0.8773888272151612,0.8788443059252019,0.8767335996293185,0.8778791912815027,0.8747350786861621,0.8732595140473884,0.8741808780452661,0.87539466054816,0.8764490244681375,0.8747607948487265,0.8732123563164159,0.8745870565113268,0.8716684339339273,0.8734834698208591,0.8632843371023211,0.8595209240913391,0.855779552877995,0.8557761305256893,0.8539859332536396,0.858219359632124,0.853836022552691,0.8556474890625267,0.8546917643463402,0.8530705964356138,0.8525197859396014,0.8524074732211598,0.8518605924489205,0.8523864112402263,0.8516382399358248,0.8515858719223424,0.8513969293811865,0.8520088095414011,0.8505978747418053,0.8502208126218695,0.8507054123962134,0.8514000156469512,0.8500352282273141,0.847666497188702,0.8452862411214594,0.8470743170955725,0.8469059801938241,0.8470980502011484,0.8460905045793767,0.8462391138076782,0.8460513085649725,0.845924871427971,0.8454527501474347,0.8461209414298074,0.8463436229187146,0.846987980081324,0.8451621149715625,0.8471179911964818,0.846168992184756,0.846569828401532,0.846326592303159,0.8461422027203075,0.8455931703249614,0.8455471452913786,0.8460329729214049,0.8458467640374836,0.8453540113934299,0.8456695393512124,0.8451194160862973,0.8448028428512707,0.8465085544084248,0.8450781092309115,0.8448885081107156,0.8459564572886417,0.8450286160435593,0.8457201089775354,0.845854785986114,0.8446613780239172,0.8459515778641952,0.8459930392733791,0.8451314219257288,0.8452294640373765,0.8441666088606182,0.8447795255142346,0.8445412267718398,0.8465924628993922,0.8461000210360476,0.8463652801095394,0.8451847170528612,0.8460663814293711,0.8449692703129953,0.8453559570145188,0.8444374216230293,0.8453342000643412,0.8449936789378786,0.8456317328570182,0.8452127287262364,0.8445834379447134,0.8449889260425902,0.8456972421261302,0.8450764043289318,0.8452190616674591,0.8451502940110993,0.8444188220459118,0.8446538400231747,0.845064025594477,0.8451062030959547,0.8450930681144982,0.8441984435968232,0.8449884019399944,0.8451873503233257,0.8451886494954427,0.8441950660002859,0.8463065446468822,0.8452463499286719,0.8439522644929719,0.845141183075152,0.8445943092045031,0.8446629243984557,0.8449067046767786,0.8459768115428457,0.844742705110918,0.8447162433674461,0.8446898324447766,0.8436301461437292,0.8453985674339428,0.8452390210670337,0.8453717405335945,0.8449713453911898,0.8452777709877282,0.8460823111366808,0.8456415283052545,0.8456616403763755,0.8453831507448565,0.8455221422931605,0.8442219907777351,0.8451243193526017,0.8443627008220606,0.8451692474515815,0.8450303464605097,0.845750757895018,0.8439132516844231,0.8443121054716277,0.8454252387347975,0.8454335342373764,0.8448330580142507,0.8442788596738848,0.8434249296522978,0.8446558559150027,0.8460608877633747,0.8449024146063286,0.8454281066593371,0.8461203619053489,0.8443355190126519,0.8445139765739441,0.8431820881994148,0.8461539826895061,0.8443193176336455,0.8442090852218762,0.8454026778539022,0.8456725262759025,0.8456782458121317,0.8453224144483867,0.844828568634234,0.8451403833272164,0.8472523628619679,0.8452584935907732,0.8449723130778263,0.8465805904907092,0.8453070795326902,0.8452919217578152,0.8449858813955072,0.8454593189975672,0.8455921001601637,0.845029882589976,0.8449102893210294,0.8441951046910202,0.8443635271306623,0.8454007014893649,0.8450774349664387,0.8457823721986067,0.8449064359330294,0.8459431932683577,0.8447637028861464,0.8447553797772056,0.8451278889388368,0.8444428782714041,0.8456182726642542,0.8455656710423921,0.8453540270788628,0.8457212759737383,0.8450636817697893,0.8463861873275356,0.8436041463885391,0.8447654186633595,0.8442097097112421,0.8446179214276766,0.8450618062103004,0.8447502834755077,0.8450800301735861,0.845384587112226,0.8449953804936325,0.8444418325758817,0.8448071688936468,0.8458657710175765,0.8447927838877628,0.8449448014560499,0.8448868471279479,0.8452276323970995,0.8442590686312893,0.8448748709862692,0.8445814678543493,0.8450902154571132,0.8458586092580829,0.8448973321078116,0.8451374219174971,0.843892514705658,0.8448433127319603,0.8445301940566615,0.8450900703145746,0.8444907303442035,0.844975011181413,0.8451015558159142,0.8442305820030078,0.8458450756574932,0.8452626165590789,0.8446421046006052,0.8449489168953477,0.8456017289245338,0.8446650651463291,0.8459586055655228,0.8447094814819202,0.8461833512573912,0.8449035318274247,0.844831116994222]},\"selected\":{\"id\":\"1046\"},\"selection_policy\":{\"id\":\"1047\"}},\"id\":\"1035\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"line_color\":\"red\",\"line_width\":2,\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1051\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"1007\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1005\",\"type\":\"DataRange1d\"},{\"attributes\":{\"source\":{\"id\":\"1035\"}},\"id\":\"1039\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1021\",\"type\":\"PanTool\"},{\"attributes\":{\"label\":{\"value\":\"Train RMSE\"},\"renderers\":[{\"id\":\"1038\"}]},\"id\":\"1049\",\"type\":\"LegendItem\"},{\"attributes\":{},\"id\":\"1022\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"items\":[{\"id\":\"1049\"},{\"id\":\"1065\"}]},\"id\":\"1048\",\"type\":\"Legend\"},{\"attributes\":{\"axis\":{\"id\":\"1013\"},\"ticker\":null},\"id\":\"1016\",\"type\":\"Grid\"},{\"attributes\":{\"overlay\":{\"id\":\"1027\"}},\"id\":\"1023\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"blue\",\"line_width\":2,\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1037\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"1026\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"1024\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"BasicTicker\"},{\"attributes\":{\"source\":{\"id\":\"1050\"}},\"id\":\"1054\",\"type\":\"CDSView\"},{\"attributes\":{\"formatter\":{\"id\":\"1041\"},\"ticker\":{\"id\":\"1018\"}},\"id\":\"1017\",\"type\":\"LinearAxis\"},{\"attributes\":{\"data_source\":{\"id\":\"1050\"},\"glyph\":{\"id\":\"1051\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1052\"},\"selection_glyph\":null,\"view\":{\"id\":\"1054\"}},\"id\":\"1053\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"text\":\"Minimum validation loss: 0.83555911315812\"},\"id\":\"1003\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"LinearScale\"},{\"attributes\":{\"formatter\":{\"id\":\"1043\"},\"ticker\":{\"id\":\"1014\"}},\"id\":\"1013\",\"type\":\"LinearAxis\"},{\"attributes\":{\"label\":{\"value\":\"Val RMSE\"},\"renderers\":[{\"id\":\"1053\"}]},\"id\":\"1065\",\"type\":\"LegendItem\"},{\"attributes\":{\"data_source\":{\"id\":\"1035\"},\"glyph\":{\"id\":\"1036\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1037\"},\"selection_glyph\":null,\"view\":{\"id\":\"1039\"}},\"id\":\"1038\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1027\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"1046\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1025\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1018\",\"type\":\"BasicTicker\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"red\",\"line_width\":2,\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1052\",\"type\":\"Line\"},{\"attributes\":{\"data\":{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299],\"y\":[0.9918600660231378,0.9894019779231813,0.9738732518421279,0.9639922868874338,0.9608128691713015,0.9533619201845593,0.9463412703739272,0.9425644079844157,0.9396407960189713,0.9360583325227102,0.9335350725385878,0.9316640479697121,0.9239320208628973,0.9198826973636945,0.91279110726383,0.9140643576780955,0.9062521449393697,0.9032143387529585,0.9063806061943372,0.8994175104631318,0.8989156641893916,0.8982280004355643,0.8977393681804339,0.8960270384947459,0.9002801287505362,0.8919637716478772,0.893084747923745,0.8933182400133874,0.8928947415616777,0.8925669458177354,0.8916400439209409,0.8926351699564192,0.8921666186716821,0.8947626559270753,0.8945681146449513,0.8866622257563803,0.88708436406321,0.8847973826858733,0.8854888238840632,0.8863415610459116,0.8838168250189887,0.8840718650155597,0.8965573608875275,0.8782855719327927,0.8799217508898841,0.8830299634072516,0.8795935577816434,0.8781033009290695,0.873311890496148,0.8730578207307391,0.8801035591297679,0.8775568844543563,0.8806138734022776,0.8777626148528523,0.8723201751708984,0.8749061930510733,0.8740607665644752,0.8708585070239173,0.8701309536894163,0.8746522905098068,0.8992134307821592,0.8753400850627158,0.871017187833786,0.871905172864596,0.868545743326346,0.8702801118294398,0.8764800089928839,0.8700465551680989,0.8654097293814024,0.8679963582091861,0.8733970663613744,0.8677593751086129,0.8668457162049081,0.8596534066730075,0.8659513791402181,0.8654833940996064,0.8634917239348093,0.8652275477846464,0.87893029799064,0.8694452436433898,0.8662177845835686,0.8706043461958567,0.8654837136467298,0.8688039382298788,0.8675620804230372,0.8568978483478228,0.854675909711255,0.8483747426006529,0.8493473264906142,0.8541225848926438,0.8474341672327783,0.8485453923543295,0.8474188612567054,0.8498144307070308,0.849444286690818,0.8486993486682574,0.842340183754762,0.8456384787956873,0.8438537195324898,0.8470913271109263,0.8468489895264307,0.8460196943746673,0.8456474103861384,0.8450293201539252,0.8476159464981821,0.8438972466521792,0.8459367586506737,0.8435506390200721,0.8401188477873802,0.8394990886251131,0.8406372823648982,0.8410652685496542,0.8394991623030769,0.841128408908844,0.8404696898327934,0.839950201412042,0.8383851225177447,0.8388161410888036,0.8403824095924696,0.8397888301147355,0.8410537184940444,0.8390427620874511,0.8394589399298032,0.8425861696402231,0.8395882629685931,0.840982135799196,0.8417993229296472,0.8406561927662956,0.8377212948269315,0.8400893037517866,0.8368155368500285,0.837348980208238,0.8408898032373853,0.8397204892502891,0.8389232009649277,0.8392629283997748,0.8379215879572762,0.8387794461515214,0.8377403343717257,0.8393982268042035,0.8383997223443456,0.8398060864872403,0.8386108941502042,0.8386156037449837,0.8377759580810865,0.8395302784111764,0.8396480232477188,0.8379402673906751,0.8384502033392588,0.8382170307967398,0.8377711019582219,0.838392597105768,0.8392620467477374,0.8379710598124398,0.8406600927313169,0.8384195930427976,0.8382573111189736,0.8375740092661645,0.8408168504635493,0.8397249339355363,0.839406614502271,0.8382135894563463,0.8376037635736995,0.8358085263106558,0.8381010161505805,0.8392424053615994,0.8379580626885096,0.8381584071450763,0.8372964014609655,0.8382785204384062,0.8385740982161628,0.8378238280614217,0.8382356622152858,0.8375355965561337,0.83555911315812,0.8397494562798076,0.8381997479332818,0.8375376264254252,0.836809343761868,0.8369869217276573,0.8391540009114478,0.8403643841544787,0.8379015276829401,0.8398974322610431,0.8405781040589014,0.8375694271590974,0.8378775376412604,0.8366926660140356,0.8395542386505339,0.8378257213367356,0.8382357060909271,0.838949504825804,0.8399125478333898,0.8376648119754262,0.8378705506523451,0.8374771889713075,0.8376383499966727,0.8380813582075967,0.8356138070424398,0.8387933952940835,0.8358989953994751,0.8372148937649198,0.840271501077546,0.8394638432396783,0.8395540722542338,0.8364293219314681,0.8375608068373468,0.8362765635053316,0.8385462231106229,0.8380749954117669,0.8398629509740405,0.8382964953780174,0.8383944125639068,0.8364925980567932,0.8372401338484552,0.8384251197179159,0.8372068819072511,0.838432146443261,0.8383734830551677,0.8387231495645311,0.837763723399904,0.8401311578022109,0.8399380130900277,0.8401056047942903,0.837349048919148,0.8382548234528966,0.839473172194428,0.8390898861818843,0.8401019622882208,0.8371122413211398,0.8377185753650136,0.8394844515456094,0.8402179115348392,0.8407891549997859,0.837457058330377,0.837117514676518,0.8367796026998096,0.8383366730478075,0.8366265537010299,0.8394273792703947,0.8380860388278961,0.8369248517685466,0.8400842050711314,0.8372123357322481,0.8396945123871168,0.8378070369362831,0.8383478199442228,0.8374718311760161,0.8392602685425017,0.8379289582371712,0.83918856167131,0.8383365935749478,0.8396413036518626,0.8360959796441926,0.8389135946830114,0.838894835776753,0.8386329271727138,0.8395864475104544,0.8381570329268774,0.8375163732303513,0.8375559738940663,0.8386850439839892,0.8396194817291366,0.8417599085304472,0.8374061220222049,0.8397337703241242,0.8383614958988296,0.8388816946082644,0.8391493368479941,0.8399429503414366,0.8390708275967174,0.836471894548999,0.8396777427858777,0.8388059586286545,0.8386466056108475,0.838072184059355,0.8397712931036949,0.837254770927959,0.8381735955675443,0.8365379232499335,0.8377810774577988,0.8393312502238486,0.8411488938662741,0.8391553850637542,0.8390081524848938,0.8393085988031493,0.8386668529775407,0.8373652125398318,0.8370435858766238,0.8397417200936211,0.8391668995221456,0.8398285218411021,0.8395751218001047,0.8399090220530828,0.8391070655650563,0.8379320485724343,0.8365263963739077,0.8383188347021738,0.8379322778847482,0.8406770428021749]},\"selected\":{\"id\":\"1063\"},\"selection_policy\":{\"id\":\"1064\"}},\"id\":\"1050\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1047\",\"type\":\"UnionRenderers\"}],\"root_ids\":[\"1002\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.1\"}};\n",
       "  var render_items = [{\"docid\":\"68f31def-683f-4e2a-b007-5e0bf5e1d103\",\"root_ids\":[\"1002\"],\"roots\":{\"1002\":\"d0ce98c8-d7bd-4d4c-983f-c3e747e2bc2e\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1002"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "title = f'Minimum validation loss: {np.min(val_loss)}'\n",
    "p = figure(title = title, plot_width=600, plot_height=250)\n",
    "x = np.arange(len(train_loss))\n",
    "p.line(x, train_loss, line_width=2, color = 'blue', legend_label = 'Train RMSE')\n",
    "p.line(x, val_loss, line_width=2, color = 'red', legend_label = 'Val RMSE')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
