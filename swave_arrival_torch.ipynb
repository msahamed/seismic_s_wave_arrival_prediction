{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os, random\n",
    "import h5py, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "### Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "### Torchvision\n",
    "from torchsummary import summary\n",
    "import torchvision.datasets as dest\n",
    "import torchvision.transforms as transformers\n",
    "\n",
    "### Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.1.min.js\": \"qkRvDQVAIfzsJo40iRBbxt6sttt0hv4lh74DG7OK4MCHv4C5oohXYoHUM5W11uqS\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.1.min.js\": \"Sb7Mr06a9TNlet/GEBeKaf5xH3eb6AlCzwjtU82wNPyDrnfoiVl26qnvlKjmcAd+\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.1.min.js\": \"HaJ15vgfmcfRtB4c4YBOI4f1MUujukqInOWVqZJZZGK7Q+ivud0OKGSTn/Vm2iso\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.1.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.1.min.js\": \"qkRvDQVAIfzsJo40iRBbxt6sttt0hv4lh74DG7OK4MCHv4C5oohXYoHUM5W11uqS\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.1.min.js\": \"Sb7Mr06a9TNlet/GEBeKaf5xH3eb6AlCzwjtU82wNPyDrnfoiVl26qnvlKjmcAd+\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.1.min.js\": \"HaJ15vgfmcfRtB4c4YBOI4f1MUujukqInOWVqZJZZGK7Q+ivud0OKGSTn/Vm2iso\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### seed_everythin\n",
    "seed = 1987\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "ngpu = 1\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'selected_data.pkl'\n",
    "outfile = open(filename,'rb')\n",
    "data = pickle.load(outfile)\n",
    "xdata = np.array(data['data'], dtype = 'float32')\n",
    "ylablel = np.array(data['labels'], dtype = 'float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((145864, 50, 3), (36466, 50, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(xdata, ylablel, test_size=0.2)\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xscalers = {}\n",
    "for i in range(xdata.shape[2]):\n",
    "    xscalers[i] = StandardScaler()\n",
    "    X_train[:, :, i] = xscalers[i].fit_transform(X_train[:, :, i])\n",
    "\n",
    "yscaler = StandardScaler()\n",
    "y_train = yscaler.fit_transform(y_train.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "for i in range(xdata.shape[2]):\n",
    "    X_val[:, :, i] = xscalers[i].transform(X_val[:, :, i])\n",
    "\n",
    "y_val = yscaler.transform(y_val.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.706501, 7.02627)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.max(), y_train.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset\n",
    "class SeismicDataset(Dataset):\n",
    "    def __init__(self, method):\n",
    "        if method == 'train':\n",
    "            self.xdata = torch.Tensor(X_train)\n",
    "            self.ylablel = torch.Tensor(y_train)\n",
    "        elif method == 'val':\n",
    "            self.xdata = torch.Tensor(X_val)\n",
    "            self.ylablel = torch.Tensor(y_val)\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.xdata)\n",
    "        \n",
    "    def __getitem__(self, indx):\n",
    "        return self.xdata[indx], self.ylablel[indx]\n",
    "        \n",
    "### Dataloader\n",
    "batch_size = 512\n",
    "num_workers = 1\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    SeismicDataset('train'),\n",
    "    batch_size= batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = num_workers\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    SeismicDataset('val'),\n",
    "    batch_size= batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels,  bias = False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.batchnorm(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1_pool):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        self.branch1 = ConvBlock(in_channels, out_1x1, kernel_size = 1)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBlock(in_channels, red_3x3, kernel_size = 1),\n",
    "            ConvBlock(red_3x3, out_3x3, kernel_size = 3, padding = 1)\n",
    "        )\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            ConvBlock(in_channels, red_5x5, kernel_size = 1),\n",
    "            ConvBlock(red_5x5, out_5x5, kernel_size = 5, padding = 2)\n",
    "        )\n",
    "        \n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=3, stride = 1, padding = 1),\n",
    "            ConvBlock(in_channels, out_1x1_pool, kernel_size = 1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat(\n",
    "            [self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], 1\n",
    "        )\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeismicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SeismicNet, self).__init__()\n",
    "        self.conv1 = ConvBlock(3, 192, kernel_size = 2, stride = 2)\n",
    "        self.inception_1a = InceptionBlock(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception_1b = InceptionBlock(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=3, stride=2, padding = 1)\n",
    "        \n",
    "        self.averagepool1 = nn.AvgPool1d(kernel_size= 7, stride= 1)\n",
    "        self.fc1 = nn.Linear(3360, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "        self.dropout1 = nn.Dropout2d(p = 0.25)\n",
    "        self.dropout2 = nn.Dropout2d(p = 0.20)\n",
    "        self.dropout3 = nn.Dropout2d(p = 0.15)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.inception_1a(x)\n",
    "        x = self.inception_1b(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.averagepool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(150, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.35),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.35),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = Variable(torch.flatten(x, start_dim = 1))\n",
    "        x = self.main(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel().to(device) #SeismicNet().to(device)\n",
    "if (device.type == 'cuda' and (ngpu> 1)):\n",
    "    model = nn.DataParallel(model, list(range(ngpu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model, input_size=(3, 50))\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=10, factor = 0.15, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for j, (data, label) in enumerate(train_dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device).view(data.shape[0], data.shape[2], data.shape[1])\n",
    "        y = label.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    mean_loss = sum(losses)/len(losses)\n",
    "    return mean_loss\n",
    "        \n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        for j, (data, label) in enumerate(val_dataloader, 0):\n",
    "            data = data.to(device).view(data.shape[0], data.shape[2], data.shape[1])\n",
    "            y = label.to(device)\n",
    "            output = model(data)\n",
    "            losses.append(criterion(output, y).item())\n",
    "    return sum(losses)/len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 0/500 | Train loss: 1.0436619532735725 | Val loss: 0.9891992029216554\n",
      "| Epoch: 1/500 | Train loss: 0.9935212423926906 | Val loss: 0.971724627746476\n",
      "| Epoch: 2/500 | Train loss: 0.9795651847856086 | Val loss: 0.9667215968171755\n",
      "| Epoch: 3/500 | Train loss: 1.0038976186200192 | Val loss: 0.9596497606900003\n",
      "| Epoch: 4/500 | Train loss: 1.8748206607082434 | Val loss: 0.9528772864076827\n",
      "| Epoch: 5/500 | Train loss: 1.1947972155453865 | Val loss: 0.9452766593959596\n",
      "| Epoch: 6/500 | Train loss: 0.984333312302305 | Val loss: 0.9357115195857154\n",
      "| Epoch: 7/500 | Train loss: 0.9474844940921716 | Val loss: 0.9342307878865136\n",
      "| Epoch: 8/500 | Train loss: 0.9675219717778658 | Val loss: 0.9270032155844901\n",
      "| Epoch: 9/500 | Train loss: 0.9283860269345735 | Val loss: 0.9121393660704294\n",
      "| Epoch: 10/500 | Train loss: 0.9347056719294765 | Val loss: 0.9076041819320785\n",
      "| Epoch: 11/500 | Train loss: 0.9180914046471579 | Val loss: 0.900248609483242\n",
      "| Epoch: 12/500 | Train loss: 0.9132980635291652 | Val loss: 0.8966818865802553\n",
      "| Epoch: 13/500 | Train loss: 0.9348597225389983 | Val loss: 0.8884450056486659\n",
      "| Epoch: 14/500 | Train loss: 0.9204469992403399 | Val loss: 0.885323666036129\n",
      "| Epoch: 15/500 | Train loss: 0.9015404891549496 | Val loss: 0.8802117564611964\n",
      "| Epoch: 16/500 | Train loss: 0.8998967578536585 | Val loss: 0.8769554388191965\n",
      "| Epoch: 17/500 | Train loss: 0.9630407996344985 | Val loss: 0.8764241279827224\n",
      "| Epoch: 18/500 | Train loss: 0.9327326550818327 | Val loss: 0.8754383913344808\n",
      "| Epoch: 19/500 | Train loss: 0.8912851961035477 | Val loss: 0.8722817061675919\n",
      "| Epoch: 20/500 | Train loss: 0.9292754641750403 | Val loss: 0.8685354085432159\n",
      "| Epoch: 21/500 | Train loss: 0.8841975465155485 | Val loss: 0.8702588139308823\n",
      "| Epoch: 22/500 | Train loss: 0.9122569983465629 | Val loss: 0.8630760171347194\n",
      "| Epoch: 23/500 | Train loss: 0.8819439860812405 | Val loss: 0.8585275808970133\n",
      "| Epoch: 24/500 | Train loss: 0.8789211994723269 | Val loss: 0.8643593672249053\n",
      "| Epoch: 25/500 | Train loss: 0.8810785335406922 | Val loss: 0.8612079843878746\n",
      "| Epoch: 26/500 | Train loss: 0.8904538290542469 | Val loss: 0.8571736911932627\n",
      "| Epoch: 27/500 | Train loss: 0.8764030151199876 | Val loss: 0.8548286532362303\n",
      "| Epoch: 28/500 | Train loss: 0.8795463432345474 | Val loss: 0.8562341936760478\n",
      "| Epoch: 29/500 | Train loss: 0.8718011027888248 | Val loss: 0.8561656632357173\n",
      "| Epoch: 30/500 | Train loss: 0.8728617854285659 | Val loss: 0.8514753041995896\n",
      "| Epoch: 31/500 | Train loss: 0.869499709313376 | Val loss: 0.8571315896179941\n",
      "| Epoch: 32/500 | Train loss: 0.9268003685432568 | Val loss: 0.8561173031727473\n",
      "| Epoch: 33/500 | Train loss: 0.8697027695806403 | Val loss: 0.8550779206885232\n",
      "| Epoch: 34/500 | Train loss: 0.8747944497225577 | Val loss: 0.8517245219813453\n",
      "| Epoch: 35/500 | Train loss: 0.9082113847397921 | Val loss: 0.8543149332205454\n",
      "| Epoch: 36/500 | Train loss: 0.8671996066444798 | Val loss: 0.8481366179055638\n",
      "| Epoch: 37/500 | Train loss: 0.9097952359601071 | Val loss: 0.8521243400043912\n",
      "| Epoch: 38/500 | Train loss: 0.8674773281080681 | Val loss: 0.8451328244474199\n",
      "| Epoch: 39/500 | Train loss: 0.862379743341814 | Val loss: 0.8425084112419022\n",
      "| Epoch: 40/500 | Train loss: 0.8623036694108395 | Val loss: 0.8468005210161209\n",
      "| Epoch: 41/500 | Train loss: 0.8615681577147098 | Val loss: 0.8429669456349479\n",
      "| Epoch: 42/500 | Train loss: 0.862846366982711 | Val loss: 0.8412027375565635\n",
      "| Epoch: 43/500 | Train loss: 0.859393680932229 | Val loss: 0.8420244447059102\n",
      "| Epoch: 44/500 | Train loss: 0.8600960461716903 | Val loss: 0.8535961574978299\n",
      "| Epoch: 45/500 | Train loss: 0.8930217768016614 | Val loss: 0.854674271411366\n",
      "| Epoch: 46/500 | Train loss: 0.8599620555576525 | Val loss: 0.8453871541553073\n",
      "| Epoch: 47/500 | Train loss: 0.8562753767297979 | Val loss: 0.8407466444704268\n",
      "| Epoch: 48/500 | Train loss: 0.8554772008929337 | Val loss: 0.8429219606849883\n",
      "| Epoch: 49/500 | Train loss: 0.8586449152544925 | Val loss: 0.8493770112593969\n",
      "| Epoch: 50/500 | Train loss: 0.8558307028653329 | Val loss: 0.8445164528157976\n",
      "| Epoch: 51/500 | Train loss: 0.8600630965149193 | Val loss: 0.8445945249663459\n",
      "| Epoch: 52/500 | Train loss: 0.857436942635921 | Val loss: 0.8446269159515699\n",
      "| Epoch: 53/500 | Train loss: 0.8626191440381502 | Val loss: 0.8487551112969717\n",
      "| Epoch: 54/500 | Train loss: 0.8543193208543878 | Val loss: 0.8372432440519333\n",
      "| Epoch: 55/500 | Train loss: 0.854023722180149 | Val loss: 0.8456824264592595\n",
      "| Epoch: 56/500 | Train loss: 0.8542293262063412 | Val loss: 0.8379013670815362\n",
      "| Epoch: 57/500 | Train loss: 0.8580778469119156 | Val loss: 0.8506973046395514\n",
      "| Epoch: 58/500 | Train loss: 0.8559066822654322 | Val loss: 0.8398593887686729\n",
      "| Epoch: 59/500 | Train loss: 0.8511840427130983 | Val loss: 0.8421588018536568\n",
      "| Epoch: 60/500 | Train loss: 0.8539232651392619 | Val loss: 0.8420484695169661\n",
      "| Epoch: 61/500 | Train loss: 0.8519134032098871 | Val loss: 0.8347638878557417\n",
      "| Epoch: 62/500 | Train loss: 0.855339132902915 | Val loss: 0.83831577665276\n",
      "| Epoch: 63/500 | Train loss: 0.84799268015644 | Val loss: 0.8410073733992047\n",
      "| Epoch: 64/500 | Train loss: 0.8492068332538271 | Val loss: 0.8380696227153143\n",
      "| Epoch: 65/500 | Train loss: 0.8501809559370342 | Val loss: 0.838368738691012\n",
      "| Epoch: 66/500 | Train loss: 0.848962158069276 | Val loss: 0.8366197107566727\n",
      "| Epoch: 67/500 | Train loss: 0.8471707785338686 | Val loss: 0.8404886747399966\n",
      "| Epoch: 68/500 | Train loss: 0.8472670109648454 | Val loss: 0.8326581633753247\n",
      "| Epoch: 69/500 | Train loss: 0.84644939334769 | Val loss: 0.8334347241454654\n",
      "| Epoch: 70/500 | Train loss: 0.8466696553062975 | Val loss: 0.8329087868332863\n",
      "| Epoch: 71/500 | Train loss: 0.8474133663010179 | Val loss: 0.8361445789535841\n",
      "| Epoch: 72/500 | Train loss: 0.8468528159877711 | Val loss: 0.8366755396127701\n",
      "| Epoch: 73/500 | Train loss: 0.8786190459602757 | Val loss: 0.836699946059121\n",
      "| Epoch: 74/500 | Train loss: 0.8449145764635321 | Val loss: 0.8382479217317369\n",
      "| Epoch: 75/500 | Train loss: 0.860667571896001 | Val loss: 0.8344531067543559\n",
      "| Epoch: 76/500 | Train loss: 1.735330591703716 | Val loss: 0.8352643408709102\n",
      "| Epoch: 77/500 | Train loss: 0.8458822298468205 | Val loss: 0.8375139128830698\n",
      "| Epoch: 78/500 | Train loss: 0.8476606084589373 | Val loss: 0.8334412020113733\n",
      "| Epoch: 79/500 | Train loss: 0.8448669561168604 | Val loss: 0.8378340154886246\n",
      "Epoch    80: reducing learning rate of group 0 to 1.5000e-04.\n",
      "| Epoch: 80/500 | Train loss: 0.8368060597202234 | Val loss: 0.8265995606780052\n",
      "| Epoch: 81/500 | Train loss: 0.8327734802898608 | Val loss: 0.8220754265785217\n",
      "| Epoch: 82/500 | Train loss: 0.8304179013821117 | Val loss: 0.819302000105381\n",
      "| Epoch: 83/500 | Train loss: 0.8305105855590419 | Val loss: 0.8175492443972163\n",
      "| Epoch: 84/500 | Train loss: 0.827529684075138 | Val loss: 0.815182606379191\n",
      "| Epoch: 85/500 | Train loss: 0.8280392895665085 | Val loss: 0.8187589570879936\n",
      "| Epoch: 86/500 | Train loss: 0.827083410714802 | Val loss: 0.8173054994808303\n",
      "| Epoch: 87/500 | Train loss: 0.8420682555750797 | Val loss: 0.8193461944659551\n",
      "| Epoch: 88/500 | Train loss: 0.826594753014414 | Val loss: 0.8181081554955907\n",
      "| Epoch: 89/500 | Train loss: 0.8289291155965705 | Val loss: 0.8218358423974779\n",
      "| Epoch: 90/500 | Train loss: 0.8259862922785575 | Val loss: 0.8166857485969862\n",
      "| Epoch: 91/500 | Train loss: 0.8262523653214437 | Val loss: 0.8184714424941275\n",
      "| Epoch: 92/500 | Train loss: 0.8246957126416659 | Val loss: 0.8195383176207542\n",
      "| Epoch: 93/500 | Train loss: 0.8268041194530955 | Val loss: 0.8120131583677398\n",
      "| Epoch: 94/500 | Train loss: 0.8252277234144378 | Val loss: 0.8140411981277995\n",
      "| Epoch: 95/500 | Train loss: 0.8259512721446522 | Val loss: 0.8169360169106059\n",
      "| Epoch: 96/500 | Train loss: 0.8263343382299992 | Val loss: 0.8180885000361336\n",
      "| Epoch: 97/500 | Train loss: 0.8242613376232616 | Val loss: 0.8178201301230325\n",
      "| Epoch: 98/500 | Train loss: 0.8243161833077146 | Val loss: 0.8171801666418711\n",
      "| Epoch: 99/500 | Train loss: 0.8283751889279014 | Val loss: 0.8148408283789953\n",
      "| Epoch: 100/500 | Train loss: 0.8231352239324335 | Val loss: 0.815776589843962\n",
      "| Epoch: 101/500 | Train loss: 0.8249388905993679 | Val loss: 0.8147634317477545\n",
      "| Epoch: 102/500 | Train loss: 0.8237478373343484 | Val loss: 0.8139332061012586\n",
      "| Epoch: 103/500 | Train loss: 0.8244009327470211 | Val loss: 0.8157869925101598\n",
      "| Epoch: 104/500 | Train loss: 0.8245394365829334 | Val loss: 0.8144871327612135\n",
      "Epoch   105: reducing learning rate of group 0 to 2.2500e-05.\n",
      "| Epoch: 105/500 | Train loss: 0.8229387203852335 | Val loss: 0.8114332200752364\n",
      "| Epoch: 106/500 | Train loss: 0.8207232705333777 | Val loss: 0.8104588836431503\n",
      "| Epoch: 107/500 | Train loss: 0.8196674957610013 | Val loss: 0.8112310419480006\n",
      "| Epoch: 108/500 | Train loss: 0.8192489780877766 | Val loss: 0.8117580289642016\n",
      "| Epoch: 109/500 | Train loss: 0.8194138413981388 | Val loss: 0.8118547052145004\n",
      "| Epoch: 110/500 | Train loss: 0.8196324340084142 | Val loss: 0.8135812042487992\n",
      "| Epoch: 111/500 | Train loss: 0.819010499485752 | Val loss: 0.8109395752350489\n",
      "| Epoch: 112/500 | Train loss: 0.8193011534841437 | Val loss: 0.8098170227474637\n",
      "| Epoch: 113/500 | Train loss: 0.8192933333547492 | Val loss: 0.810955742167102\n",
      "| Epoch: 114/500 | Train loss: 0.8199579086220056 | Val loss: 0.8104084001647102\n",
      "| Epoch: 115/500 | Train loss: 0.8195493907259221 | Val loss: 0.8116664547059271\n",
      "| Epoch: 116/500 | Train loss: 0.8197121032497339 | Val loss: 0.8107312271992365\n",
      "| Epoch: 117/500 | Train loss: 0.8203877812937687 | Val loss: 0.813602606455485\n",
      "| Epoch: 118/500 | Train loss: 0.8194223813843309 | Val loss: 0.8117449151145087\n",
      "| Epoch: 119/500 | Train loss: 0.8189321685255619 | Val loss: 0.814445748925209\n",
      "| Epoch: 120/500 | Train loss: 0.8192384458424752 | Val loss: 0.8095029062694974\n",
      "| Epoch: 121/500 | Train loss: 0.8242258619843867 | Val loss: 0.8117578766412206\n",
      "| Epoch: 122/500 | Train loss: 0.8185957151546813 | Val loss: 0.812056690454483\n",
      "| Epoch: 123/500 | Train loss: 0.8200839373103359 | Val loss: 0.8105647092064222\n",
      "| Epoch: 124/500 | Train loss: 0.817942215894398 | Val loss: 0.8110146133436097\n",
      "| Epoch: 125/500 | Train loss: 0.8193064045487789 | Val loss: 0.8105830550193787\n",
      "| Epoch: 126/500 | Train loss: 0.8241392503704941 | Val loss: 0.8105695959594514\n",
      "| Epoch: 127/500 | Train loss: 0.8230806737615352 | Val loss: 0.8083370816376474\n",
      "| Epoch: 128/500 | Train loss: 0.8190653238380164 | Val loss: 0.8111667144629691\n",
      "| Epoch: 129/500 | Train loss: 0.8190174567072015 | Val loss: 0.8130114790466096\n",
      "| Epoch: 130/500 | Train loss: 0.81871643359201 | Val loss: 0.8085205430785815\n",
      "| Epoch: 131/500 | Train loss: 0.8185219823268421 | Val loss: 0.810083442264133\n",
      "| Epoch: 132/500 | Train loss: 0.8184723337491353 | Val loss: 0.8096034692393409\n",
      "| Epoch: 133/500 | Train loss: 0.819226486641064 | Val loss: 0.8095726586050458\n",
      "| Epoch: 134/500 | Train loss: 0.817765172113452 | Val loss: 0.8106825939483113\n",
      "| Epoch: 135/500 | Train loss: 0.8187943483653821 | Val loss: 0.8104426422052913\n",
      "| Epoch: 136/500 | Train loss: 0.8186209203904136 | Val loss: 0.80710702141126\n",
      "| Epoch: 137/500 | Train loss: 0.8193806353368257 | Val loss: 0.8083303040928311\n",
      "| Epoch: 138/500 | Train loss: 0.8195348647602817 | Val loss: 0.8117299990521537\n",
      "| Epoch: 139/500 | Train loss: 0.8175035489232917 | Val loss: 0.8090188552935919\n",
      "| Epoch: 140/500 | Train loss: 0.8208042690628453 | Val loss: 0.8086754356821378\n",
      "| Epoch: 141/500 | Train loss: 0.8183975962170383 | Val loss: 0.8107193253106542\n",
      "| Epoch: 142/500 | Train loss: 0.8199484285555387 | Val loss: 0.8088333987527423\n",
      "| Epoch: 143/500 | Train loss: 0.9687178712142142 | Val loss: 0.8091061520907614\n",
      "| Epoch: 144/500 | Train loss: 0.8187779029210408 | Val loss: 0.8103059753775597\n",
      "| Epoch: 145/500 | Train loss: 0.8207050024417408 | Val loss: 0.8097229194309976\n",
      "| Epoch: 146/500 | Train loss: 0.8180256306079396 | Val loss: 0.8102441603938738\n",
      "| Epoch: 147/500 | Train loss: 0.8179677413220992 | Val loss: 0.8092459183600214\n",
      "Epoch   148: reducing learning rate of group 0 to 3.3750e-06.\n",
      "| Epoch: 148/500 | Train loss: 0.8197528956229226 | Val loss: 0.8102330358492004\n",
      "| Epoch: 149/500 | Train loss: 0.8194755750789977 | Val loss: 0.8106954387492604\n",
      "| Epoch: 150/500 | Train loss: 0.8183800824901514 | Val loss: 0.8087582116325697\n",
      "| Epoch: 151/500 | Train loss: 0.8172853375736036 | Val loss: 0.8099706338511573\n",
      "| Epoch: 152/500 | Train loss: 0.8181895101279543 | Val loss: 0.811319408317407\n",
      "| Epoch: 153/500 | Train loss: 0.8160826026347645 | Val loss: 0.8080961638026767\n",
      "| Epoch: 154/500 | Train loss: 0.8183016569990861 | Val loss: 0.8087351479464107\n",
      "| Epoch: 155/500 | Train loss: 0.8189750608644988 | Val loss: 0.8099106798569361\n",
      "| Epoch: 156/500 | Train loss: 0.8182623024572406 | Val loss: 0.8109414329131445\n",
      "| Epoch: 157/500 | Train loss: 0.8178768203969587 | Val loss: 0.8087849401765399\n",
      "| Epoch: 158/500 | Train loss: 0.8172868906405935 | Val loss: 0.808569529818164\n",
      "Epoch   159: reducing learning rate of group 0 to 5.0625e-07.\n",
      "| Epoch: 159/500 | Train loss: 0.8181778861765276 | Val loss: 0.8108319325579537\n",
      "| Epoch: 160/500 | Train loss: 0.8184802327239723 | Val loss: 0.8069951368702782\n",
      "| Epoch: 161/500 | Train loss: 0.8182060415284675 | Val loss: 0.8106676803694831\n",
      "| Epoch: 162/500 | Train loss: 0.818929270066713 | Val loss: 0.808936308655474\n",
      "| Epoch: 163/500 | Train loss: 0.8171102726668642 | Val loss: 0.8114773655931155\n",
      "| Epoch: 164/500 | Train loss: 0.8174639990455226 | Val loss: 0.8101602163579729\n",
      "| Epoch: 165/500 | Train loss: 0.8176174916719136 | Val loss: 0.808842228518592\n",
      "| Epoch: 166/500 | Train loss: 0.8177205123399434 | Val loss: 0.8087180066439841\n",
      "| Epoch: 167/500 | Train loss: 0.8177638482629207 | Val loss: 0.8098152892457114\n",
      "| Epoch: 168/500 | Train loss: 0.8176002123899627 | Val loss: 0.8081222176551819\n",
      "| Epoch: 169/500 | Train loss: 0.8173456139731825 | Val loss: 0.8095281057887607\n",
      "| Epoch: 170/500 | Train loss: 0.8168584783871968 | Val loss: 0.8090020906594064\n",
      "| Epoch: 171/500 | Train loss: 0.817795337292186 | Val loss: 0.808978353937467\n",
      "Epoch   172: reducing learning rate of group 0 to 7.5937e-08.\n",
      "| Epoch: 172/500 | Train loss: 0.8206070841404429 | Val loss: 0.8088409602642059\n",
      "| Epoch: 173/500 | Train loss: 0.8181620175378365 | Val loss: 0.809088389078776\n",
      "| Epoch: 174/500 | Train loss: 0.8181566805170294 | Val loss: 0.8073758019341363\n",
      "| Epoch: 175/500 | Train loss: 0.8172777276290091 | Val loss: 0.8106712425748507\n",
      "| Epoch: 176/500 | Train loss: 0.8169437364528054 | Val loss: 0.808367076019446\n",
      "| Epoch: 177/500 | Train loss: 0.818173485053213 | Val loss: 0.8084518503811624\n",
      "| Epoch: 178/500 | Train loss: 0.816826653898808 | Val loss: 0.8076268682877222\n",
      "| Epoch: 179/500 | Train loss: 0.8174499279574344 | Val loss: 0.8104965372218026\n",
      "| Epoch: 180/500 | Train loss: 0.817704613166943 | Val loss: 0.8096865912278494\n",
      "| Epoch: 181/500 | Train loss: 0.8184425592422485 | Val loss: 0.8085640047987303\n",
      "| Epoch: 182/500 | Train loss: 0.818569663114715 | Val loss: 0.8085658541984029\n",
      "Epoch   183: reducing learning rate of group 0 to 1.1391e-08.\n",
      "| Epoch: 183/500 | Train loss: 0.8174358648166322 | Val loss: 0.80975970129172\n",
      "| Epoch: 184/500 | Train loss: 0.8166778683662415 | Val loss: 0.8074348891774813\n",
      "| Epoch: 185/500 | Train loss: 0.8189437661254615 | Val loss: 0.8099664532476001\n",
      "| Epoch: 186/500 | Train loss: 0.8171066085497538 | Val loss: 0.8080480603708161\n",
      "| Epoch: 187/500 | Train loss: 0.8188100047278822 | Val loss: 0.8100395972530047\n",
      "| Epoch: 188/500 | Train loss: 0.8175850874499271 | Val loss: 0.8081921388705572\n",
      "| Epoch: 189/500 | Train loss: 0.8185750967577884 | Val loss: 0.810744939578904\n",
      "| Epoch: 190/500 | Train loss: 0.8174786103399176 | Val loss: 0.8078950254453553\n",
      "| Epoch: 191/500 | Train loss: 0.8160820375409042 | Val loss: 0.8076850631170802\n",
      "| Epoch: 192/500 | Train loss: 0.8169401773235254 | Val loss: 0.8077142925726043\n",
      "| Epoch: 193/500 | Train loss: 0.8175924031358016 | Val loss: 0.8080043585764037\n",
      "| Epoch: 194/500 | Train loss: 0.8187619464439259 | Val loss: 0.8076213440961308\n",
      "| Epoch: 195/500 | Train loss: 0.8170595834129735 | Val loss: 0.8087053000926971\n",
      "| Epoch: 196/500 | Train loss: 0.81790477225655 | Val loss: 0.8085134368803766\n",
      "| Epoch: 197/500 | Train loss: 0.8175089472218564 | Val loss: 0.8078509892026583\n",
      "| Epoch: 198/500 | Train loss: 0.8169879944700944 | Val loss: 0.8106616155968772\n",
      "| Epoch: 199/500 | Train loss: 0.8177848587956345 | Val loss: 0.8111386944850286\n",
      "| Epoch: 200/500 | Train loss: 0.8179144227713869 | Val loss: 0.8085736392272843\n",
      "| Epoch: 201/500 | Train loss: 0.8173055673900403 | Val loss: 0.8084024273686938\n",
      "| Epoch: 202/500 | Train loss: 0.8168752933803357 | Val loss: 0.8092949108944999\n",
      "| Epoch: 203/500 | Train loss: 0.8160638675355074 | Val loss: 0.8077722423606448\n",
      "| Epoch: 204/500 | Train loss: 0.8187931378682455 | Val loss: 0.8094352020157708\n",
      "| Epoch: 205/500 | Train loss: 0.8174566248006988 | Val loss: 0.808050673868921\n",
      "| Epoch: 206/500 | Train loss: 0.8171075260430052 | Val loss: 0.8059689187341266\n",
      "| Epoch: 207/500 | Train loss: 0.8197364125335426 | Val loss: 0.8084110915660858\n",
      "| Epoch: 208/500 | Train loss: 0.8172023618430422 | Val loss: 0.8118467239869965\n",
      "| Epoch: 209/500 | Train loss: 0.8172024431981538 | Val loss: 0.8092196823822128\n",
      "| Epoch: 210/500 | Train loss: 0.8179032334110193 | Val loss: 0.8089173229204284\n",
      "| Epoch: 211/500 | Train loss: 0.817360213974066 | Val loss: 0.8076102104451921\n",
      "| Epoch: 212/500 | Train loss: 0.81702892069231 | Val loss: 0.8085421489344703\n",
      "| Epoch: 213/500 | Train loss: 0.8175589528000146 | Val loss: 0.8081919070747163\n",
      "| Epoch: 214/500 | Train loss: 0.8182753694684882 | Val loss: 0.8087398649917709\n",
      "| Epoch: 215/500 | Train loss: 0.8180653938075952 | Val loss: 0.8071307713786761\n",
      "| Epoch: 216/500 | Train loss: 0.8171522924774571 | Val loss: 0.8085794871052107\n",
      "| Epoch: 217/500 | Train loss: 0.8187534738005253 | Val loss: 0.8161854007177882\n",
      "| Epoch: 218/500 | Train loss: 0.8189117527844613 | Val loss: 0.8079822659492493\n",
      "| Epoch: 219/500 | Train loss: 0.8171732057604874 | Val loss: 0.8121307185954518\n",
      "| Epoch: 220/500 | Train loss: 0.8174905344059593 | Val loss: 0.8077971008088853\n",
      "| Epoch: 221/500 | Train loss: 0.8197380856463784 | Val loss: 0.8092020882500542\n",
      "| Epoch: 222/500 | Train loss: 0.8181066136611136 | Val loss: 0.8101974386307929\n",
      "| Epoch: 223/500 | Train loss: 0.8175824698648955 | Val loss: 0.8107129890057776\n",
      "| Epoch: 224/500 | Train loss: 0.848292442999388 | Val loss: 0.8082259636786249\n",
      "| Epoch: 225/500 | Train loss: 0.8176243037508245 | Val loss: 0.8128167125913832\n",
      "| Epoch: 226/500 | Train loss: 0.8190181610877054 | Val loss: 0.8096936005685065\n",
      "| Epoch: 227/500 | Train loss: 0.8188166336009377 | Val loss: 0.809995113975472\n",
      "| Epoch: 228/500 | Train loss: 0.8179167561363756 | Val loss: 0.8093807440665033\n",
      "| Epoch: 229/500 | Train loss: 0.8191840452060365 | Val loss: 0.8076981579264005\n",
      "| Epoch: 230/500 | Train loss: 0.818022896950705 | Val loss: 0.8070152675112089\n",
      "| Epoch: 231/500 | Train loss: 0.818298868338267 | Val loss: 0.8087489497330453\n",
      "| Epoch: 232/500 | Train loss: 0.8192062237806488 | Val loss: 0.808436207473278\n",
      "| Epoch: 233/500 | Train loss: 0.8180773689035784 | Val loss: 0.8113103542062972\n",
      "| Epoch: 234/500 | Train loss: 0.8182173764496519 | Val loss: 0.8099145053161515\n",
      "| Epoch: 235/500 | Train loss: 0.8167472939742239 | Val loss: 0.8099814818965064\n",
      "| Epoch: 236/500 | Train loss: 0.817918336391449 | Val loss: 0.8091884255409241\n",
      "| Epoch: 237/500 | Train loss: 0.8178254382652149 | Val loss: 0.808986582689815\n",
      "| Epoch: 238/500 | Train loss: 0.8171345148170204 | Val loss: 0.8078392744064331\n",
      "| Epoch: 239/500 | Train loss: 0.8168037339260704 | Val loss: 0.8079766010244688\n",
      "| Epoch: 240/500 | Train loss: 0.8179462727747465 | Val loss: 0.8078049918015798\n",
      "| Epoch: 241/500 | Train loss: 0.8172980091028046 | Val loss: 0.8075349595811632\n",
      "| Epoch: 242/500 | Train loss: 0.8186044343730859 | Val loss: 0.807278912100527\n",
      "| Epoch: 243/500 | Train loss: 0.8176792592333074 | Val loss: 0.8076615341835551\n",
      "| Epoch: 244/500 | Train loss: 0.8165852163967333 | Val loss: 0.8099973590837585\n",
      "| Epoch: 245/500 | Train loss: 0.8176528453826905 | Val loss: 0.8081589912374815\n",
      "| Epoch: 246/500 | Train loss: 0.8171261394233035 | Val loss: 0.8085716590285301\n",
      "| Epoch: 247/500 | Train loss: 0.8171880782696239 | Val loss: 0.8099286788039737\n",
      "| Epoch: 248/500 | Train loss: 0.81691814661026 | Val loss: 0.8084926903247833\n",
      "| Epoch: 249/500 | Train loss: 0.8198996914060492 | Val loss: 0.8087897159987025\n",
      "| Epoch: 250/500 | Train loss: 0.818744649176012 | Val loss: 0.8102368298504088\n",
      "| Epoch: 251/500 | Train loss: 0.8180277479322333 | Val loss: 0.8081850773758359\n",
      "| Epoch: 252/500 | Train loss: 0.816909514812001 | Val loss: 0.8099079719848103\n",
      "| Epoch: 253/500 | Train loss: 0.8174932291633205 | Val loss: 0.8068503008948432\n",
      "| Epoch: 254/500 | Train loss: 0.817707804211399 | Val loss: 0.8085965621802542\n",
      "| Epoch: 255/500 | Train loss: 0.8163456228741428 | Val loss: 0.8096148810452886\n",
      "| Epoch: 256/500 | Train loss: 0.8182916442553202 | Val loss: 0.8071979209780693\n",
      "| Epoch: 257/500 | Train loss: 0.8170170652238946 | Val loss: 0.8065631927715408\n",
      "| Epoch: 258/500 | Train loss: 0.8182029586089284 | Val loss: 0.8069796901610162\n",
      "| Epoch: 259/500 | Train loss: 0.819273623039848 | Val loss: 0.8119052449862162\n",
      "| Epoch: 260/500 | Train loss: 0.8158548969971506 | Val loss: 0.8087643583615621\n",
      "| Epoch: 261/500 | Train loss: 0.8176389489257545 | Val loss: 0.8071700260043144\n",
      "| Epoch: 262/500 | Train loss: 0.8190556371421145 | Val loss: 0.8089103177189827\n",
      "| Epoch: 263/500 | Train loss: 0.8172566441067478 | Val loss: 0.8095093791683515\n",
      "| Epoch: 264/500 | Train loss: 0.8175928705617002 | Val loss: 0.8081662861837281\n",
      "| Epoch: 265/500 | Train loss: 0.8177809499857719 | Val loss: 0.8080528072184987\n",
      "| Epoch: 266/500 | Train loss: 0.8182169581714429 | Val loss: 0.8097840688294835\n",
      "| Epoch: 267/500 | Train loss: 0.818739356283556 | Val loss: 0.8090495044986407\n",
      "| Epoch: 268/500 | Train loss: 0.8188436449619761 | Val loss: 0.8090340801411204\n",
      "| Epoch: 269/500 | Train loss: 0.8176982007528606 | Val loss: 0.809609936343299\n",
      "| Epoch: 270/500 | Train loss: 0.8187028408050537 | Val loss: 0.8090698288546668\n",
      "| Epoch: 271/500 | Train loss: 0.8176325461320709 | Val loss: 0.80836181425386\n",
      "| Epoch: 272/500 | Train loss: 0.8191457984740274 | Val loss: 0.8070918280217383\n",
      "| Epoch: 273/500 | Train loss: 0.8178195089624639 | Val loss: 0.8088041792313257\n",
      "| Epoch: 274/500 | Train loss: 0.8181805336684511 | Val loss: 0.8080931098924743\n",
      "| Epoch: 275/500 | Train loss: 0.81817151705424 | Val loss: 0.8095491652687391\n",
      "| Epoch: 276/500 | Train loss: 0.8187401579137434 | Val loss: 0.8080413060055839\n",
      "| Epoch: 277/500 | Train loss: 0.8168003782891391 | Val loss: 0.8080840508143107\n",
      "| Epoch: 278/500 | Train loss: 0.8182647866115236 | Val loss: 0.8085840609338548\n",
      "| Epoch: 279/500 | Train loss: 0.8181290289811921 | Val loss: 0.8096777283483081\n",
      "| Epoch: 280/500 | Train loss: 0.8178341991023014 | Val loss: 0.80919582148393\n",
      "| Epoch: 281/500 | Train loss: 0.818253403379206 | Val loss: 0.8089518116580116\n",
      "| Epoch: 282/500 | Train loss: 0.8157937470235322 | Val loss: 0.8084758801592721\n",
      "| Epoch: 283/500 | Train loss: 0.8171072288563377 | Val loss: 0.8099795977274576\n",
      "| Epoch: 284/500 | Train loss: 0.8179048718067637 | Val loss: 0.8086272784405284\n",
      "| Epoch: 285/500 | Train loss: 0.8165359986455817 | Val loss: 0.8091043788525794\n",
      "| Epoch: 286/500 | Train loss: 0.8216233161457798 | Val loss: 0.8090138940347565\n",
      "| Epoch: 287/500 | Train loss: 0.8176313780901725 | Val loss: 0.8090422939923074\n",
      "| Epoch: 288/500 | Train loss: 0.8173415895093952 | Val loss: 0.80927734159761\n",
      "| Epoch: 289/500 | Train loss: 0.8185817224937573 | Val loss: 0.8086706002553304\n",
      "| Epoch: 290/500 | Train loss: 0.816899872035311 | Val loss: 0.8099100697371695\n",
      "| Epoch: 291/500 | Train loss: 0.8174204615124485 | Val loss: 0.8063603755500581\n",
      "| Epoch: 292/500 | Train loss: 0.8175856366492155 | Val loss: 0.8093327416314019\n",
      "| Epoch: 293/500 | Train loss: 0.8193060883304529 | Val loss: 0.8112937692138884\n",
      "| Epoch: 294/500 | Train loss: 0.8181495099736933 | Val loss: 0.8076881501409743\n",
      "| Epoch: 295/500 | Train loss: 0.8169640884064792 | Val loss: 0.807786934905582\n",
      "| Epoch: 296/500 | Train loss: 0.8174873475442853 | Val loss: 0.8073438074853685\n",
      "| Epoch: 297/500 | Train loss: 0.818248999327944 | Val loss: 0.8097008350822661\n",
      "| Epoch: 298/500 | Train loss: 0.8164712094424064 | Val loss: 0.8071956518623564\n",
      "| Epoch: 299/500 | Train loss: 0.8174522061096995 | Val loss: 0.8091781329777505\n",
      "| Epoch: 300/500 | Train loss: 0.8175419575289676 | Val loss: 0.8083651752935516\n",
      "| Epoch: 301/500 | Train loss: 0.8165617150172853 | Val loss: 0.8090461020668348\n",
      "| Epoch: 302/500 | Train loss: 0.8183937028834695 | Val loss: 0.8070375025272369\n",
      "| Epoch: 303/500 | Train loss: 0.8177124655037595 | Val loss: 0.8073375365800328\n",
      "| Epoch: 304/500 | Train loss: 0.8175962082126684 | Val loss: 0.8085384840766588\n",
      "| Epoch: 305/500 | Train loss: 0.816760308073278 | Val loss: 0.8084139385157161\n",
      "| Epoch: 306/500 | Train loss: 0.8178115303056281 | Val loss: 0.8089043266243405\n",
      "| Epoch: 307/500 | Train loss: 0.8173090426545394 | Val loss: 0.8092596605420113\n",
      "| Epoch: 308/500 | Train loss: 0.8182663511811641 | Val loss: 0.8081861668162875\n",
      "| Epoch: 309/500 | Train loss: 0.8177124667585942 | Val loss: 0.8094894836346308\n",
      "| Epoch: 310/500 | Train loss: 0.818296738256488 | Val loss: 0.8077279503146807\n",
      "| Epoch: 311/500 | Train loss: 0.8171882390975952 | Val loss: 0.8087618193692632\n",
      "| Epoch: 312/500 | Train loss: 0.8185186501134906 | Val loss: 0.8082553446292877\n",
      "| Epoch: 313/500 | Train loss: 0.8167575363527264 | Val loss: 0.8102209187216229\n",
      "| Epoch: 314/500 | Train loss: 0.8173865581813612 | Val loss: 0.8092584601706929\n",
      "| Epoch: 315/500 | Train loss: 0.8173003265732213 | Val loss: 0.8083861809637811\n",
      "| Epoch: 316/500 | Train loss: 0.8172842743103964 | Val loss: 0.8065193386541473\n",
      "| Epoch: 317/500 | Train loss: 0.819080846978907 | Val loss: 0.808386843237612\n",
      "| Epoch: 318/500 | Train loss: 0.8164745410283406 | Val loss: 0.8109659577409426\n",
      "| Epoch: 319/500 | Train loss: 0.8177061147857131 | Val loss: 0.8083720182379087\n",
      "| Epoch: 320/500 | Train loss: 0.8178475578625997 | Val loss: 0.8087693850199381\n",
      "| Epoch: 321/500 | Train loss: 0.8180461304229603 | Val loss: 0.8072819295856688\n",
      "| Epoch: 322/500 | Train loss: 0.8174699176821792 | Val loss: 0.8096201394995054\n",
      "| Epoch: 323/500 | Train loss: 0.8168470288577833 | Val loss: 0.8079052807556258\n",
      "| Epoch: 324/500 | Train loss: 0.8169871857291774 | Val loss: 0.808511452542411\n",
      "| Epoch: 325/500 | Train loss: 0.8189233443193268 | Val loss: 0.8076395359304216\n",
      "| Epoch: 326/500 | Train loss: 0.8176966299090469 | Val loss: 0.8082818455166287\n",
      "| Epoch: 327/500 | Train loss: 0.8187957111157869 | Val loss: 0.8096990196241273\n",
      "| Epoch: 328/500 | Train loss: 0.8173151543265895 | Val loss: 0.8107133342160119\n",
      "| Epoch: 329/500 | Train loss: 0.8167334659057751 | Val loss: 0.8087466814451747\n",
      "| Epoch: 330/500 | Train loss: 0.8178669910681875 | Val loss: 0.8084316394395299\n",
      "| Epoch: 331/500 | Train loss: 0.8178561806678772 | Val loss: 0.8072762464483579\n",
      "| Epoch: 332/500 | Train loss: 0.8160624842894705 | Val loss: 0.8096334089835485\n",
      "| Epoch: 333/500 | Train loss: 0.8170281995806777 | Val loss: 0.8084987998008728\n",
      "| Epoch: 334/500 | Train loss: 0.8186223314519514 | Val loss: 0.8090727768010564\n",
      "| Epoch: 335/500 | Train loss: 0.8173990103236416 | Val loss: 0.8082955222990778\n",
      "| Epoch: 336/500 | Train loss: 0.8178627208659524 | Val loss: 0.8090397367874781\n",
      "| Epoch: 337/500 | Train loss: 0.8174398238198799 | Val loss: 0.8076805530322922\n",
      "| Epoch: 338/500 | Train loss: 0.8172120715442457 | Val loss: 0.8087984356615279\n",
      "| Epoch: 339/500 | Train loss: 0.8181982590441118 | Val loss: 0.8101485984192954\n",
      "| Epoch: 340/500 | Train loss: 0.8168428508858931 | Val loss: 0.8077323031094339\n",
      "| Epoch: 341/500 | Train loss: 0.8165965046798974 | Val loss: 0.8088746782806184\n",
      "| Epoch: 342/500 | Train loss: 0.8193965154781676 | Val loss: 0.8076527358757125\n",
      "| Epoch: 343/500 | Train loss: 0.817179853037784 | Val loss: 0.8091327067878511\n",
      "| Epoch: 344/500 | Train loss: 0.817082869379144 | Val loss: 0.8086256144775285\n",
      "| Epoch: 345/500 | Train loss: 0.817370777590233 | Val loss: 0.8088445473048422\n",
      "| Epoch: 346/500 | Train loss: 0.8175281453550908 | Val loss: 0.8098864265614085\n",
      "| Epoch: 347/500 | Train loss: 0.8175056327853286 | Val loss: 0.8098354215423266\n",
      "| Epoch: 348/500 | Train loss: 0.8195545987079018 | Val loss: 0.8106890072425207\n",
      "| Epoch: 349/500 | Train loss: 0.8177147053835685 | Val loss: 0.8087072943647703\n",
      "| Epoch: 350/500 | Train loss: 0.8186355492524934 | Val loss: 0.8091803648405604\n",
      "| Epoch: 351/500 | Train loss: 0.8178926804609467 | Val loss: 0.8086008090111945\n",
      "| Epoch: 352/500 | Train loss: 0.8178342482499908 | Val loss: 0.8085728444986873\n",
      "| Epoch: 353/500 | Train loss: 0.8172187748708223 | Val loss: 0.8088894337415695\n",
      "| Epoch: 354/500 | Train loss: 0.8231612588229932 | Val loss: 0.8071144670248032\n",
      "| Epoch: 355/500 | Train loss: 0.816659498842139 | Val loss: 0.8089669769008955\n",
      "| Epoch: 356/500 | Train loss: 0.8191293703882317 | Val loss: 0.8075068891048431\n",
      "| Epoch: 357/500 | Train loss: 0.8180877428305776 | Val loss: 0.8075756455461184\n",
      "| Epoch: 358/500 | Train loss: 0.8174659883766844 | Val loss: 0.8097177263763216\n",
      "| Epoch: 359/500 | Train loss: 0.8177345727619372 | Val loss: 0.8086679246690538\n",
      "| Epoch: 360/500 | Train loss: 0.8180994125834683 | Val loss: 0.8115339767601755\n",
      "| Epoch: 361/500 | Train loss: 0.8172992436509383 | Val loss: 0.8087292139728864\n",
      "| Epoch: 362/500 | Train loss: 0.8174945509224607 | Val loss: 0.8085508760478761\n",
      "| Epoch: 363/500 | Train loss: 0.8178448024548982 | Val loss: 0.8090966310766008\n",
      "| Epoch: 364/500 | Train loss: 0.817892741320426 | Val loss: 0.8090011965897348\n",
      "| Epoch: 365/500 | Train loss: 0.8183967975147983 | Val loss: 0.8083370394176908\n",
      "| Epoch: 366/500 | Train loss: 0.8169195480513991 | Val loss: 0.809631809592247\n",
      "| Epoch: 367/500 | Train loss: 0.8169412249012997 | Val loss: 0.8082493369777998\n",
      "| Epoch: 368/500 | Train loss: 0.8168429634027313 | Val loss: 0.8094529584050179\n",
      "| Epoch: 369/500 | Train loss: 0.8187541336344 | Val loss: 0.8086320435007414\n",
      "| Epoch: 370/500 | Train loss: 0.8178903404035066 | Val loss: 0.8085798886087205\n",
      "| Epoch: 371/500 | Train loss: 0.8174923309108667 | Val loss: 0.8091129577822156\n",
      "| Epoch: 372/500 | Train loss: 0.8180579551479272 | Val loss: 0.8071140141950713\n",
      "| Epoch: 373/500 | Train loss: 0.8176897532061527 | Val loss: 0.809252659479777\n",
      "| Epoch: 374/500 | Train loss: 0.8170616317213627 | Val loss: 0.8087400711245007\n",
      "| Epoch: 375/500 | Train loss: 0.8182773941441586 | Val loss: 0.8078742374976476\n",
      "| Epoch: 376/500 | Train loss: 0.8183503387267129 | Val loss: 0.8090024342139562\n",
      "| Epoch: 377/500 | Train loss: 0.8164423817082456 | Val loss: 0.8085911456081603\n",
      "| Epoch: 378/500 | Train loss: 0.8175043658206337 | Val loss: 0.8089775534139739\n",
      "| Epoch: 379/500 | Train loss: 0.8177546988453781 | Val loss: 0.8075604264934858\n",
      "| Epoch: 380/500 | Train loss: 0.8182429303202713 | Val loss: 0.807528018951416\n",
      "| Epoch: 381/500 | Train loss: 0.8188612337698016 | Val loss: 0.8111512106325891\n",
      "| Epoch: 382/500 | Train loss: 0.8187902279067458 | Val loss: 0.8087621538175477\n",
      "| Epoch: 383/500 | Train loss: 0.8180348724649663 | Val loss: 0.8074407693412569\n",
      "| Epoch: 384/500 | Train loss: 0.816133077102795 | Val loss: 0.8062359574768279\n",
      "| Epoch: 385/500 | Train loss: 0.8175581674826773 | Val loss: 0.8083568215370178\n",
      "| Epoch: 386/500 | Train loss: 0.817715535247535 | Val loss: 0.8097226661112573\n",
      "| Epoch: 387/500 | Train loss: 0.8172539131683215 | Val loss: 0.8087570940454801\n",
      "| Epoch: 388/500 | Train loss: 0.8173744722416526 | Val loss: 0.8093381391631232\n",
      "| Epoch: 389/500 | Train loss: 0.8183806369179174 | Val loss: 0.8124873033828206\n",
      "| Epoch: 390/500 | Train loss: 0.8172574597492552 | Val loss: 0.8067229241132736\n",
      "| Epoch: 391/500 | Train loss: 0.8169728651381376 | Val loss: 0.8071671980950568\n",
      "| Epoch: 392/500 | Train loss: 0.8195808025828579 | Val loss: 0.8110491608579954\n",
      "| Epoch: 393/500 | Train loss: 0.8195566796419913 | Val loss: 0.8089104551408026\n",
      "| Epoch: 394/500 | Train loss: 0.8169375745873703 | Val loss: 0.8105882994002767\n",
      "| Epoch: 395/500 | Train loss: 0.8169689950190092 | Val loss: 0.8095422751373715\n",
      "| Epoch: 396/500 | Train loss: 0.8177433559769078 | Val loss: 0.8087635181016393\n",
      "| Epoch: 397/500 | Train loss: 0.8169792371883727 | Val loss: 0.8090774921907319\n",
      "| Epoch: 398/500 | Train loss: 0.817492867980087 | Val loss: 0.808583854801125\n",
      "| Epoch: 399/500 | Train loss: 0.8169098799688774 | Val loss: 0.8084612695707215\n",
      "| Epoch: 400/500 | Train loss: 0.8176699245185183 | Val loss: 0.8095291612876786\n",
      "| Epoch: 401/500 | Train loss: 0.8185438952947918 | Val loss: 0.8078145219220055\n",
      "| Epoch: 402/500 | Train loss: 0.8183438273898342 | Val loss: 0.8082686637838682\n",
      "| Epoch: 403/500 | Train loss: 0.8184574225492645 | Val loss: 0.8097374017039934\n",
      "| Epoch: 404/500 | Train loss: 0.8174718871451261 | Val loss: 0.8098092451691628\n",
      "| Epoch: 405/500 | Train loss: 0.8170221772110253 | Val loss: 0.8106604235039817\n",
      "| Epoch: 406/500 | Train loss: 0.8185087933874967 | Val loss: 0.8088042562206587\n",
      "| Epoch: 407/500 | Train loss: 0.8182559642875403 | Val loss: 0.8072097856137488\n",
      "| Epoch: 408/500 | Train loss: 0.8175099305939256 | Val loss: 0.8089709820018874\n",
      "| Epoch: 409/500 | Train loss: 0.8182887326207078 | Val loss: 0.8089310046699312\n",
      "| Epoch: 410/500 | Train loss: 0.816365549229739 | Val loss: 0.8090920199950536\n",
      "| Epoch: 411/500 | Train loss: 0.8163704253079599 | Val loss: 0.8068561007579168\n",
      "| Epoch: 412/500 | Train loss: 0.8174834238855462 | Val loss: 0.8097144174906943\n",
      "| Epoch: 413/500 | Train loss: 0.8173635194176122 | Val loss: 0.809162733455499\n",
      "| Epoch: 414/500 | Train loss: 0.8164032887994197 | Val loss: 0.8073155350155301\n",
      "| Epoch: 415/500 | Train loss: 0.8181622237489935 | Val loss: 0.8089202865958214\n",
      "| Epoch: 416/500 | Train loss: 0.8177139955654479 | Val loss: 0.8093719647990333\n",
      "| Epoch: 417/500 | Train loss: 0.8180684781911081 | Val loss: 0.8111453586154513\n",
      "| Epoch: 418/500 | Train loss: 0.8177356740884614 | Val loss: 0.8081039082672861\n",
      "| Epoch: 419/500 | Train loss: 0.8189926544825236 | Val loss: 0.810452031592528\n",
      "| Epoch: 420/500 | Train loss: 0.8174184930951972 | Val loss: 0.8091977602905698\n",
      "| Epoch: 421/500 | Train loss: 0.8180493515834474 | Val loss: 0.8072808293832673\n",
      "| Epoch: 422/500 | Train loss: 0.8181704217927498 | Val loss: 0.8088864766889148\n",
      "| Epoch: 423/500 | Train loss: 0.818934374733975 | Val loss: 0.8085994083020422\n",
      "| Epoch: 424/500 | Train loss: 0.8175275978289153 | Val loss: 0.8110422119498253\n",
      "| Epoch: 425/500 | Train loss: 0.8181860177140486 | Val loss: 0.809932173954116\n",
      "| Epoch: 426/500 | Train loss: 0.8168073672997324 | Val loss: 0.809154249727726\n",
      "| Epoch: 427/500 | Train loss: 0.8174488496362118 | Val loss: 0.8097427553600736\n",
      "| Epoch: 428/500 | Train loss: 0.8177445440961604 | Val loss: 0.8089091860585742\n",
      "| Epoch: 429/500 | Train loss: 0.8182271047642357 | Val loss: 0.8081408366560936\n",
      "| Epoch: 430/500 | Train loss: 0.8165880615251107 | Val loss: 0.8089287239644263\n",
      "| Epoch: 431/500 | Train loss: 0.8187714731484129 | Val loss: 0.80897898806466\n",
      "| Epoch: 432/500 | Train loss: 0.8177521278983668 | Val loss: 0.8081367371810807\n",
      "| Epoch: 433/500 | Train loss: 0.8194705461200915 | Val loss: 0.8121668505999777\n",
      "| Epoch: 434/500 | Train loss: 0.818293685662119 | Val loss: 0.8078997433185577\n",
      "| Epoch: 435/500 | Train loss: 0.8183701224494399 | Val loss: 0.8087905819217364\n",
      "| Epoch: 436/500 | Train loss: 0.8167006415233278 | Val loss: 0.8097507738404803\n",
      "| Epoch: 437/500 | Train loss: 0.818694612017849 | Val loss: 0.8086254290408559\n",
      "| Epoch: 438/500 | Train loss: 0.8179222516846238 | Val loss: 0.810343092514409\n",
      "| Epoch: 439/500 | Train loss: 0.8178329160338954 | Val loss: 0.8085818505949445\n",
      "| Epoch: 440/500 | Train loss: 0.8174231759288855 | Val loss: 0.807699123190509\n",
      "| Epoch: 441/500 | Train loss: 0.817761562790787 | Val loss: 0.8095385548141267\n",
      "| Epoch: 442/500 | Train loss: 0.8177460250101591 | Val loss: 0.8082000795337889\n",
      "| Epoch: 443/500 | Train loss: 0.8187255006087454 | Val loss: 0.8081207159492705\n",
      "| Epoch: 444/500 | Train loss: 0.8178658868137159 | Val loss: 0.8079668324854639\n",
      "| Epoch: 445/500 | Train loss: 0.8163761256033915 | Val loss: 0.808558452460501\n",
      "| Epoch: 446/500 | Train loss: 0.8182285906975729 | Val loss: 0.8102221629685826\n",
      "| Epoch: 447/500 | Train loss: 0.8186531230023033 | Val loss: 0.8095322706633143\n",
      "| Epoch: 448/500 | Train loss: 0.8180174106045773 | Val loss: 0.8076977630456289\n",
      "| Epoch: 449/500 | Train loss: 0.8177266434619301 | Val loss: 0.8076775529318385\n",
      "| Epoch: 450/500 | Train loss: 0.8174695104883428 | Val loss: 0.8100463325778643\n",
      "| Epoch: 451/500 | Train loss: 0.8176258963451051 | Val loss: 0.8086974306239022\n",
      "| Epoch: 452/500 | Train loss: 0.8164703963095682 | Val loss: 0.8091855090525415\n",
      "| Epoch: 453/500 | Train loss: 0.8172335570318657 | Val loss: 0.8099746588203642\n",
      "| Epoch: 454/500 | Train loss: 0.8168367569906669 | Val loss: 0.8092454589075513\n",
      "| Epoch: 455/500 | Train loss: 0.817515312370501 | Val loss: 0.8080490661991967\n",
      "| Epoch: 456/500 | Train loss: 0.8181193805577462 | Val loss: 0.8108174552520117\n",
      "| Epoch: 457/500 | Train loss: 0.8174377955888447 | Val loss: 0.8084930984510316\n",
      "| Epoch: 458/500 | Train loss: 0.8174919741195544 | Val loss: 0.8109085708856583\n",
      "| Epoch: 459/500 | Train loss: 0.8159860853563276 | Val loss: 0.8072955972618527\n",
      "| Epoch: 460/500 | Train loss: 0.817594395813189 | Val loss: 0.8079600508014361\n",
      "| Epoch: 461/500 | Train loss: 0.8178664740763213 | Val loss: 0.8090143965350257\n",
      "| Epoch: 462/500 | Train loss: 0.8187435926052562 | Val loss: 0.8078390707572302\n",
      "| Epoch: 463/500 | Train loss: 0.8167321612960414 | Val loss: 0.8089158294929398\n",
      "| Epoch: 464/500 | Train loss: 0.8183255400573998 | Val loss: 0.8086554913057221\n",
      "| Epoch: 465/500 | Train loss: 0.8173687276087309 | Val loss: 0.8091256320476532\n",
      "| Epoch: 466/500 | Train loss: 0.818217518664243 | Val loss: 0.8091983422636986\n",
      "| Epoch: 467/500 | Train loss: 0.8188523127321612 | Val loss: 0.8092192088564237\n",
      "| Epoch: 468/500 | Train loss: 0.8181703092759116 | Val loss: 0.8094411260551877\n",
      "| Epoch: 469/500 | Train loss: 0.8179251984546059 | Val loss: 0.8072399232122633\n",
      "| Epoch: 470/500 | Train loss: 0.8177738007746245 | Val loss: 0.8151183567113347\n",
      "| Epoch: 471/500 | Train loss: 0.8174097115533394 | Val loss: 0.8086568233039644\n",
      "| Epoch: 472/500 | Train loss: 0.8177204677933141 | Val loss: 0.8097983789112833\n",
      "| Epoch: 473/500 | Train loss: 0.8165348567460713 | Val loss: 0.8094019128216637\n",
      "| Epoch: 474/500 | Train loss: 0.8177729577348943 | Val loss: 0.8079203127159013\n",
      "| Epoch: 475/500 | Train loss: 0.8169056089300858 | Val loss: 0.8078036341402266\n",
      "| Epoch: 476/500 | Train loss: 0.816405959714923 | Val loss: 0.8091829394300779\n",
      "| Epoch: 477/500 | Train loss: 0.8179459703596015 | Val loss: 0.809814126127296\n",
      "| Epoch: 478/500 | Train loss: 0.8184370189382318 | Val loss: 0.8086923062801361\n",
      "| Epoch: 479/500 | Train loss: 0.8178885244486624 | Val loss: 0.8080351443754302\n",
      "| Epoch: 480/500 | Train loss: 0.8176776842067116 | Val loss: 0.8079497540990511\n",
      "| Epoch: 481/500 | Train loss: 0.8503860101365206 | Val loss: 0.8088748902082443\n",
      "| Epoch: 482/500 | Train loss: 0.8186213880254511 | Val loss: 0.8091093144483037\n",
      "| Epoch: 483/500 | Train loss: 0.8178274409812794 | Val loss: 0.8080832925107744\n",
      "| Epoch: 484/500 | Train loss: 0.8163506658453691 | Val loss: 0.8089999664160941\n",
      "| Epoch: 485/500 | Train loss: 0.8180912530213071 | Val loss: 0.8097801332672437\n",
      "| Epoch: 486/500 | Train loss: 0.816753467760588 | Val loss: 0.8095715749594901\n",
      "| Epoch: 487/500 | Train loss: 0.8184863094697918 | Val loss: 0.8087335005402565\n",
      "| Epoch: 488/500 | Train loss: 0.8159048753872252 | Val loss: 0.8094235766265128\n",
      "| Epoch: 489/500 | Train loss: 0.8173691360574019 | Val loss: 0.8104525167081091\n",
      "| Epoch: 490/500 | Train loss: 0.8178893076746088 | Val loss: 0.8080242027839025\n",
      "| Epoch: 491/500 | Train loss: 0.8176838000615437 | Val loss: 0.8088686019182205\n",
      "| Epoch: 492/500 | Train loss: 0.816064883533277 | Val loss: 0.808358919289377\n",
      "| Epoch: 493/500 | Train loss: 0.817185410072929 | Val loss: 0.8091670109166039\n",
      "| Epoch: 494/500 | Train loss: 0.816871788836362 | Val loss: 0.8093677320414119\n",
      "| Epoch: 495/500 | Train loss: 0.817045641781991 | Val loss: 0.8080899665753046\n",
      "| Epoch: 496/500 | Train loss: 0.8176018100035818 | Val loss: 0.8091463430060281\n",
      "| Epoch: 497/500 | Train loss: 0.8188850156047888 | Val loss: 0.8106915867990918\n",
      "| Epoch: 498/500 | Train loss: 0.8175443971366213 | Val loss: 0.8091716691851616\n",
      "| Epoch: 499/500 | Train loss: 0.8168278934662803 | Val loss: 0.809890349706014\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 500\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "best_val_loss = np.inf\n",
    "\n",
    "for i, epoch in enumerate(range(num_epoch)):\n",
    "    train_loss.append(train())\n",
    "    val_loss.append(evaluate())\n",
    "    \n",
    "    print(f'| Epoch: {epoch}/{num_epoch} | Train loss: {train_loss[i]} | Val loss: {val_loss[i]}')\n",
    "        \n",
    "    if val_loss[i] < best_val_loss:\n",
    "        best_val_loss = val_loss[i]\n",
    "        best_model = model\n",
    "        \n",
    "        best_model_weights = copy.deepcopy(model.state_dict())\n",
    "        torch.save(best_model_weights, f'./best_models/seismic_net_epoch_{epoch}_val_loss_{val_loss[i]}.pt')\n",
    "\n",
    "    scheduler.step(val_loss[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"52181f20-42cd-4c79-9d93-9ac5a771d260\" data-root-id=\"1126\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"0799129a-6b8d-447b-aea7-7d75324470f8\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1137\"}],\"center\":[{\"id\":\"1140\"},{\"id\":\"1144\"},{\"id\":\"1172\"}],\"left\":[{\"id\":\"1141\"}],\"plot_height\":250,\"renderers\":[{\"id\":\"1162\"},{\"id\":\"1177\"}],\"title\":{\"id\":\"1127\"},\"toolbar\":{\"id\":\"1152\"},\"x_range\":{\"id\":\"1129\"},\"x_scale\":{\"id\":\"1133\"},\"y_range\":{\"id\":\"1131\"},\"y_scale\":{\"id\":\"1135\"}},\"id\":\"1126\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1142\",\"type\":\"BasicTicker\"},{\"attributes\":{\"data_source\":{\"id\":\"1159\"},\"glyph\":{\"id\":\"1160\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1161\"},\"selection_glyph\":null,\"view\":{\"id\":\"1163\"}},\"id\":\"1162\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"axis\":{\"id\":\"1137\"},\"ticker\":null},\"id\":\"1140\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1169\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1150\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"1131\",\"type\":\"DataRange1d\"},{\"attributes\":{\"data_source\":{\"id\":\"1174\"},\"glyph\":{\"id\":\"1175\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1176\"},\"selection_glyph\":null,\"view\":{\"id\":\"1178\"}},\"id\":\"1177\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"items\":[{\"id\":\"1173\"},{\"id\":\"1189\"}]},\"id\":\"1172\",\"type\":\"Legend\"},{\"attributes\":{\"formatter\":{\"id\":\"1168\"},\"ticker\":{\"id\":\"1138\"}},\"id\":\"1137\",\"type\":\"LinearAxis\"},{\"attributes\":{\"source\":{\"id\":\"1159\"}},\"id\":\"1163\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1133\",\"type\":\"LinearScale\"},{\"attributes\":{\"data\":{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299],\"y\":[0.9902655432621638,0.9796949989265866,0.9702216022544436,0.9581734248333507,0.9484715908765793,0.9414091664883826,0.9395052674743865,0.9278086904022429,0.9179291112555398,0.9028075734774271,0.9004326976007886,0.9052686351868842,0.8956909908188714,0.893318249947495,0.8904362701707416,0.8886090599828296,0.8883606303069327,0.8871325511071417,0.8764022398326132,0.8698064130213525,0.8703151345252991,0.8678771389855279,0.8729238286614418,0.8681436901291212,0.8715101174182363,0.8672295320365164,0.8617345400982432,0.8692145024736723,0.8599471168385612,0.860303575793902,0.857306908402178,0.8599456316894956,0.8495566902889146,0.8532280176877975,0.8587811026308272,0.8558874328931173,0.8496564941273795,0.8483088687062263,0.8579150256183412,0.8514690084589852,0.8479956123563979,0.8512581272257699,0.844597592122025,0.8541224574049314,0.8503529205918312,0.854343949092759,0.8489495565493902,0.8452131715085771,0.847236268222332,0.8446688627203306,0.8498072922229767,0.8504370599985123,0.8500230742825402,0.8430096382896105,0.8441086875067817,0.8450269674261411,0.8438101079728868,0.8398650644554032,0.8422774333092902,0.8416249710652564,0.835226070549753,0.8489101628462473,0.8456874969932768,0.8442523860269122,0.8397606271836493,0.8416611808869574,0.8376245001951853,0.8378884949617915,0.8457073974940512,0.839912061062124,0.84142657286591,0.8461070772674348,0.8262692623668246,0.8233596252070533,0.8221264017952813,0.8208366905649503,0.8243615412049823,0.8222008463409212,0.8214841658870379,0.8219789084461,0.820319778389401,0.8228517207834456,0.8202791089812914,0.8193098761969142,0.8204816745387183,0.8206351771950722,0.8206066762407621,0.8191205279694663,0.8208706991540061,0.8191154061092271,0.8174714172879854,0.8179900613096025,0.8143408190872934,0.8182940897014406,0.817830488913589,0.8190720594591565,0.8180475781361262,0.8179929554462433,0.8182590537601047,0.816597638030847,0.816153815223111,0.820342532462544,0.818222394420041,0.8167878521813287,0.8149860575795174,0.8130888533261087,0.8116618477635913,0.8119528765479723,0.8134632557630539,0.8125485719905959,0.810783954958121,0.8116413959198527,0.8119478879703416,0.8111345718304316,0.8093301935328377,0.8095744384659661,0.8095954954624176,0.8095096341437764,0.8115307258235084,0.8111156597733498,0.809734915693601,0.81245649440421,0.8107472111781439,0.8142983143528303,0.811898218260871,0.810776530040635,0.8105492600136333,0.8095806646678183,0.8116336779461967,0.8097130929430326,0.8096899365385374,0.8107428260975413,0.8110767743653722,0.8109778496954176,0.8098862311906285,0.8089939595924484,0.8101827378074328,0.8108243453833792,0.8120836946699355,0.8082339689135551,0.807957267595662,0.8094771454731623,0.8104082768162092,0.808492405547036,0.8103870708081458,0.8104589432477951,0.8091459456417296,0.810585592355993,0.8093242140279876,0.8099943887856271,0.808793005016115,0.8076258624593416,0.8113586538367801,0.8097376839982139,0.8100604663292567,0.8084690388705995,0.8104091129369206,0.8119139166341888,0.8112594733635584,0.8138898892535104,0.8103421206275622,0.8082772079441283,0.8107030822171105,0.8102195204959975,0.809167854487896,0.8107234918408923,0.8094827342364523,0.811169210407469,0.8103660783833928,0.8093764119678073,0.8100129150682025,0.8080117992228932,0.8097083369890848,0.8091091571582688,0.8093836671776242,0.8092050469583936,0.8096156807409393,0.8100153861774338,0.8078603504432572,0.8091030882464515,0.8097435211141905,0.8095412419901954,0.8123257681727409,0.8100725726948844,0.8101088388098611,0.8098367485735152,0.8105335690908961,0.8100520943601927,0.8127586245536804,0.8098231611980332,0.8093780270881124,0.8108117679754893,0.8134042587545183,0.8087049449483553,0.8104718319243855,0.8096484856473075,0.8095983084705141,0.8082724553015497,0.8088472998804517,0.8117495824893316,0.8093009011612998,0.8109937724139955,0.8099640566441748,0.809761594567034,0.811190535624822,0.8100012300742997,0.8091075801187091,0.8108318282498254,0.8091869288020663,0.8115851341022385,0.8122224079238044,0.8102139607071877,0.8097121839721998,0.8099633579452833,0.8092275088032087,0.8097362567981085,0.8104489751987987,0.8100552724467384,0.808338905374209,0.8100043800142076,0.810720455315378,0.8085639493332969,0.8100348421268992,0.8097681121693717,0.8103271532389853,0.8088699239823554,0.8103098447124163,0.8105793297290802,0.8097756091091368,0.8100509602162573,0.8102578818798065,0.8105588662955496,0.8105191306935416,0.8100843967662917,0.8103178532587157,0.8098694301313825,0.8121047996812396,0.8105647870235972,0.8112683710124757,0.8104976829555299,0.8106027651164267,0.811069346136517,0.8110647938317723,0.8086284614271588,0.8117239657375548,0.8126151917709244,0.8083446555667453,0.8111365520291858,0.8091082134180598,0.8099930228458511,0.8116864313681921,0.8102278568678432,0.8116334502895673,0.8111655927366681,0.809258802069558,0.8123238657911619,0.8105195711056391,0.8114890340301726,0.8111863202518887,0.8095002166099019,0.8118020809359021,0.8111421830124326,0.8100830622845225,0.8102089050743315,0.8096883793671926,0.8106564573115773,0.8099208904637231,0.8110433295369148,0.8104339150918854,0.8092053433259329,0.8094202478726705,0.8111050253113111,0.8094772415028678,0.8092649496263928,0.8078268021345139,0.8092855695221159,0.8102016713884141,0.8099121459656291,0.809892963204119,0.8092774922649065,0.8085016392999225,0.8078470635745261,0.8105697168244256,0.8105435685978996,0.8097183207670847,0.8092143932978312,0.8097430542111397,0.8089040832387077,0.8110102017720541,0.8112573797504107,0.8120553882585632,0.8092451956537035,0.8107095774677064,0.8094941973686218,0.810603012641271,0.8107011856304275,0.8096262829171287,0.810675317214595,0.8099411072002517,0.8095780594481362]},\"selected\":{\"id\":\"1186\"},\"selection_policy\":{\"id\":\"1187\"}},\"id\":\"1174\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"blue\",\"line_width\":2,\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1161\",\"type\":\"Line\"},{\"attributes\":{\"text\":\"Minimum validation loss: 0.8076258624593416\"},\"id\":\"1127\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1138\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1148\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1149\",\"type\":\"ResetTool\"},{\"attributes\":{\"line_color\":\"blue\",\"line_width\":2,\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1160\",\"type\":\"Line\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1151\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"1170\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1166\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1145\",\"type\":\"PanTool\"},{\"attributes\":{\"data\":{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299],\"y\":[1.046431214558451,0.9971261045389008,1.239817442182909,0.9703412790047495,0.9691911937897666,0.9618106078683284,0.9540238171293024,0.9648076902356064,1.3393237885675933,0.9379043530999569,0.9371624124677558,1.004440410722766,0.9290333229198791,0.9140418565064146,0.9301487040101436,0.905291105362407,1.0399882860351026,1.3889516523009853,0.8954066067411188,0.8910244824593527,0.8922036637339675,0.8879674072851215,0.8858457414727462,0.9043809395087393,0.8857387099349708,0.8835792451574092,0.8802156356343052,0.8775330976436012,0.894863812546981,0.8771496203907749,0.8729088415179336,0.8705808066485221,0.8758425597558942,0.8697326124760143,0.8885022134111639,0.8697407887693037,0.8698796811856722,0.8699082912060252,0.8654490268021299,0.8658990523271394,0.8655519309796785,0.8627998879081324,0.8618095956350628,0.8622221246100309,0.861195194721222,0.860166923623336,0.9144117671146728,0.8627174228952642,0.8599564799091273,0.8569768432985272,0.8560237250829997,1.0172381995017068,0.8571708424049511,0.8585824744743213,0.8831986366656789,0.8545200180589108,0.8556109292465344,0.8530063949133221,0.8547800250220717,0.8516572674115499,0.8515593200399164,0.8530497469400105,0.8526523625641539,0.8525919861960829,0.8522276087811119,0.8515721143337718,0.8628138445971305,0.8488581268410934,0.8519664661926135,0.8501194155007078,0.8489228706610831,0.8459777721187525,0.8369759335852506,0.8326134704706961,0.8325372618541383,0.8314834036325154,0.831047679248609,0.8303210498993857,0.83007515438816,0.8305586806514806,0.8289718328860768,0.8284396437176487,0.8292770268624289,0.8281539069978814,0.8280027558929042,0.8267710280000118,0.8285332374405443,0.8286259720199987,0.8269112212616101,0.8275452097256978,0.8262435954913758,0.8275640378918564,0.8246310455757275,0.8250277577785023,0.8266123976623803,0.8243475092084784,0.8255871312660084,0.8248144256441217,0.8236834325288471,0.824415129318572,0.8241647737068042,0.8236692489239208,0.8253260583208318,0.8243022042408324,0.8206365704536438,0.8194311102231343,0.8190868450884233,0.8184255583244457,0.8196634482919124,0.8194831808408102,0.8177732484382495,0.8194452078718888,0.8189168643533138,0.8183176446379277,0.8183561274879857,0.8177162325173094,0.8185790831582588,0.8179465262513411,0.81785703709251,0.8176486697113304,0.8182097405718084,0.8175555160171107,0.817315407594045,0.8191857149726466,0.8182463694037052,0.8180986375139471,0.8185568177909182,0.8165240193668165,0.8186415389964455,0.8170295849181058,0.8176279427712424,0.8194198476640802,0.816803879486887,0.8187780430442408,0.8172379581551803,0.8169827206092969,0.8182888200408533,0.8170526293286106,0.8172336797965201,0.816385191156153,0.8165526605488961,0.8178320087884602,0.8173233339661046,0.8163882330844277,0.8183022787696437,0.8161592790954991,0.8164365977571721,0.8162924298068933,0.8174290834811696,0.816568500744669,0.8165438160561679,0.8154962853381508,0.8176993558281347,0.8179641144317493,0.8173195121581094,0.8175246709271481,0.8166644566937497,0.8178971317776462,0.8181751976933396,0.8175703931273075,0.8172030448913574,0.8169967847957945,0.8169027792780023,0.8171025654725861,0.8182173024144089,0.8165051750969469,0.8174639812686987,0.8175832422156083,0.8172656833079823,0.8171071278421502,0.8179752586180704,0.8177629790808025,0.8175216781465631,0.8168133948978625,0.8173463311111718,0.8168766247598749,0.8170528495520876,0.8181096091605069,0.8166160922301443,0.8163508915064628,0.8159551097635638,0.8178197070171959,0.8171643746526618,0.8168654439742105,0.8159463978650278,0.8162889570520635,0.8189284284909566,0.817299833423213,0.8162816616526821,0.816972892117082,0.8171861909983451,0.8171796934646472,0.8172815080274616,0.8177686377575523,0.8169970154762268,0.8160433292388916,0.81681711109061,0.8187670601041693,0.8176981478406672,0.8156885858167682,0.8172707494936491,0.8161363369540164,0.8181469787631118,0.8171731879836635,0.8155244789625469,0.8197648941424855,0.8164407968521118,0.816272967740109,0.8180275266630608,0.8159396355612236,0.8159944440189161,0.8172217136935184,0.8172988609263772,0.816894603193852,0.8175691152873792,0.8168278522658766,0.8159964019792122,0.8167584574013426,0.8169146707183437,0.8166100023085611,0.8176100774815208,0.815788804020798,0.8171021156143724,0.8171141553343388,0.8160834186955502,0.8164533083898979,0.8163743230334499,0.818206915102507,0.8163038565401446,0.8164235734103019,0.8176113963127136,0.816599392472652,0.816977382124516,0.8170442650192662,0.8161106092888012,0.8181760359228702,0.8177154348607649,0.820169450734791,0.9055779908832751,0.8164877420977542,0.8168424315619887,0.8158337503148798,0.8176780702774985,0.8172177958906742,0.8178136743997273,0.8158420073358637,0.8182172919574536,0.8180517353509602,0.8159194712053266,0.8182413366802952,0.8171094860946923,0.8152933677037557,0.816829940938113,0.8169837347248144,0.8173878031864501,0.8162151585545456,0.8166118163811533,0.8172179228381107,0.8179148059142264,0.8175562160056934,0.8175157549088461,0.8155412487816392,0.8168649901423538,0.8163049101829529,0.8165583650271098,0.8169656749357257,0.8164420713458145,0.8585011519883808,0.8168268295756558,0.8182591469664323,0.817021113738679,0.8164257239877132,0.816461329711111,0.817320287855048,0.8167384455078527,0.8173595420101233,0.8161747150253831,0.8185566542441385,0.8178304921116746,0.8171629119337651,0.8158953775439346,0.8169307689917715,0.8175423996490344,0.8167136541584081,0.8165613755845187,0.8164429825648927,0.8176685630229481,0.8182818404415197,0.8168803271494414,0.8170526676010668,0.8164724149202046,0.8181235244399623,0.8172601162341603,0.8161315208987185,0.8166325213616354,0.8192094811222009,0.8174593344069364,0.8159613613496747,0.8170541549983777,0.8176224953249881]},\"selected\":{\"id\":\"1169\"},\"selection_policy\":{\"id\":\"1170\"}},\"id\":\"1159\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1146\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1129\",\"type\":\"DataRange1d\"},{\"attributes\":{\"label\":{\"value\":\"Train RMSE\"},\"renderers\":[{\"id\":\"1162\"}]},\"id\":\"1173\",\"type\":\"LegendItem\"},{\"attributes\":{\"axis\":{\"id\":\"1141\"},\"dimension\":1,\"ticker\":null},\"id\":\"1144\",\"type\":\"Grid\"},{\"attributes\":{\"label\":{\"value\":\"Val RMSE\"},\"renderers\":[{\"id\":\"1177\"}]},\"id\":\"1189\",\"type\":\"LegendItem\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1145\"},{\"id\":\"1146\"},{\"id\":\"1147\"},{\"id\":\"1148\"},{\"id\":\"1149\"},{\"id\":\"1150\"}]},\"id\":\"1152\",\"type\":\"Toolbar\"},{\"attributes\":{\"overlay\":{\"id\":\"1151\"}},\"id\":\"1147\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"formatter\":{\"id\":\"1166\"},\"ticker\":{\"id\":\"1142\"}},\"id\":\"1141\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1135\",\"type\":\"LinearScale\"},{\"attributes\":{\"source\":{\"id\":\"1174\"}},\"id\":\"1178\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1186\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1187\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"line_color\":\"red\",\"line_width\":2,\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1175\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"1168\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"red\",\"line_width\":2,\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1176\",\"type\":\"Line\"}],\"root_ids\":[\"1126\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.1\"}};\n",
       "  var render_items = [{\"docid\":\"0799129a-6b8d-447b-aea7-7d75324470f8\",\"root_ids\":[\"1126\"],\"roots\":{\"1126\":\"52181f20-42cd-4c79-9d93-9ac5a771d260\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1126"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "title = f'Minimum validation loss: {np.min(val_loss)}'\n",
    "p = figure(title = title, plot_width=600, plot_height=250)\n",
    "x = np.arange(len(train_loss))\n",
    "p.line(x, train_loss, line_width=2, color = 'blue', legend_label = 'Train RMSE')\n",
    "p.line(x, val_loss, line_width=2, color = 'red', legend_label = 'Val RMSE')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
