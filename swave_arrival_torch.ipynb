{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os, random\n",
    "import h5py, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "### Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "### Torchvision\n",
    "from torchsummary import summary\n",
    "import torchvision.datasets as dest\n",
    "import torchvision.transforms as transformers\n",
    "\n",
    "### Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.1.min.js\": \"qkRvDQVAIfzsJo40iRBbxt6sttt0hv4lh74DG7OK4MCHv4C5oohXYoHUM5W11uqS\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.1.min.js\": \"Sb7Mr06a9TNlet/GEBeKaf5xH3eb6AlCzwjtU82wNPyDrnfoiVl26qnvlKjmcAd+\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.1.min.js\": \"HaJ15vgfmcfRtB4c4YBOI4f1MUujukqInOWVqZJZZGK7Q+ivud0OKGSTn/Vm2iso\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.1.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.1.min.js\": \"qkRvDQVAIfzsJo40iRBbxt6sttt0hv4lh74DG7OK4MCHv4C5oohXYoHUM5W11uqS\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.1.min.js\": \"Sb7Mr06a9TNlet/GEBeKaf5xH3eb6AlCzwjtU82wNPyDrnfoiVl26qnvlKjmcAd+\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.1.min.js\": \"HaJ15vgfmcfRtB4c4YBOI4f1MUujukqInOWVqZJZZGK7Q+ivud0OKGSTn/Vm2iso\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### seed_everythin\n",
    "seed = 1987\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "ngpu = 1\n",
    "batch_size = 512\n",
    "num_workers = 1\n",
    "channels = 3\n",
    "data_range = 10\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'selected_data.pkl'\n",
    "outfile = open(filename,'rb')\n",
    "data = pickle.load(outfile)\n",
    "xdata = np.array(data['data'], dtype = 'float32')\n",
    "ylablel = np.array(data['labels'], dtype = 'float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((145864, 50, 3), (36466, 50, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(xdata, ylablel, test_size=0.2)\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xscalers = {}\n",
    "for i in range(xdata.shape[2]):\n",
    "    xscalers[i] = StandardScaler()\n",
    "    X_train[:, :, i] = xscalers[i].fit_transform(X_train[:, :, i])\n",
    "\n",
    "yscaler = StandardScaler()\n",
    "y_train = yscaler.fit_transform(y_train.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "for i in range(xdata.shape[2]):\n",
    "    X_val[:, :, i] = xscalers[i].transform(X_val[:, :, i])\n",
    "\n",
    "y_val = yscaler.transform(y_val.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.706501, 7.02627)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.max(), y_train.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset\n",
    "class SeismicDataset(Dataset):\n",
    "    def __init__(self, method):\n",
    "        if method == 'train':\n",
    "            shp = X_train.shape\n",
    "            self.xdata = torch.Tensor(X_train).view(shp[0], shp[2], shp[1])[:,:, 0:data_range]\n",
    "            self.ylablel = torch.Tensor(y_train)\n",
    "        elif method == 'val':\n",
    "            shp = X_val.shape\n",
    "            self.xdata = torch.Tensor(X_val).view(shp[0], shp[2], shp[1])[:,:, 0:data_range]\n",
    "            self.ylablel = torch.Tensor(y_val)\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.xdata)\n",
    "        \n",
    "    def __getitem__(self, indx):\n",
    "        return self.xdata[indx], self.ylablel[indx]\n",
    "        \n",
    "### Dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    SeismicDataset('train'),\n",
    "    batch_size= batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = num_workers\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    SeismicDataset('val'),\n",
    "    batch_size= batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 3, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels,  bias = False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.batchnorm(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1_pool):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        self.branch1 = ConvBlock(in_channels, out_1x1, kernel_size = 1)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBlock(in_channels, red_3x3, kernel_size = 1),\n",
    "            ConvBlock(red_3x3, out_3x3, kernel_size = 3, padding = 1)\n",
    "        )\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            ConvBlock(in_channels, red_5x5, kernel_size = 1),\n",
    "            ConvBlock(red_5x5, out_5x5, kernel_size = 5, padding = 2)\n",
    "        )\n",
    "        \n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=3, stride = 1, padding = 1),\n",
    "            ConvBlock(in_channels, out_1x1_pool, kernel_size = 1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat(\n",
    "            [self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], 1\n",
    "        )\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeismicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SeismicNet, self).__init__()\n",
    "        self.conv1 = ConvBlock(3, 192, kernel_size = 2, stride = 2)\n",
    "        self.inception_1a = InceptionBlock(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception_1b = InceptionBlock(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=3, stride=2, padding = 1)\n",
    "        \n",
    "        self.averagepool1 = nn.AvgPool1d(kernel_size= 7, stride= 1)\n",
    "        self.fc1 = nn.Linear(3360, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "        self.dropout1 = nn.Dropout2d(p = 0.25)\n",
    "        self.dropout2 = nn.Dropout2d(p = 0.20)\n",
    "        self.dropout3 = nn.Dropout2d(p = 0.15)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.inception_1a(x)\n",
    "        x = self.inception_1b(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.averagepool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.input_dim = channels*data_range\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.35),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.35),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = Variable(torch.flatten(x, start_dim = 1))\n",
    "        x = self.main(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.input_dim = channels*data_range\n",
    "#         self.conv1 = nn.Conv1d(3, 20, kernel_size = 3, stride = 2)\n",
    "#         self.bn1 = nn.BatchNorm1d(20)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_dim, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dp1 = nn.Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dp2 = nn.Dropout(0.25)\n",
    "        self.fc3 = nn.Linear(64, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(x)\n",
    "        \n",
    "        x = Variable(torch.flatten(x, start_dim = 1))\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dp1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    \"\"\"initialize weight of Linear layer as constant_weight\"\"\"\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0)\n",
    "    if type(m) == nn.Conv1d:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel().to(device) #SeismicNet().to(device)\n",
    "init_weights(model)\n",
    "if (device.type == 'cuda' and (ngpu> 1)):\n",
    "    model = nn.DataParallel(model, list(range(ngpu)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = SummaryWriter()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.003)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=10, factor = 0.15, verbose=True\n",
    ")\n",
    "\n",
    "# scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0003, max_lr=0.1, cycle_momentum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for j, (data, label) in enumerate(train_dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        data, y = data.to(device), label.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "               \n",
    "        tb.add_histogram('gradients/fc1', model.fc1.weight.grad, epoch)\n",
    "        tb.add_histogram('gradients/fc2', model.fc2.weight.grad, epoch)\n",
    "        tb.add_histogram('gradients/fc3', model.fc3.weight.grad, epoch)\n",
    "        tb.add_histogram('gradients/fc4', model.fc4.weight.grad, epoch)\n",
    "        \n",
    "        tb.add_scalars('fc_grad/mean',{\n",
    "            'fc1': model.fc1.weight.grad.abs().mean(),\n",
    "            'fc2': model.fc2.weight.grad.abs().mean(),\n",
    "            'fc3': model.fc3.weight.grad.abs().mean(),\n",
    "            'fc4': model.fc4.weight.grad.abs().mean()\n",
    "            \n",
    "        }, epoch)\n",
    "        \n",
    "        tb.add_scalars('fc_grad/variance',{\n",
    "            'fc1': model.fc1.weight.grad.abs().var(),\n",
    "            'fc2': model.fc2.weight.grad.abs().var(),\n",
    "            'fc3': model.fc3.weight.grad.abs().var(),\n",
    "            'fc4': model.fc4.weight.grad.abs().var()\n",
    "            \n",
    "        }, epoch)\n",
    "        \n",
    "        tb.add_scalars('fc_grad/max',{\n",
    "            'fc1': model.fc1.weight.grad.abs().max(),\n",
    "            'fc2': model.fc2.weight.grad.abs().max(),\n",
    "            'fc3': model.fc3.weight.grad.abs().max(),\n",
    "            'fc4': model.fc4.weight.grad.abs().max()\n",
    "            \n",
    "        }, epoch)\n",
    "        \n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    mean_loss = sum(losses)/len(losses)\n",
    "    return mean_loss\n",
    "        \n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        for j, (data, label) in enumerate(val_dataloader, 0):\n",
    "            data = data.to(device)\n",
    "            y = label.to(device)\n",
    "            output = model(data)\n",
    "            losses.append(criterion(output, y).item())\n",
    "    return sum(losses)/len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 0/300 | Train loss: 1.0334288963100366 | Val loss: 0.9922582672701942\n",
      "| Epoch: 1/300 | Train loss: 0.998504044089401 | Val loss: 0.9931024867627356\n",
      "| Epoch: 2/300 | Train loss: 0.9940716214347304 | Val loss: 0.984762547744645\n",
      "| Epoch: 3/300 | Train loss: 0.9959930767092788 | Val loss: 0.9843847494986322\n",
      "| Epoch: 4/300 | Train loss: 0.9909853483501233 | Val loss: 0.9851949082480537\n",
      "| Epoch: 5/300 | Train loss: 0.9867492880737573 | Val loss: 0.9805067421661483\n",
      "| Epoch: 6/300 | Train loss: 0.9855387650038067 | Val loss: 0.9824621048238542\n",
      "| Epoch: 7/300 | Train loss: 0.9873436239727756 | Val loss: 0.986379973590374\n",
      "| Epoch: 8/300 | Train loss: 0.9862075364380553 | Val loss: 0.9792261314060953\n",
      "| Epoch: 9/300 | Train loss: 0.9840352445317988 | Val loss: 0.9816234078672197\n",
      "| Epoch: 10/300 | Train loss: 0.9843720028274938 | Val loss: 0.9789395183324814\n",
      "| Epoch: 11/300 | Train loss: 0.9850000506953189 | Val loss: 0.9790899455547333\n",
      "| Epoch: 12/300 | Train loss: 0.9823251431448418 | Val loss: 0.9805436448918449\n",
      "| Epoch: 13/300 | Train loss: 0.9875350778562981 | Val loss: 0.9752378654148843\n",
      "| Epoch: 14/300 | Train loss: 0.9947850984439516 | Val loss: 0.9711804986000061\n",
      "| Epoch: 15/300 | Train loss: 0.9843198017070168 | Val loss: 0.978230838974317\n",
      "| Epoch: 16/300 | Train loss: 1.0145523249057302 | Val loss: 0.9767037414842181\n",
      "| Epoch: 17/300 | Train loss: 0.9806772506027891 | Val loss: 0.9783105933003955\n",
      "| Epoch: 18/300 | Train loss: 0.9805430280534845 | Val loss: 0.976051174932056\n",
      "| Epoch: 19/300 | Train loss: 0.9817254091563977 | Val loss: 0.9705656742056211\n",
      "| Epoch: 20/300 | Train loss: 0.9805183665794238 | Val loss: 0.9830229199594922\n",
      "| Epoch: 21/300 | Train loss: 0.9796897965565062 | Val loss: 0.9738082695338461\n",
      "| Epoch: 22/300 | Train loss: 0.9793175318784881 | Val loss: 0.9775928325123258\n",
      "| Epoch: 23/300 | Train loss: 0.9780614863362229 | Val loss: 0.9700358038147291\n",
      "| Epoch: 24/300 | Train loss: 0.974752914696409 | Val loss: 0.9708643605311712\n",
      "| Epoch: 25/300 | Train loss: 0.9808251023292541 | Val loss: 0.9729005669554075\n",
      "| Epoch: 26/300 | Train loss: 0.9763320544309784 | Val loss: 0.9724673057595888\n",
      "| Epoch: 27/300 | Train loss: 0.9809613200656154 | Val loss: 0.9721786139739884\n",
      "| Epoch: 28/300 | Train loss: 0.9771394704517565 | Val loss: 0.9705885193414159\n",
      "| Epoch: 29/300 | Train loss: 0.9765773499221132 | Val loss: 0.9660152122378349\n",
      "| Epoch: 30/300 | Train loss: 0.9741248875333551 | Val loss: 0.9740442029303975\n",
      "| Epoch: 31/300 | Train loss: 0.9756703548264085 | Val loss: 0.9726009940107664\n",
      "| Epoch: 32/300 | Train loss: 0.9787196991736429 | Val loss: 0.9806113243103027\n",
      "| Epoch: 33/300 | Train loss: 0.9807283792579383 | Val loss: 0.9861281696293089\n",
      "| Epoch: 34/300 | Train loss: 0.9846269996542679 | Val loss: 0.9840213846829202\n",
      "| Epoch: 35/300 | Train loss: 0.9820626549553453 | Val loss: 0.9787274069256253\n",
      "| Epoch: 36/300 | Train loss: 0.9953461316593907 | Val loss: 0.9823408408297433\n",
      "| Epoch: 37/300 | Train loss: 0.9810896743807876 | Val loss: 0.9692030731174681\n",
      "| Epoch: 38/300 | Train loss: 0.9786127163652788 | Val loss: 0.9729582650793923\n",
      "| Epoch: 39/300 | Train loss: 0.9822722606491624 | Val loss: 0.9820884731080797\n",
      "| Epoch: 40/300 | Train loss: 0.9805240122895492 | Val loss: 0.9689310251010789\n",
      "Epoch    41: reducing learning rate of group 0 to 4.5000e-04.\n",
      "| Epoch: 41/300 | Train loss: 0.9643745936845478 | Val loss: 0.9576036880413691\n",
      "| Epoch: 42/300 | Train loss: 0.9591801043142352 | Val loss: 0.9549025677972369\n",
      "| Epoch: 43/300 | Train loss: 0.9578995031222962 | Val loss: 0.9516536287135549\n",
      "| Epoch: 44/300 | Train loss: 0.9565963881057605 | Val loss: 0.9499814882874489\n",
      "| Epoch: 45/300 | Train loss: 0.9559136643744351 | Val loss: 0.9496711144844691\n",
      "| Epoch: 46/300 | Train loss: 0.9533569973811769 | Val loss: 0.9461927637457848\n",
      "| Epoch: 47/300 | Train loss: 0.9527000228563944 | Val loss: 0.9473491013050079\n",
      "| Epoch: 48/300 | Train loss: 0.9512391920675312 | Val loss: 0.9453555602166388\n",
      "| Epoch: 49/300 | Train loss: 0.9521434583162006 | Val loss: 0.9475467759701941\n",
      "| Epoch: 50/300 | Train loss: 0.9508232583079421 | Val loss: 0.9441518518659804\n",
      "| Epoch: 51/300 | Train loss: 0.9499379666228044 | Val loss: 0.9463593918416235\n",
      "| Epoch: 52/300 | Train loss: 0.9501703829096074 | Val loss: 0.9470600419574313\n",
      "| Epoch: 53/300 | Train loss: 0.9492271423339844 | Val loss: 0.9454877807034386\n",
      "| Epoch: 54/300 | Train loss: 0.9500262473758898 | Val loss: 0.9443304729130533\n",
      "| Epoch: 55/300 | Train loss: 0.9492450308381466 | Val loss: 0.9440725727213753\n",
      "| Epoch: 56/300 | Train loss: 0.9494198962261803 | Val loss: 0.9448396033710904\n",
      "| Epoch: 57/300 | Train loss: 0.9492342595468488 | Val loss: 0.9409114643931389\n",
      "| Epoch: 58/300 | Train loss: 0.9491658417802108 | Val loss: 0.9463727118240463\n",
      "| Epoch: 59/300 | Train loss: 0.9482704440752665 | Val loss: 0.938861269917753\n",
      "| Epoch: 60/300 | Train loss: 0.9477496030037863 | Val loss: 0.9437217232253816\n",
      "| Epoch: 61/300 | Train loss: 0.9497327302631579 | Val loss: 0.943556923005316\n",
      "| Epoch: 62/300 | Train loss: 0.9481141395736159 | Val loss: 0.9436360307865672\n",
      "| Epoch: 63/300 | Train loss: 0.9466544393907513 | Val loss: 0.9417403390010198\n",
      "| Epoch: 64/300 | Train loss: 0.9489661852518717 | Val loss: 0.9448001873162057\n",
      "| Epoch: 65/300 | Train loss: 0.9483282731290449 | Val loss: 0.9425798108180364\n",
      "| Epoch: 66/300 | Train loss: 0.946803639437023 | Val loss: 0.9403496094875865\n",
      "| Epoch: 67/300 | Train loss: 0.9481697005138062 | Val loss: 0.9386386689212587\n",
      "| Epoch: 68/300 | Train loss: 0.946922182827665 | Val loss: 0.9409268995126089\n",
      "| Epoch: 69/300 | Train loss: 0.9453539350576569 | Val loss: 0.9378230422735214\n",
      "| Epoch: 70/300 | Train loss: 0.946466146644793 | Val loss: 0.9436746678418584\n",
      "| Epoch: 71/300 | Train loss: 0.9456815778163441 | Val loss: 0.9333481043577194\n",
      "| Epoch: 72/300 | Train loss: 0.9469877830722876 | Val loss: 0.9400633614924219\n",
      "| Epoch: 73/300 | Train loss: 0.946292275294923 | Val loss: 0.9347043989433182\n",
      "| Epoch: 74/300 | Train loss: 0.9462349912576508 | Val loss: 0.9375963972674476\n",
      "| Epoch: 75/300 | Train loss: 0.9479165395100911 | Val loss: 0.9435157312287225\n",
      "| Epoch: 76/300 | Train loss: 0.9461076757364105 | Val loss: 0.937469832599163\n",
      "| Epoch: 77/300 | Train loss: 0.9460580809074536 | Val loss: 0.9403787122832404\n",
      "| Epoch: 78/300 | Train loss: 0.9456008323451929 | Val loss: 0.9380005631181929\n",
      "| Epoch: 79/300 | Train loss: 0.9468181173006693 | Val loss: 0.940279902683364\n",
      "| Epoch: 80/300 | Train loss: 0.9466972652234529 | Val loss: 0.9460883869065179\n",
      "| Epoch: 81/300 | Train loss: 0.946055811539031 | Val loss: 0.9396175791819891\n",
      "| Epoch: 82/300 | Train loss: 0.9453444221563506 | Val loss: 0.9379457549916373\n",
      "Epoch    83: reducing learning rate of group 0 to 6.7500e-05.\n",
      "| Epoch: 83/300 | Train loss: 0.9387514988581339 | Val loss: 0.9345464242829217\n",
      "| Epoch: 84/300 | Train loss: 0.936902932325999 | Val loss: 0.9334683823916647\n",
      "| Epoch: 85/300 | Train loss: 0.935850827735767 | Val loss: 0.9319880795147684\n",
      "| Epoch: 86/300 | Train loss: 0.9351398829828229 | Val loss: 0.9330692721737756\n",
      "| Epoch: 87/300 | Train loss: 0.9350552734575773 | Val loss: 0.933320988383558\n",
      "| Epoch: 88/300 | Train loss: 0.9355861893871374 | Val loss: 0.9321070081657834\n",
      "| Epoch: 89/300 | Train loss: 0.9351558706216645 | Val loss: 0.9302130672666762\n",
      "| Epoch: 90/300 | Train loss: 0.9350110125123409 | Val loss: 0.9286894069777595\n",
      "| Epoch: 91/300 | Train loss: 0.9344050539167303 | Val loss: 0.9300193339586258\n",
      "| Epoch: 92/300 | Train loss: 0.9344660068813123 | Val loss: 0.9312468071778616\n",
      "| Epoch: 93/300 | Train loss: 0.9389040465940509 | Val loss: 0.9305252383152643\n",
      "| Epoch: 94/300 | Train loss: 0.9345756438740512 | Val loss: 0.9300142584575547\n",
      "| Epoch: 95/300 | Train loss: 0.9342750486574675 | Val loss: 0.9306162181827757\n",
      "| Epoch: 96/300 | Train loss: 0.9340883652369182 | Val loss: 0.9323971155616972\n",
      "| Epoch: 97/300 | Train loss: 0.9346569211859452 | Val loss: 0.9277369164758258\n",
      "| Epoch: 98/300 | Train loss: 0.9345368075789067 | Val loss: 0.9325237489408917\n",
      "| Epoch: 99/300 | Train loss: 0.9343223180687218 | Val loss: 0.9292431680692567\n",
      "| Epoch: 100/300 | Train loss: 0.9350917172013667 | Val loss: 0.9275608443551593\n",
      "| Epoch: 101/300 | Train loss: 0.9341238634628162 | Val loss: 0.9328259237938457\n",
      "| Epoch: 102/300 | Train loss: 0.935467681550143 | Val loss: 0.9312874418165948\n",
      "| Epoch: 103/300 | Train loss: 0.9340851181431821 | Val loss: 0.9296782008475728\n",
      "| Epoch: 104/300 | Train loss: 0.9340418117088184 | Val loss: 0.9297164736522568\n",
      "| Epoch: 105/300 | Train loss: 0.9343240775560078 | Val loss: 0.9318266900049316\n",
      "| Epoch: 106/300 | Train loss: 0.9341926664636846 | Val loss: 0.9296879842877388\n",
      "| Epoch: 107/300 | Train loss: 0.9335398743027135 | Val loss: 0.9304626542660925\n",
      "| Epoch: 108/300 | Train loss: 0.935367795040733 | Val loss: 0.929543679787053\n",
      "| Epoch: 109/300 | Train loss: 0.9340237650954932 | Val loss: 0.9278319138619635\n",
      "| Epoch: 110/300 | Train loss: 0.9333514644388567 | Val loss: 0.9278176253040632\n",
      "| Epoch: 111/300 | Train loss: 0.9334619001338357 | Val loss: 0.9303185310628679\n",
      "Epoch   112: reducing learning rate of group 0 to 1.0125e-05.\n",
      "| Epoch: 112/300 | Train loss: 0.9321994860967 | Val loss: 0.9279032738672363\n",
      "| Epoch: 113/300 | Train loss: 0.9309026659580699 | Val loss: 0.9296980640954442\n",
      "| Epoch: 114/300 | Train loss: 0.9325565296306945 | Val loss: 0.9264102809958987\n",
      "| Epoch: 115/300 | Train loss: 0.9317013240697091 | Val loss: 0.9269530814554956\n",
      "| Epoch: 116/300 | Train loss: 0.9322087327639262 | Val loss: 0.9294258339537514\n",
      "| Epoch: 117/300 | Train loss: 0.9319878530084041 | Val loss: 0.9277097309629122\n",
      "| Epoch: 118/300 | Train loss: 0.9318840857137713 | Val loss: 0.9260333130757014\n",
      "| Epoch: 119/300 | Train loss: 0.9321153230834426 | Val loss: 0.9306820813152525\n",
      "| Epoch: 120/300 | Train loss: 0.9321391009447867 | Val loss: 0.9254614172710313\n",
      "| Epoch: 121/300 | Train loss: 0.9322790672904566 | Val loss: 0.9276389239562882\n",
      "| Epoch: 122/300 | Train loss: 0.9312998696377403 | Val loss: 0.9277038358979754\n",
      "| Epoch: 123/300 | Train loss: 0.93154341714424 | Val loss: 0.928006089395947\n",
      "| Epoch: 124/300 | Train loss: 0.9317013694528948 | Val loss: 0.9271529341737429\n",
      "| Epoch: 125/300 | Train loss: 0.9324590011646873 | Val loss: 0.9302559238341119\n",
      "| Epoch: 126/300 | Train loss: 0.9320831702466597 | Val loss: 0.9263952117827203\n",
      "| Epoch: 127/300 | Train loss: 0.9321769802193892 | Val loss: 0.9257334321737289\n",
      "| Epoch: 128/300 | Train loss: 0.9326267491307175 | Val loss: 0.9280457844336828\n",
      "| Epoch: 129/300 | Train loss: 0.9313952059076543 | Val loss: 0.9274594684441885\n",
      "| Epoch: 130/300 | Train loss: 0.932072745498858 | Val loss: 0.9283667579293251\n",
      "| Epoch: 131/300 | Train loss: 0.932090501408828 | Val loss: 0.9266295548942354\n",
      "Epoch   132: reducing learning rate of group 0 to 1.5188e-06.\n",
      "| Epoch: 132/300 | Train loss: 0.9313020748004579 | Val loss: 0.928119550148646\n",
      "| Epoch: 133/300 | Train loss: 0.9311668324888798 | Val loss: 0.927215963602066\n",
      "| Epoch: 134/300 | Train loss: 0.9321647472548903 | Val loss: 0.926291286945343\n",
      "| Epoch: 135/300 | Train loss: 0.9309227985248231 | Val loss: 0.9298002852333916\n",
      "| Epoch: 136/300 | Train loss: 0.9319921399417677 | Val loss: 0.92610930899779\n",
      "| Epoch: 137/300 | Train loss: 0.9305695194947092 | Val loss: 0.9268261194229126\n",
      "| Epoch: 138/300 | Train loss: 0.9315890328925952 | Val loss: 0.9277568840318255\n",
      "| Epoch: 139/300 | Train loss: 0.9318438180705957 | Val loss: 0.9254938347472085\n",
      "| Epoch: 140/300 | Train loss: 0.930866035034782 | Val loss: 0.9271979480981827\n",
      "| Epoch: 141/300 | Train loss: 0.9322545494949609 | Val loss: 0.9288510059316953\n",
      "| Epoch: 142/300 | Train loss: 0.9314383709639834 | Val loss: 0.9269117762645086\n",
      "Epoch   143: reducing learning rate of group 0 to 2.2781e-07.\n",
      "| Epoch: 143/300 | Train loss: 0.93217128640727 | Val loss: 0.9285441032714314\n",
      "| Epoch: 144/300 | Train loss: 0.9318742764623542 | Val loss: 0.9271291154954169\n",
      "| Epoch: 145/300 | Train loss: 0.9316892186800639 | Val loss: 0.92760012878312\n",
      "| Epoch: 146/300 | Train loss: 0.9319273547122353 | Val loss: 0.9265561981333627\n",
      "| Epoch: 147/300 | Train loss: 0.9314668548734565 | Val loss: 0.9279889389872551\n",
      "| Epoch: 148/300 | Train loss: 0.9308776847103186 | Val loss: 0.9265345732371012\n",
      "| Epoch: 149/300 | Train loss: 0.9307814683830529 | Val loss: 0.9281992647382948\n",
      "| Epoch: 150/300 | Train loss: 0.9316337096063714 | Val loss: 0.9293611471851667\n",
      "| Epoch: 151/300 | Train loss: 0.9306036526696724 | Val loss: 0.9271935762630569\n",
      "| Epoch: 152/300 | Train loss: 0.9308338834528338 | Val loss: 0.9262514760096868\n",
      "| Epoch: 153/300 | Train loss: 0.9308943016487256 | Val loss: 0.9281664904620912\n",
      "Epoch   154: reducing learning rate of group 0 to 3.4172e-08.\n",
      "| Epoch: 154/300 | Train loss: 0.931900703907013 | Val loss: 0.9276819941070344\n",
      "| Epoch: 155/300 | Train loss: 0.931661261591995 | Val loss: 0.9274374801251624\n",
      "| Epoch: 156/300 | Train loss: 0.9305706277228238 | Val loss: 0.9267660371131368\n",
      "| Epoch: 157/300 | Train loss: 0.9316761219710634 | Val loss: 0.9272636746366819\n",
      "| Epoch: 158/300 | Train loss: 0.9319199505605196 | Val loss: 0.9258169713947508\n",
      "| Epoch: 159/300 | Train loss: 0.9317813254239267 | Val loss: 0.9261224493384361\n",
      "| Epoch: 160/300 | Train loss: 0.9316922348842286 | Val loss: 0.9272180199623108\n",
      "| Epoch: 161/300 | Train loss: 0.9325768801203945 | Val loss: 0.9272375644909011\n",
      "| Epoch: 162/300 | Train loss: 0.9316054808466058 | Val loss: 0.9291282850835059\n",
      "| Epoch: 163/300 | Train loss: 0.9312317419470402 | Val loss: 0.9272914123204019\n",
      "| Epoch: 164/300 | Train loss: 0.931112169592004 | Val loss: 0.9263073321845796\n",
      "Epoch   165: reducing learning rate of group 0 to 5.1258e-09.\n",
      "| Epoch: 165/300 | Train loss: 0.931937725502148 | Val loss: 0.9279696353607707\n",
      "| Epoch: 166/300 | Train loss: 0.9308904610182109 | Val loss: 0.9295904023779763\n",
      "| Epoch: 167/300 | Train loss: 0.9311253911570498 | Val loss: 0.9271457071105639\n",
      "| Epoch: 168/300 | Train loss: 0.932217604444738 | Val loss: 0.9286556334959136\n",
      "| Epoch: 169/300 | Train loss: 0.9315843684631482 | Val loss: 0.9286629698342748\n",
      "| Epoch: 170/300 | Train loss: 0.9313220823020266 | Val loss: 0.9288882729079988\n",
      "| Epoch: 171/300 | Train loss: 0.931200941612846 | Val loss: 0.9242815574010214\n",
      "| Epoch: 172/300 | Train loss: 0.9321548018539161 | Val loss: 0.9272496410542064\n",
      "| Epoch: 173/300 | Train loss: 0.9324849273029127 | Val loss: 0.9294475060370233\n",
      "| Epoch: 174/300 | Train loss: 0.9313530296610113 | Val loss: 0.9278000162707435\n",
      "| Epoch: 175/300 | Train loss: 0.9307960131712127 | Val loss: 0.9263621436225044\n",
      "| Epoch: 176/300 | Train loss: 0.9315734350890444 | Val loss: 0.9270298712783389\n",
      "| Epoch: 177/300 | Train loss: 0.9322398223375019 | Val loss: 0.927596413426929\n",
      "| Epoch: 178/300 | Train loss: 0.9313013329840543 | Val loss: 0.9246765772501627\n",
      "| Epoch: 179/300 | Train loss: 0.9320004691157424 | Val loss: 0.9260841152734227\n",
      "| Epoch: 180/300 | Train loss: 0.9317181842368946 | Val loss: 0.9278516496221224\n",
      "| Epoch: 181/300 | Train loss: 0.9323713984405785 | Val loss: 0.9286156305008464\n",
      "| Epoch: 182/300 | Train loss: 0.930891317861122 | Val loss: 0.9264053247041173\n",
      "| Epoch: 183/300 | Train loss: 0.931840369575902 | Val loss: 0.9269906373487579\n",
      "| Epoch: 184/300 | Train loss: 0.9316919977204842 | Val loss: 0.9298145572344462\n",
      "| Epoch: 185/300 | Train loss: 0.9310535943299009 | Val loss: 0.9275694919957055\n",
      "| Epoch: 186/300 | Train loss: 0.9314605035279927 | Val loss: 0.9277794477012422\n",
      "| Epoch: 187/300 | Train loss: 0.9320630236675865 | Val loss: 0.9269409047232734\n",
      "| Epoch: 188/300 | Train loss: 0.9317342540674042 | Val loss: 0.9253064120809237\n",
      "| Epoch: 189/300 | Train loss: 0.9316366204044275 | Val loss: 0.9269154775473807\n",
      "| Epoch: 190/300 | Train loss: 0.9316859722137452 | Val loss: 0.9270525731974177\n",
      "| Epoch: 191/300 | Train loss: 0.9317230728634617 | Val loss: 0.9277065305246247\n",
      "| Epoch: 192/300 | Train loss: 0.9310492302242078 | Val loss: 0.9298500170310339\n",
      "| Epoch: 193/300 | Train loss: 0.9309057746017189 | Val loss: 0.928337622847822\n",
      "| Epoch: 194/300 | Train loss: 0.9316546893956369 | Val loss: 0.9262383075224029\n",
      "| Epoch: 195/300 | Train loss: 0.9311485591687654 | Val loss: 0.9292826842930582\n",
      "| Epoch: 196/300 | Train loss: 0.9309359636223107 | Val loss: 0.927699126303196\n",
      "| Epoch: 197/300 | Train loss: 0.93073130741454 | Val loss: 0.9270053025748994\n",
      "| Epoch: 198/300 | Train loss: 0.9315414062717504 | Val loss: 0.9281534958216879\n",
      "| Epoch: 199/300 | Train loss: 0.9316120630816409 | Val loss: 0.9286740769942602\n",
      "| Epoch: 200/300 | Train loss: 0.93140231580065 | Val loss: 0.9263516407873895\n",
      "| Epoch: 201/300 | Train loss: 0.9323971932394463 | Val loss: 0.9273599435885748\n",
      "| Epoch: 202/300 | Train loss: 0.931685429497769 | Val loss: 0.9264673151903682\n",
      "| Epoch: 203/300 | Train loss: 0.9316727790916175 | Val loss: 0.929085718260871\n",
      "| Epoch: 204/300 | Train loss: 0.9310609369947199 | Val loss: 0.9279395457771089\n",
      "| Epoch: 205/300 | Train loss: 0.9314659622677586 | Val loss: 0.9293949579199156\n",
      "| Epoch: 206/300 | Train loss: 0.9316557010014852 | Val loss: 0.9251619569129415\n",
      "| Epoch: 207/300 | Train loss: 0.9310743725090697 | Val loss: 0.9265172895458009\n",
      "| Epoch: 208/300 | Train loss: 0.9311684535260786 | Val loss: 0.9274059144987\n",
      "| Epoch: 209/300 | Train loss: 0.931056715312757 | Val loss: 0.9277438670396805\n",
      "| Epoch: 210/300 | Train loss: 0.9321302234080799 | Val loss: 0.9288927805092599\n",
      "| Epoch: 211/300 | Train loss: 0.9318519282759281 | Val loss: 0.9260867196652625\n",
      "| Epoch: 212/300 | Train loss: 0.9313365469899094 | Val loss: 0.9296471012963189\n",
      "| Epoch: 213/300 | Train loss: 0.9322366128888047 | Val loss: 0.9264069356852107\n",
      "| Epoch: 214/300 | Train loss: 0.93101319011889 | Val loss: 0.927885970307721\n",
      "| Epoch: 215/300 | Train loss: 0.9317116883763096 | Val loss: 0.9287861792577637\n",
      "| Epoch: 216/300 | Train loss: 0.9308416241093685 | Val loss: 0.9260682397418551\n",
      "| Epoch: 217/300 | Train loss: 0.9310140764504148 | Val loss: 0.9278825918833414\n",
      "| Epoch: 218/300 | Train loss: 0.930929278281697 | Val loss: 0.9287150858177079\n",
      "| Epoch: 219/300 | Train loss: 0.9320672813214754 | Val loss: 0.9270598888397217\n",
      "| Epoch: 220/300 | Train loss: 0.9309654681306136 | Val loss: 0.926599894132879\n",
      "| Epoch: 221/300 | Train loss: 0.9310808047913668 | Val loss: 0.9275563491715325\n",
      "| Epoch: 222/300 | Train loss: 0.9306070382134957 | Val loss: 0.9285272500581212\n",
      "| Epoch: 223/300 | Train loss: 0.9316964396259241 | Val loss: 0.9251180738210678\n",
      "| Epoch: 224/300 | Train loss: 0.9318534349140368 | Val loss: 0.9282131062613593\n",
      "| Epoch: 225/300 | Train loss: 0.9308460179128145 | Val loss: 0.9279748052358627\n",
      "| Epoch: 226/300 | Train loss: 0.9313984214213856 | Val loss: 0.927964319785436\n",
      "| Epoch: 227/300 | Train loss: 0.9335872750533255 | Val loss: 0.9287296111385027\n",
      "| Epoch: 228/300 | Train loss: 0.9313801470555757 | Val loss: 0.9282232208384408\n",
      "| Epoch: 229/300 | Train loss: 0.9315749268782766 | Val loss: 0.9261349919769499\n",
      "| Epoch: 230/300 | Train loss: 0.9313866470989428 | Val loss: 0.9260479584336281\n",
      "| Epoch: 231/300 | Train loss: 0.9315213682358725 | Val loss: 0.9268038695057234\n",
      "| Epoch: 232/300 | Train loss: 0.9317558441245765 | Val loss: 0.9274659496214654\n",
      "| Epoch: 233/300 | Train loss: 0.9304879027500487 | Val loss: 0.9298304443558058\n",
      "| Epoch: 234/300 | Train loss: 0.9312598606996368 | Val loss: 0.9249821503957113\n",
      "| Epoch: 235/300 | Train loss: 0.9315217043224134 | Val loss: 0.9277461907929845\n",
      "| Epoch: 236/300 | Train loss: 0.9308900341652987 | Val loss: 0.9254525634977553\n",
      "| Epoch: 237/300 | Train loss: 0.9309145776849044 | Val loss: 0.9278863734669156\n",
      "| Epoch: 238/300 | Train loss: 0.9318206007020515 | Val loss: 0.9262832875053088\n",
      "| Epoch: 239/300 | Train loss: 0.9310024336764687 | Val loss: 0.9294701168934504\n",
      "| Epoch: 240/300 | Train loss: 0.9319658459278575 | Val loss: 0.928447818590535\n",
      "| Epoch: 241/300 | Train loss: 0.9311251537841663 | Val loss: 0.9277535768018829\n",
      "| Epoch: 242/300 | Train loss: 0.9314985139328137 | Val loss: 0.9265411520997683\n",
      "| Epoch: 243/300 | Train loss: 0.9314481137091654 | Val loss: 0.9253456617395083\n",
      "| Epoch: 244/300 | Train loss: 0.9321020647099144 | Val loss: 0.9272684612207942\n",
      "| Epoch: 245/300 | Train loss: 0.9314964633238944 | Val loss: 0.9274447982509931\n",
      "| Epoch: 246/300 | Train loss: 0.9300486949452182 | Val loss: 0.9275188917915026\n",
      "| Epoch: 247/300 | Train loss: 0.9311205707098308 | Val loss: 0.9282186155517896\n",
      "| Epoch: 248/300 | Train loss: 0.9312306391565424 | Val loss: 0.9276970699429512\n",
      "| Epoch: 249/300 | Train loss: 0.932435256974739 | Val loss: 0.9266874218980471\n",
      "| Epoch: 250/300 | Train loss: 0.9313235561052958 | Val loss: 0.9267003726628091\n",
      "| Epoch: 251/300 | Train loss: 0.9314371416443272 | Val loss: 0.9284050050708983\n",
      "| Epoch: 252/300 | Train loss: 0.9314471899417409 | Val loss: 0.9262391221192148\n",
      "| Epoch: 253/300 | Train loss: 0.9317714653517071 | Val loss: 0.9288807287812233\n",
      "| Epoch: 254/300 | Train loss: 0.9320327808982447 | Val loss: 0.9270139353142844\n",
      "| Epoch: 255/300 | Train loss: 0.930435936283647 | Val loss: 0.9293141778972414\n",
      "| Epoch: 256/300 | Train loss: 0.9306453598173041 | Val loss: 0.9282659101817343\n",
      "| Epoch: 257/300 | Train loss: 0.9318243932305721 | Val loss: 0.9281212248735957\n",
      "| Epoch: 258/300 | Train loss: 0.9310107431913677 | Val loss: 0.9257710004846255\n",
      "| Epoch: 259/300 | Train loss: 0.9309524431563261 | Val loss: 0.9254775047302246\n",
      "| Epoch: 260/300 | Train loss: 0.9317801895894502 | Val loss: 0.9279728978872299\n",
      "| Epoch: 261/300 | Train loss: 0.9315435438825374 | Val loss: 0.9256822533077664\n",
      "| Epoch: 262/300 | Train loss: 0.9311119485319707 | Val loss: 0.9284045348564783\n",
      "| Epoch: 263/300 | Train loss: 0.9320610192784092 | Val loss: 0.9257623470491834\n",
      "| Epoch: 264/300 | Train loss: 0.9318156740121674 | Val loss: 0.9257972853051292\n",
      "| Epoch: 265/300 | Train loss: 0.9305864900873418 | Val loss: 0.9280068410767449\n",
      "| Epoch: 266/300 | Train loss: 0.9322620483866909 | Val loss: 0.9275265625781484\n",
      "| Epoch: 267/300 | Train loss: 0.9311306022761161 | Val loss: 0.9258004484905137\n",
      "| Epoch: 268/300 | Train loss: 0.9316565674647951 | Val loss: 0.9276273060176108\n",
      "| Epoch: 269/300 | Train loss: 0.9308644863597134 | Val loss: 0.9290545773175027\n",
      "| Epoch: 270/300 | Train loss: 0.9322346557650649 | Val loss: 0.9266062180201212\n",
      "| Epoch: 271/300 | Train loss: 0.9309902642902576 | Val loss: 0.9278874778085284\n",
      "| Epoch: 272/300 | Train loss: 0.93103280966742 | Val loss: 0.9262075059943728\n",
      "| Epoch: 273/300 | Train loss: 0.9307429449600085 | Val loss: 0.9283019776145617\n",
      "| Epoch: 274/300 | Train loss: 0.9311401296080204 | Val loss: 0.9256928761800131\n",
      "| Epoch: 275/300 | Train loss: 0.931111534854822 | Val loss: 0.9266900055938296\n",
      "| Epoch: 276/300 | Train loss: 0.9320046366306773 | Val loss: 0.9312863722443581\n",
      "| Epoch: 277/300 | Train loss: 0.9322856641652292 | Val loss: 0.9262802791264322\n",
      "| Epoch: 278/300 | Train loss: 0.930717326674545 | Val loss: 0.927341485189067\n",
      "| Epoch: 279/300 | Train loss: 0.9319859034136722 | Val loss: 0.9287288164099058\n",
      "| Epoch: 280/300 | Train loss: 0.9312424438041553 | Val loss: 0.9263946347766452\n",
      "| Epoch: 281/300 | Train loss: 0.932189412911733 | Val loss: 0.9281203498442968\n",
      "| Epoch: 282/300 | Train loss: 0.9308123839528937 | Val loss: 0.9273302016986741\n",
      "| Epoch: 283/300 | Train loss: 0.9304233302149856 | Val loss: 0.9283610574073262\n",
      "| Epoch: 284/300 | Train loss: 0.9311251966576827 | Val loss: 0.9276614329881139\n",
      "| Epoch: 285/300 | Train loss: 0.9311682843325431 | Val loss: 0.9264778428607516\n",
      "| Epoch: 286/300 | Train loss: 0.9325506022101955 | Val loss: 0.9277060305078825\n",
      "| Epoch: 287/300 | Train loss: 0.9316825333394503 | Val loss: 0.9270219438605838\n",
      "| Epoch: 288/300 | Train loss: 0.9309423277252599 | Val loss: 0.9277931145495839\n",
      "| Epoch: 289/300 | Train loss: 0.9319556589712177 | Val loss: 0.9254958546823926\n",
      "| Epoch: 290/300 | Train loss: 0.9310102379112913 | Val loss: 0.9260920509696007\n",
      "| Epoch: 291/300 | Train loss: 0.9314630370391043 | Val loss: 0.9273054574926695\n",
      "| Epoch: 292/300 | Train loss: 0.9304438199913293 | Val loss: 0.9270854799283875\n",
      "| Epoch: 293/300 | Train loss: 0.9305951327608343 | Val loss: 0.9265758511092927\n",
      "| Epoch: 294/300 | Train loss: 0.9313655721513848 | Val loss: 0.9267827288972007\n",
      "| Epoch: 295/300 | Train loss: 0.9311330924954331 | Val loss: 0.9267955836322572\n",
      "| Epoch: 296/300 | Train loss: 0.931317060662989 | Val loss: 0.929186549451616\n",
      "| Epoch: 297/300 | Train loss: 0.931687439115424 | Val loss: 0.9276712056663301\n",
      "| Epoch: 298/300 | Train loss: 0.9309702902509455 | Val loss: 0.9260759014222357\n",
      "| Epoch: 299/300 | Train loss: 0.9311028478438395 | Val loss: 0.9272741857502196\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 300\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "best_val_loss = np.inf\n",
    "\n",
    "for i, epoch in enumerate(range(num_epoch)):\n",
    "    train_loss.append(train(epoch))\n",
    "    val_loss.append(evaluate())\n",
    "    \n",
    "    tb.add_scalars('Loss', {'Train loss': train_loss[i], 'Val loss': val_loss[i]}, epoch)\n",
    "    tb.add_scalars('fc_weight/abs_mean',{\n",
    "            'fc1': model.fc1.weight.abs().mean(),\n",
    "            'fc2': model.fc2.weight.abs().mean(),\n",
    "            'fc3': model.fc3.weight.abs().mean(),\n",
    "            'fc4': model.fc4.weight.abs().mean()\n",
    "        }, epoch)\n",
    "    \n",
    "    tb.add_scalars('fc_weight/abd_var',{\n",
    "            'fc1': model.fc1.weight.abs().var(),\n",
    "            'fc2': model.fc2.weight.abs().var(),\n",
    "            'fc3': model.fc3.weight.abs().var(),\n",
    "            'fc4': model.fc4.weight.abs().var()\n",
    "        }, epoch)\n",
    "        \n",
    "    tb.add_histogram('weight/fc1', model.fc1.weight, epoch)\n",
    "    tb.add_histogram('weight/fc2', model.fc2.weight, epoch)\n",
    "    tb.add_histogram('weight/fc3', model.fc3.weight, epoch)\n",
    "    tb.add_histogram('weight/fc4', model.fc4.weight, epoch)\n",
    "    print(f'| Epoch: {epoch}/{num_epoch} | Train loss: {train_loss[i]} | Val loss: {val_loss[i]}')\n",
    "        \n",
    "    if val_loss[i] < best_val_loss:\n",
    "        best_val_loss = val_loss[i]\n",
    "        best_model = model\n",
    "        \n",
    "        best_model_weights = copy.deepcopy(model.state_dict())\n",
    "        torch.save(best_model_weights, f'./best_models/seismic_net_epoch_{epoch}_val_loss_{val_loss[i]}.pt')\n",
    "\n",
    "    scheduler.step(val_loss[i])\n",
    "\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"9153a75d-7d98-4a4a-ba6f-4f09bece3cbb\" data-root-id=\"1260\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"76fe6586-c464-417d-95bc-3e2374cb160c\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1271\"}],\"center\":[{\"id\":\"1274\"},{\"id\":\"1278\"},{\"id\":\"1306\"}],\"left\":[{\"id\":\"1275\"}],\"plot_height\":250,\"renderers\":[{\"id\":\"1296\"},{\"id\":\"1311\"}],\"title\":{\"id\":\"1261\"},\"toolbar\":{\"id\":\"1286\"},\"x_range\":{\"id\":\"1263\"},\"x_scale\":{\"id\":\"1267\"},\"y_range\":{\"id\":\"1265\"},\"y_scale\":{\"id\":\"1269\"}},\"id\":\"1260\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1269\",\"type\":\"LinearScale\"},{\"attributes\":{\"data_source\":{\"id\":\"1308\"},\"glyph\":{\"id\":\"1309\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1310\"},\"selection_glyph\":null,\"view\":{\"id\":\"1312\"}},\"id\":\"1311\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1283\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1282\",\"type\":\"SaveTool\"},{\"attributes\":{\"line_color\":\"blue\",\"line_width\":2,\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1294\",\"type\":\"Line\"},{\"attributes\":{\"axis\":{\"id\":\"1271\"},\"ticker\":null},\"id\":\"1274\",\"type\":\"Grid\"},{\"attributes\":{\"items\":[{\"id\":\"1307\"},{\"id\":\"1323\"}]},\"id\":\"1306\",\"type\":\"Legend\"},{\"attributes\":{\"source\":{\"id\":\"1308\"}},\"id\":\"1312\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1284\",\"type\":\"HelpTool\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"blue\",\"line_width\":2,\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1295\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"1320\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1303\",\"type\":\"Selection\"},{\"attributes\":{\"formatter\":{\"id\":\"1301\"},\"ticker\":{\"id\":\"1272\"}},\"id\":\"1271\",\"type\":\"LinearAxis\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1279\"},{\"id\":\"1280\"},{\"id\":\"1281\"},{\"id\":\"1282\"},{\"id\":\"1283\"},{\"id\":\"1284\"}]},\"id\":\"1286\",\"type\":\"Toolbar\"},{\"attributes\":{\"overlay\":{\"id\":\"1285\"}},\"id\":\"1281\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1265\",\"type\":\"DataRange1d\"},{\"attributes\":{\"source\":{\"id\":\"1293\"}},\"id\":\"1297\",\"type\":\"CDSView\"},{\"attributes\":{\"data\":{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299],\"y\":[0.9922582672701942,0.9931024867627356,0.984762547744645,0.9843847494986322,0.9851949082480537,0.9805067421661483,0.9824621048238542,0.986379973590374,0.9792261314060953,0.9816234078672197,0.9789395183324814,0.9790899455547333,0.9805436448918449,0.9752378654148843,0.9711804986000061,0.978230838974317,0.9767037414842181,0.9783105933003955,0.976051174932056,0.9705656742056211,0.9830229199594922,0.9738082695338461,0.9775928325123258,0.9700358038147291,0.9708643605311712,0.9729005669554075,0.9724673057595888,0.9721786139739884,0.9705885193414159,0.9660152122378349,0.9740442029303975,0.9726009940107664,0.9806113243103027,0.9861281696293089,0.9840213846829202,0.9787274069256253,0.9823408408297433,0.9692030731174681,0.9729582650793923,0.9820884731080797,0.9689310251010789,0.9576036880413691,0.9549025677972369,0.9516536287135549,0.9499814882874489,0.9496711144844691,0.9461927637457848,0.9473491013050079,0.9453555602166388,0.9475467759701941,0.9441518518659804,0.9463593918416235,0.9470600419574313,0.9454877807034386,0.9443304729130533,0.9440725727213753,0.9448396033710904,0.9409114643931389,0.9463727118240463,0.938861269917753,0.9437217232253816,0.943556923005316,0.9436360307865672,0.9417403390010198,0.9448001873162057,0.9425798108180364,0.9403496094875865,0.9386386689212587,0.9409268995126089,0.9378230422735214,0.9436746678418584,0.9333481043577194,0.9400633614924219,0.9347043989433182,0.9375963972674476,0.9435157312287225,0.937469832599163,0.9403787122832404,0.9380005631181929,0.940279902683364,0.9460883869065179,0.9396175791819891,0.9379457549916373,0.9345464242829217,0.9334683823916647,0.9319880795147684,0.9330692721737756,0.933320988383558,0.9321070081657834,0.9302130672666762,0.9286894069777595,0.9300193339586258,0.9312468071778616,0.9305252383152643,0.9300142584575547,0.9306162181827757,0.9323971155616972,0.9277369164758258,0.9325237489408917,0.9292431680692567,0.9275608443551593,0.9328259237938457,0.9312874418165948,0.9296782008475728,0.9297164736522568,0.9318266900049316,0.9296879842877388,0.9304626542660925,0.929543679787053,0.9278319138619635,0.9278176253040632,0.9303185310628679,0.9279032738672363,0.9296980640954442,0.9264102809958987,0.9269530814554956,0.9294258339537514,0.9277097309629122,0.9260333130757014,0.9306820813152525,0.9254614172710313,0.9276389239562882,0.9277038358979754,0.928006089395947,0.9271529341737429,0.9302559238341119,0.9263952117827203,0.9257334321737289,0.9280457844336828,0.9274594684441885,0.9283667579293251,0.9266295548942354,0.928119550148646,0.927215963602066,0.926291286945343,0.9298002852333916,0.92610930899779,0.9268261194229126,0.9277568840318255,0.9254938347472085,0.9271979480981827,0.9288510059316953,0.9269117762645086,0.9285441032714314,0.9271291154954169,0.92760012878312,0.9265561981333627,0.9279889389872551,0.9265345732371012,0.9281992647382948,0.9293611471851667,0.9271935762630569,0.9262514760096868,0.9281664904620912,0.9276819941070344,0.9274374801251624,0.9267660371131368,0.9272636746366819,0.9258169713947508,0.9261224493384361,0.9272180199623108,0.9272375644909011,0.9291282850835059,0.9272914123204019,0.9263073321845796,0.9279696353607707,0.9295904023779763,0.9271457071105639,0.9286556334959136,0.9286629698342748,0.9288882729079988,0.9242815574010214,0.9272496410542064,0.9294475060370233,0.9278000162707435,0.9263621436225044,0.9270298712783389,0.927596413426929,0.9246765772501627,0.9260841152734227,0.9278516496221224,0.9286156305008464,0.9264053247041173,0.9269906373487579,0.9298145572344462,0.9275694919957055,0.9277794477012422,0.9269409047232734,0.9253064120809237,0.9269154775473807,0.9270525731974177,0.9277065305246247,0.9298500170310339,0.928337622847822,0.9262383075224029,0.9292826842930582,0.927699126303196,0.9270053025748994,0.9281534958216879,0.9286740769942602,0.9263516407873895,0.9273599435885748,0.9264673151903682,0.929085718260871,0.9279395457771089,0.9293949579199156,0.9251619569129415,0.9265172895458009,0.9274059144987,0.9277438670396805,0.9288927805092599,0.9260867196652625,0.9296471012963189,0.9264069356852107,0.927885970307721,0.9287861792577637,0.9260682397418551,0.9278825918833414,0.9287150858177079,0.9270598888397217,0.926599894132879,0.9275563491715325,0.9285272500581212,0.9251180738210678,0.9282131062613593,0.9279748052358627,0.927964319785436,0.9287296111385027,0.9282232208384408,0.9261349919769499,0.9260479584336281,0.9268038695057234,0.9274659496214654,0.9298304443558058,0.9249821503957113,0.9277461907929845,0.9254525634977553,0.9278863734669156,0.9262832875053088,0.9294701168934504,0.928447818590535,0.9277535768018829,0.9265411520997683,0.9253456617395083,0.9272684612207942,0.9274447982509931,0.9275188917915026,0.9282186155517896,0.9276970699429512,0.9266874218980471,0.9267003726628091,0.9284050050708983,0.9262391221192148,0.9288807287812233,0.9270139353142844,0.9293141778972414,0.9282659101817343,0.9281212248735957,0.9257710004846255,0.9254775047302246,0.9279728978872299,0.9256822533077664,0.9284045348564783,0.9257623470491834,0.9257972853051292,0.9280068410767449,0.9275265625781484,0.9258004484905137,0.9276273060176108,0.9290545773175027,0.9266062180201212,0.9278874778085284,0.9262075059943728,0.9283019776145617,0.9256928761800131,0.9266900055938296,0.9312863722443581,0.9262802791264322,0.927341485189067,0.9287288164099058,0.9263946347766452,0.9281203498442968,0.9273302016986741,0.9283610574073262,0.9276614329881139,0.9264778428607516,0.9277060305078825,0.9270219438605838,0.9277931145495839,0.9254958546823926,0.9260920509696007,0.9273054574926695,0.9270854799283875,0.9265758511092927,0.9267827288972007,0.9267955836322572,0.929186549451616,0.9276712056663301,0.9260759014222357,0.9272741857502196]},\"selected\":{\"id\":\"1320\"},\"selection_policy\":{\"id\":\"1321\"}},\"id\":\"1308\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"text\":\"Minimum validation loss: 0.9242815574010214\"},\"id\":\"1261\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1280\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1272\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1276\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1304\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1321\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"label\":{\"value\":\"Val RMSE\"},\"renderers\":[{\"id\":\"1311\"}]},\"id\":\"1323\",\"type\":\"LegendItem\"},{\"attributes\":{},\"id\":\"1301\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"line_color\":\"red\",\"line_width\":2,\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1309\",\"type\":\"Line\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"red\",\"line_width\":2,\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1310\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"1267\",\"type\":\"LinearScale\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1285\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"data_source\":{\"id\":\"1293\"},\"glyph\":{\"id\":\"1294\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1295\"},\"selection_glyph\":null,\"view\":{\"id\":\"1297\"}},\"id\":\"1296\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1279\",\"type\":\"PanTool\"},{\"attributes\":{\"label\":{\"value\":\"Train RMSE\"},\"renderers\":[{\"id\":\"1296\"}]},\"id\":\"1307\",\"type\":\"LegendItem\"},{\"attributes\":{\"formatter\":{\"id\":\"1299\"},\"ticker\":{\"id\":\"1276\"}},\"id\":\"1275\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1263\",\"type\":\"DataRange1d\"},{\"attributes\":{\"data\":{\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299],\"y\":[1.0334288963100366,0.998504044089401,0.9940716214347304,0.9959930767092788,0.9909853483501233,0.9867492880737573,0.9855387650038067,0.9873436239727756,0.9862075364380553,0.9840352445317988,0.9843720028274938,0.9850000506953189,0.9823251431448418,0.9875350778562981,0.9947850984439516,0.9843198017070168,1.0145523249057302,0.9806772506027891,0.9805430280534845,0.9817254091563977,0.9805183665794238,0.9796897965565062,0.9793175318784881,0.9780614863362229,0.974752914696409,0.9808251023292541,0.9763320544309784,0.9809613200656154,0.9771394704517565,0.9765773499221132,0.9741248875333551,0.9756703548264085,0.9787196991736429,0.9807283792579383,0.9846269996542679,0.9820626549553453,0.9953461316593907,0.9810896743807876,0.9786127163652788,0.9822722606491624,0.9805240122895492,0.9643745936845478,0.9591801043142352,0.9578995031222962,0.9565963881057605,0.9559136643744351,0.9533569973811769,0.9527000228563944,0.9512391920675312,0.9521434583162006,0.9508232583079421,0.9499379666228044,0.9501703829096074,0.9492271423339844,0.9500262473758898,0.9492450308381466,0.9494198962261803,0.9492342595468488,0.9491658417802108,0.9482704440752665,0.9477496030037863,0.9497327302631579,0.9481141395736159,0.9466544393907513,0.9489661852518717,0.9483282731290449,0.946803639437023,0.9481697005138062,0.946922182827665,0.9453539350576569,0.946466146644793,0.9456815778163441,0.9469877830722876,0.946292275294923,0.9462349912576508,0.9479165395100911,0.9461076757364105,0.9460580809074536,0.9456008323451929,0.9468181173006693,0.9466972652234529,0.946055811539031,0.9453444221563506,0.9387514988581339,0.936902932325999,0.935850827735767,0.9351398829828229,0.9350552734575773,0.9355861893871374,0.9351558706216645,0.9350110125123409,0.9344050539167303,0.9344660068813123,0.9389040465940509,0.9345756438740512,0.9342750486574675,0.9340883652369182,0.9346569211859452,0.9345368075789067,0.9343223180687218,0.9350917172013667,0.9341238634628162,0.935467681550143,0.9340851181431821,0.9340418117088184,0.9343240775560078,0.9341926664636846,0.9335398743027135,0.935367795040733,0.9340237650954932,0.9333514644388567,0.9334619001338357,0.9321994860967,0.9309026659580699,0.9325565296306945,0.9317013240697091,0.9322087327639262,0.9319878530084041,0.9318840857137713,0.9321153230834426,0.9321391009447867,0.9322790672904566,0.9312998696377403,0.93154341714424,0.9317013694528948,0.9324590011646873,0.9320831702466597,0.9321769802193892,0.9326267491307175,0.9313952059076543,0.932072745498858,0.932090501408828,0.9313020748004579,0.9311668324888798,0.9321647472548903,0.9309227985248231,0.9319921399417677,0.9305695194947092,0.9315890328925952,0.9318438180705957,0.930866035034782,0.9322545494949609,0.9314383709639834,0.93217128640727,0.9318742764623542,0.9316892186800639,0.9319273547122353,0.9314668548734565,0.9308776847103186,0.9307814683830529,0.9316337096063714,0.9306036526696724,0.9308338834528338,0.9308943016487256,0.931900703907013,0.931661261591995,0.9305706277228238,0.9316761219710634,0.9319199505605196,0.9317813254239267,0.9316922348842286,0.9325768801203945,0.9316054808466058,0.9312317419470402,0.931112169592004,0.931937725502148,0.9308904610182109,0.9311253911570498,0.932217604444738,0.9315843684631482,0.9313220823020266,0.931200941612846,0.9321548018539161,0.9324849273029127,0.9313530296610113,0.9307960131712127,0.9315734350890444,0.9322398223375019,0.9313013329840543,0.9320004691157424,0.9317181842368946,0.9323713984405785,0.930891317861122,0.931840369575902,0.9316919977204842,0.9310535943299009,0.9314605035279927,0.9320630236675865,0.9317342540674042,0.9316366204044275,0.9316859722137452,0.9317230728634617,0.9310492302242078,0.9309057746017189,0.9316546893956369,0.9311485591687654,0.9309359636223107,0.93073130741454,0.9315414062717504,0.9316120630816409,0.93140231580065,0.9323971932394463,0.931685429497769,0.9316727790916175,0.9310609369947199,0.9314659622677586,0.9316557010014852,0.9310743725090697,0.9311684535260786,0.931056715312757,0.9321302234080799,0.9318519282759281,0.9313365469899094,0.9322366128888047,0.93101319011889,0.9317116883763096,0.9308416241093685,0.9310140764504148,0.930929278281697,0.9320672813214754,0.9309654681306136,0.9310808047913668,0.9306070382134957,0.9316964396259241,0.9318534349140368,0.9308460179128145,0.9313984214213856,0.9335872750533255,0.9313801470555757,0.9315749268782766,0.9313866470989428,0.9315213682358725,0.9317558441245765,0.9304879027500487,0.9312598606996368,0.9315217043224134,0.9308900341652987,0.9309145776849044,0.9318206007020515,0.9310024336764687,0.9319658459278575,0.9311251537841663,0.9314985139328137,0.9314481137091654,0.9321020647099144,0.9314964633238944,0.9300486949452182,0.9311205707098308,0.9312306391565424,0.932435256974739,0.9313235561052958,0.9314371416443272,0.9314471899417409,0.9317714653517071,0.9320327808982447,0.930435936283647,0.9306453598173041,0.9318243932305721,0.9310107431913677,0.9309524431563261,0.9317801895894502,0.9315435438825374,0.9311119485319707,0.9320610192784092,0.9318156740121674,0.9305864900873418,0.9322620483866909,0.9311306022761161,0.9316565674647951,0.9308644863597134,0.9322346557650649,0.9309902642902576,0.93103280966742,0.9307429449600085,0.9311401296080204,0.931111534854822,0.9320046366306773,0.9322856641652292,0.930717326674545,0.9319859034136722,0.9312424438041553,0.932189412911733,0.9308123839528937,0.9304233302149856,0.9311251966576827,0.9311682843325431,0.9325506022101955,0.9316825333394503,0.9309423277252599,0.9319556589712177,0.9310102379112913,0.9314630370391043,0.9304438199913293,0.9305951327608343,0.9313655721513848,0.9311330924954331,0.931317060662989,0.931687439115424,0.9309702902509455,0.9311028478438395]},\"selected\":{\"id\":\"1303\"},\"selection_policy\":{\"id\":\"1304\"}},\"id\":\"1293\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"axis\":{\"id\":\"1275\"},\"dimension\":1,\"ticker\":null},\"id\":\"1278\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1299\",\"type\":\"BasicTickFormatter\"}],\"root_ids\":[\"1260\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.1\"}};\n",
       "  var render_items = [{\"docid\":\"76fe6586-c464-417d-95bc-3e2374cb160c\",\"root_ids\":[\"1260\"],\"roots\":{\"1260\":\"9153a75d-7d98-4a4a-ba6f-4f09bece3cbb\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1260"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "title = f'Minimum validation loss: {np.min(val_loss)}'\n",
    "p = figure(title = title, plot_width=600, plot_height=250)\n",
    "x = np.arange(len(train_loss))\n",
    "p.line(x, train_loss, line_width=2, color = 'blue', legend_label = 'Train RMSE')\n",
    "p.line(x, val_loss, line_width=2, color = 'red', legend_label = 'Val RMSE')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
